{"docstore/metadata": {"3300603e-6396-4503-ace0-497dcd8029cf": {"doc_hash": "442409102d71f8083d5c176cd680970a326a79b80f4e3bbc2c5db81f28a68937"}, "0cd449d0-3faf-43e2-8997-77a60dd8bc6e": {"doc_hash": "6af96f779b3ab025a1d8a3558ccd1e0db4b7c047a7e50a3a75b344d54e8d51cd"}, "36163862-d97e-44e2-9afa-b6588cc4e49b": {"doc_hash": "ecb3ff08dfa1d96d367754c4fa222eab039c6d8407cedd4d8b3ce55d7ad8e80b"}, "1f45d831-0530-4252-970d-3f3cf0636715": {"doc_hash": "9ade8e9aa8648e4056ad4de37c49debb4cdd84d50e2a649c128a976b27cfe206"}, "d3c86b19-ae2a-4cbf-b2dc-cbc196285c11": {"doc_hash": "b8b076e3679238b38a7eada70e64297b1060fb53aab2fe2f3feadec9038779e5"}, "abd720f7-e7c7-4f0c-90b0-55c5c600a64a": {"doc_hash": "78eac3543db985b49a47e38d04f8a7abc25320c78b40fe39387c3f002d05e794"}, "c1aaab2c-7d44-497e-8cc6-6ec01aaf2b67": {"doc_hash": "a66238ad15a00535a8833526e2292928efc7de240331969fc8b8225f27e601ff"}, "46b2ca3a-e9c9-4443-9a1e-bdb5ebf5cfa9": {"doc_hash": "19e1f6a5ab9933bd459bafe5352307ba5f2dfd0f4304fba31736d62b8063a192"}, "3fe1691c-6cb0-40c8-961d-e25dfc04a1fe": {"doc_hash": "388597f470f98ba204965fdb3dc49a21568ee9ce4c2370cc98276989f6d45636"}, "6b01cfc6-6781-42a8-98b0-3bc0ae981d95": {"doc_hash": "b7a084ddcb62ee5eca50b7f32101bd4527d0bee52c75e56aa9dddb6f4265e844"}, "702dbea1-3fd7-4e63-98f3-514d0596c2bb": {"doc_hash": "45709a804a8faf32c8f98570063e8fdf78bd405ec5b8720bef7287d1c1fcecd3"}, "12ac5c52-43db-47fd-8c86-b6acc1d7b19b": {"doc_hash": "b82a1c45fdafeac9a4fb02af931baf4c4ce4cfc1dc0163b6d1e372a6c251eb1f"}, "15cb1b71-56a6-4191-94ca-b2417fc78085": {"doc_hash": "934fb0564c4b84c9f453e703aac199f7f59a06da7086fef6fdb2c204c6ade290"}, "f7bba5bf-f672-4398-8849-16ba4dd084fc": {"doc_hash": "683afed788e26248269e793fe53ef7e6542876bbef11905674399cf21bcce888"}, "04947ad0-6e74-4888-ad68-561644f6ce91": {"doc_hash": "613db13869148cf0d718f276fa11a39ddee9eee338163d399dd5f8a75bc1c069"}, "199c7629-974f-40ae-94a7-b516538f02cd": {"doc_hash": "078c62bb8a8db88bf0eb267ce24452fe12be71f10c8c963756e62893f1c27916"}, "48769f2d-0bc5-4887-9019-0f89da8f3a15": {"doc_hash": "da5c41ccb996cd8fde8cc642b0d35bf597bbd7e617c004e2ca7f877580a399b7"}, "e5c821c4-f03f-40b6-8875-c89eeac79c24": {"doc_hash": "c1dd9ceb5ab2368995aa8e2bb2a82f634f72b20de33a50cf432bc2f49281d605"}, "072d472c-0b9e-4c76-9399-7b1237cdf258": {"doc_hash": "ef6d275f0ec45f92580ef7c2eb0c4746e01422d557fa3a3c0218b213f82566dd"}, "50278bb3-b938-4698-a028-5b396c83bcdb": {"doc_hash": "94d4e9afccc3505a41c24c793cf4219a3310c20a9081a5e20f5e40a7d9a21cc0"}, "2f6b3f08-cf99-44da-9369-338a6365b30b": {"doc_hash": "fe39947021b130e579e155834fdb4b3c46b6efe66105324b18307f60b5e09f58"}, "31f060e9-ff3a-48b7-83c9-724b39d49613": {"doc_hash": "22dc2aa6433501b4da4c0f92f0f4a10f54e6c1cd0b540063ef813bb9d3dd969f"}, "de25f2b7-b7a3-4cf1-80ba-053408f47823": {"doc_hash": "c4551400b635fd0aed770b01474784fec462dd7b5fcf07098021167b64dc446a"}, "66f4c94e-ebda-460b-b057-e2956e0cc581": {"doc_hash": "25de27f4660a60f521474b693f0f79168ebfa7b5ffd4eb11c9cad28b1d6a88aa"}, "1ed27771-4fba-4efb-ba51-eaf25a346d2f": {"doc_hash": "f72e274a969a631229a21d338259dad4f50463c181cd195c7e1ff5a6a644d239"}, "211288c5-2a13-4149-b0db-ddb5f946ff17": {"doc_hash": "64638a8eabf3a667e1bfa77663ab1580a41636dfac2ba7ce11e1ebabcb89224c"}, "8d726df1-939d-456c-99cc-0ff9bd6fa13f": {"doc_hash": "865759d071c730ec3190651649ea9d8cb2c645b4ded76c1dd14a5cab720dcb60"}, "e30dacd8-a1b3-4a64-bdf3-07ac52a35a25": {"doc_hash": "e4f8b254d44b4006a10f35c17fddce5cd0f58369e85694c0cc9b6409e1117159"}, "7aab6e5a-8007-4760-aa2d-c59bdb1d7cdf": {"doc_hash": "5572624d4328fa6d7a46ab3c9b938dcf8b2c76918dfa472e1e43c54fe5c083af"}, "eb6d4eb8-f779-4455-ab83-71f04ad97cc4": {"doc_hash": "8c65987194ddbaf1d6195fd778a6742ccb9ec6d33338d585bd4b7d8a7ea6b0fa"}, "2ff2ec0f-515d-4cf3-a152-beb3dc3f5428": {"doc_hash": "af50b89c27654da3274d265c67135462168dff75563a9f1ff2caabf4c20abd5f"}, "fd7cb11f-69ff-4a19-bb3d-6eb2a9d7d6ca": {"doc_hash": "bda8550a9fb3bee322138e7c54a49b33b967b1f506018397089b7a2a17ec617d"}, "8c735fee-441e-4c0c-b648-3f8927f54a84": {"doc_hash": "b46f5ef54d65cb1b4131d9706c72ca537e5dbb021fb1b7271cdbb36aca239357"}, "45814271-f6ec-4320-b1e3-0ab05b675675": {"doc_hash": "c1237047e55ea53e5bf0f45b7e95c915cdfd43d101ebf48b27f0e9e40583f9e5"}, "c3fc50ad-4f2d-4c95-ae46-5044eb9ce075": {"doc_hash": "cb3bcda5528496156bdf3c86a9a63a95d7577d129b0ba9756fefce5ee533d13c"}, "6c02fa2c-5fa8-4429-b452-20051d98420a": {"doc_hash": "6c31a1f69c709c3c4eeef472a375655def3ab4d5356ac8c863d1bab3fbf3dd11"}, "fa132b5f-9ded-444a-b92a-c1fd59ea8883": {"doc_hash": "664ad2a3729945a551a00bae3e56fd9fcfdd495a6b7384a7c34658e1d25329aa"}, "92cbcd51-6bba-470b-9f79-b1f11f89181d": {"doc_hash": "4b5e881cd577b765a3add6099da35e24d3e125c2888510a13076fb8c502d83bc"}, "4fa2b42e-4e5a-4f5a-8421-38540b1c6c1b": {"doc_hash": "5a6c72c3bf9fc79b1c597adce2745e4b0a9d0997b975ee9f4ac3f8370d1e0d0f"}, "95d855cc-70e1-4f45-a1ca-bf68dc39041b": {"doc_hash": "c989a48fb7da024627bfc24962a9440ff940107b893e2c4dbbdf1bb865de4f26"}, "379ee6e7-e1e5-4cc1-b19d-b6ba9ee3474d": {"doc_hash": "7a5e49643296f786cf952eb97ed7b7d0d94769534958316f84d84b22d37c5326"}, "0e469dfe-ab9a-44e9-8705-5c55f9815440": {"doc_hash": "c2071253e8853fcff3112b1e199ac528fd43196e7f6bd76471dcea8251f6e39b"}, "0531013c-7af7-43f3-80c9-108a054e3641": {"doc_hash": "a4298d79a426b2bc235911dd9096ca410c156a751b5f032852ceececb1878b41"}, "f39d25a6-d2f8-41da-a57e-5675bf769a17": {"doc_hash": "aeaabeb3f250bbba126b04fe18412c23179213fff9a8cd4082ca671a2d308fe2"}, "35f12626-eb91-4636-9d1e-b11150a477a1": {"doc_hash": "baf296effc09532f1a817669828d780e36c6956a34815e02d3cacfb486557dc4"}, "fb9635b4-1982-4ae3-b466-0d05e732bb67": {"doc_hash": "5cea3ef7a090578f5e676c4e2b1ebeae2d1aa1ba9347765a74c3df1a2b0b6949"}, "5b7b49de-1f13-4737-a47c-32ac293626f9": {"doc_hash": "4726fc15d7f48f0d9273d24e057fbe64118f68f1ba9a7c9ad8a96605d96ddd0a"}, "cd42df60-847b-4bde-b4cc-5306dfde5f1b": {"doc_hash": "93657660469f4cc780fb465c68426b7c568627c68c48f136405394d7259a9bfb"}, "725f8c93-52b6-48e3-86a2-8c30d1c419b0": {"doc_hash": "5d3fd7a207fb7770f31a59aa13efc5666d45af976a875a24da9a13430c9cb90e"}, "c7c28efb-12e7-47e1-a7fe-7b2fab2f2107": {"doc_hash": "dc278ac6027e288d4d0cbb482db7fa2fc878b10fe65afd4f5f144c48e38889bd"}, "6d9917be-8db3-4511-b957-1bcc2b4be680": {"doc_hash": "bf7b2abcaa3818a12fbd82c9085e424b293573a639749eb40460f7993ec12e2e"}, "c844d012-1072-4175-91e7-79f4de4f077a": {"doc_hash": "438ea858f094990dd5ace549874102096ab6d4a0913b883847e22927ff30157a"}, "b851bf06-f4c9-4140-94b6-dd2b06d017f7": {"doc_hash": "69715634dd87d8c9f88322f349ecb9831675369e75f3c4e267199750b986f1ed"}, "53102d6c-a640-4c4f-baad-a18dbcf4efc7": {"doc_hash": "18b3e5ddf55f407b46ddd9a95bae8ce7133b31d546264befaacc6548c06e81f8"}, "3d542561-e275-4984-9121-7a6ad0e6bf52": {"doc_hash": "ae3bf9892627bfc219270cdde4030228633440e31165847171338bb27a99bbca"}, "b54e3c4b-2163-4710-8425-e7ea55298923": {"doc_hash": "d35c3c27dbb0afe32745c39e34ae9e2cc8ab63e26351782722a8c7a3f64db23e"}, "e5dd65d2-5eda-4373-a9f5-258e655492bf": {"doc_hash": "ccbf3c4420edca8343d4a731b4e732a2d3fa0ef3d473fa064d2c9d14ad4326ca"}, "1bf610f3-69fc-4de9-ae7c-b738a8f645da": {"doc_hash": "8c0582c67d0653bc0c0eab131f40fb14d0497d8f4044304298d9d6b58be5f6ef"}, "bde80ebd-a216-4211-afd9-b0a403478f83": {"doc_hash": "4b8e36e243504d739a1a276ec4c86d7fe6dccc6f96c5401c8a1d3a8d28d4520c"}, "9bb0cde4-e229-43e8-8579-490e9d192022": {"doc_hash": "747f540377dafeb78e136d6bfcd1a8fb71237c51236b106ddb9294d5ddd6c702"}, "409bb29e-17e8-420e-833e-9d8b18248de1": {"doc_hash": "753458188b2a551025a5b669e65ebf32d379254b1c8600b79c02ff8af01f649d"}, "a31a05fe-7eca-465b-85da-4c39b0936020": {"doc_hash": "38d986503e7cceaabc8d64c55475727ad01d73dcb8079690a23d1a8ba4c3faef"}, "249f8562-a7a5-40a4-a029-700ee5f1463d": {"doc_hash": "edd31adcc456970d9d43ed19036c3cf36a3a239377cfe6b2b029f8d17c3e9a94"}, "d55e4055-e446-4d88-b7cb-b108aab513ed": {"doc_hash": "a6023d0c7b2824a61a88497dec9a049798f34cf1c2b6be8509e7a6f6ce1e2194"}, "0b61cf34-83e5-4ce7-9f95-da6fd850700d": {"doc_hash": "935ce458e5aeb468b174825a6facb6d9c9928c359befb36177dcaf18f0674d43"}, "718e8467-1957-4cb9-b710-f7a18ce719ca": {"doc_hash": "88091653e0371d698e78dfb63bae57c6510826ede6f22835cd78b971e391e80e"}, "f84bfaf7-4c6b-48bc-a7f7-8251d9c317fe": {"doc_hash": "a58c047177b1e93035d31f9c3ade8287a92f2bf12c5739f68b9f3fd4b82730cd"}, "920f1b13-bf7c-4e18-a5ce-3c0b6d700a7f": {"doc_hash": "6c23fe5bd08d27ad8aee057e324c1c16686abcfda4dbc82fb064db24c331e598"}, "a0a69297-355a-4dc3-bb23-e9e52f949efa": {"doc_hash": "66f8957d67b5d604acc43be8fe6b7734b626af61c06fd7560fdac05fe032dde7"}, "31cc409f-9256-431c-8121-9c5756707107": {"doc_hash": "2c2846f973c4ee76224388f1e0d2f648e9dc68aefba15597ba5645678e9bcacc"}, "43cbcae8-6cc7-4c56-b367-e7d073c73e2e": {"doc_hash": "442409102d71f8083d5c176cd680970a326a79b80f4e3bbc2c5db81f28a68937", "ref_doc_id": "3300603e-6396-4503-ace0-497dcd8029cf"}, "3b2d001f-00fc-4be0-914f-6d363e110b28": {"doc_hash": "87da3ecd14b5a355563536b799c427b339a42a19dc4a56548d5cb586333ee263", "ref_doc_id": "0cd449d0-3faf-43e2-8997-77a60dd8bc6e"}, "852669ea-9eaa-4666-959f-feeeef0043d4": {"doc_hash": "d958375fad9498e617817a9725caf71ed55c278d3884eeba733955ac60bff4e9", "ref_doc_id": "0cd449d0-3faf-43e2-8997-77a60dd8bc6e"}, "dd63402e-3d67-4c55-a6ca-534e31dceb92": {"doc_hash": "ecb3ff08dfa1d96d367754c4fa222eab039c6d8407cedd4d8b3ce55d7ad8e80b", "ref_doc_id": "36163862-d97e-44e2-9afa-b6588cc4e49b"}, "2d3ef193-bd71-4d92-a043-52f2543179e2": {"doc_hash": "85aac0ea0a318c3c699fd4534a779814dcbc134ba644f2d454dc8551d39e0e8e", "ref_doc_id": "1f45d831-0530-4252-970d-3f3cf0636715"}, "1d037085-21aa-40c4-9af7-09e68658c56c": {"doc_hash": "de00474512f04159901f73c70f61e0447a85a6f7582e3c66b8b070ebc5e68646", "ref_doc_id": "1f45d831-0530-4252-970d-3f3cf0636715"}, "a730939c-ce27-4c35-9d7c-eb55921c6671": {"doc_hash": "0a45e308d4ab5bb806455c8fa4845734498ad63ec03241912633d037639061e7", "ref_doc_id": "d3c86b19-ae2a-4cbf-b2dc-cbc196285c11"}, "09763838-24c2-4eb1-9e1b-4eac3aff565e": {"doc_hash": "db0752856b1731b76c5199c87f41ddec4d82421be60c9f2b3a98ebe2682c4839", "ref_doc_id": "d3c86b19-ae2a-4cbf-b2dc-cbc196285c11"}, "b5b522c3-0b03-41d8-9c58-af812f9d2cc9": {"doc_hash": "91d0742f47b2ea8732b0df454fe98c905dd28638a57b22b4e3c514ac82909363", "ref_doc_id": "abd720f7-e7c7-4f0c-90b0-55c5c600a64a"}, "d378120f-4074-4020-8adf-dd9cb539b3a0": {"doc_hash": "e71ab802ffc70ecdfcff8dc2e37d9a0dae8117c59d26385fcaf88877da902190", "ref_doc_id": "abd720f7-e7c7-4f0c-90b0-55c5c600a64a"}, "02110f77-ff94-4534-b3dd-7c4c7c0976c2": {"doc_hash": "038c8c497ca6404f0cca621a6652e654a5cb9ea853ff302b7b06a851223085ca", "ref_doc_id": "c1aaab2c-7d44-497e-8cc6-6ec01aaf2b67"}, "361afec3-2d22-46ef-8e45-b692f342f90e": {"doc_hash": "434ce6bfea892b57615502e6ac31d906cae3b765d91689404d7841c5dbd4945a", "ref_doc_id": "c1aaab2c-7d44-497e-8cc6-6ec01aaf2b67"}, "72064272-5b7a-4547-b7da-9b1b02c4f638": {"doc_hash": "bf1a5c880a1acc9ed8545c270281e1f6fad616674ddc674625143a67d3319101", "ref_doc_id": "46b2ca3a-e9c9-4443-9a1e-bdb5ebf5cfa9"}, "14a36df0-a23a-4bff-8d9a-bec8352fd24c": {"doc_hash": "ba75b5d00e8598fb0ecbf6cb45e8820b238e6de24af0e1cb1c1c32272e4bba38", "ref_doc_id": "46b2ca3a-e9c9-4443-9a1e-bdb5ebf5cfa9"}, "32e367c7-d4d9-42cf-af1e-fcc2ef7a4916": {"doc_hash": "388597f470f98ba204965fdb3dc49a21568ee9ce4c2370cc98276989f6d45636", "ref_doc_id": "3fe1691c-6cb0-40c8-961d-e25dfc04a1fe"}, "213385ad-7f81-43e4-b831-83b0a542143b": {"doc_hash": "690c91c834c2035b73d1834e299b18cba4859909873d3a797a45a8149fea94af", "ref_doc_id": "6b01cfc6-6781-42a8-98b0-3bc0ae981d95"}, "11d20e68-07f8-49ae-b5ae-4afa2c177498": {"doc_hash": "34927a9ee0ed31eb68a2f00a108e2e9fe9bd10219bcec1a82c3df3a039bc051f", "ref_doc_id": "6b01cfc6-6781-42a8-98b0-3bc0ae981d95"}, "c57e6931-2394-43c7-859b-47395aada179": {"doc_hash": "c4055128c30a874ebbc911a5b5bf01643ca53e5d7201c64d07317c7b435e11f7", "ref_doc_id": "702dbea1-3fd7-4e63-98f3-514d0596c2bb"}, "c9630248-3990-4524-94d8-92e12325c13e": {"doc_hash": "45c147eaad1a194c7849b4f3705c09aab02a4a914f3c675e70d5540fc052cf30", "ref_doc_id": "702dbea1-3fd7-4e63-98f3-514d0596c2bb"}, "2b53c0a7-6c0f-48e9-be31-f72eb7bac96c": {"doc_hash": "97b46a36c1b899634b2013a793d989d24622a738ea9f7d9d32cd544abd3950ca", "ref_doc_id": "12ac5c52-43db-47fd-8c86-b6acc1d7b19b"}, "562dd12c-08d7-42d4-8352-04ce8ff6bc35": {"doc_hash": "fc6bf20d6a53fa94dff9fff0062f47fd75beb940a0c5f077e7cc4ee0a5de40cc", "ref_doc_id": "12ac5c52-43db-47fd-8c86-b6acc1d7b19b"}, "696bf2ab-3eb5-4680-8d43-d16c9c40bf49": {"doc_hash": "b5c6443698a00e81a3f23e4bd80687ed3efa237212ab2ab4eb8ea3f9fd999383", "ref_doc_id": "15cb1b71-56a6-4191-94ca-b2417fc78085"}, "47a6a4a1-753d-4588-bbde-b69906a25140": {"doc_hash": "0e47fbd53905b02a3109c92949d89a952d748ea43942b7f22e4f3569550eb0ae", "ref_doc_id": "15cb1b71-56a6-4191-94ca-b2417fc78085"}, "9209a8d9-7f9c-442b-965a-3938a956c95c": {"doc_hash": "6f0bca805876a95463bd20cd5264e284596838fde217d3ec126f224741a53582", "ref_doc_id": "f7bba5bf-f672-4398-8849-16ba4dd084fc"}, "9c8061f2-9b95-48b4-a614-19d60f880b64": {"doc_hash": "b8859ed64929644e49d4d05835a21db71b388bd0887075c0183e0bc0596de0b3", "ref_doc_id": "f7bba5bf-f672-4398-8849-16ba4dd084fc"}, "2231e565-3d42-44d2-a8ba-8449cf61ba30": {"doc_hash": "3c5f7d52d526763d8f180c9b5c76d96d6d887a9ddb8bf068afce02ef73969c9b", "ref_doc_id": "04947ad0-6e74-4888-ad68-561644f6ce91"}, "8d251e7a-b243-4039-94b0-d52417ddba74": {"doc_hash": "b6ef01e97d814535bc615578320d72e1e1c5e281bff4e00f0d016fae82a005c3", "ref_doc_id": "04947ad0-6e74-4888-ad68-561644f6ce91"}, "cc4926e3-c0f9-4ca9-8b15-a3bbef9e36d9": {"doc_hash": "078c62bb8a8db88bf0eb267ce24452fe12be71f10c8c963756e62893f1c27916", "ref_doc_id": "199c7629-974f-40ae-94a7-b516538f02cd"}, "73e97549-8258-4c51-91ee-03c795f38ad7": {"doc_hash": "da5c41ccb996cd8fde8cc642b0d35bf597bbd7e617c004e2ca7f877580a399b7", "ref_doc_id": "48769f2d-0bc5-4887-9019-0f89da8f3a15"}, "c5ae9077-55ab-40aa-a6c1-70da3f751a61": {"doc_hash": "c1dd9ceb5ab2368995aa8e2bb2a82f634f72b20de33a50cf432bc2f49281d605", "ref_doc_id": "e5c821c4-f03f-40b6-8875-c89eeac79c24"}, "f2f4b5bb-0be8-4794-b3f9-8054454b1858": {"doc_hash": "ef6d275f0ec45f92580ef7c2eb0c4746e01422d557fa3a3c0218b213f82566dd", "ref_doc_id": "072d472c-0b9e-4c76-9399-7b1237cdf258"}, "6c75945b-edeb-4716-8c12-3b75e62b5cc0": {"doc_hash": "94d4e9afccc3505a41c24c793cf4219a3310c20a9081a5e20f5e40a7d9a21cc0", "ref_doc_id": "50278bb3-b938-4698-a028-5b396c83bcdb"}, "65977908-800a-4f24-9295-9fb50c47da63": {"doc_hash": "6e23a690d248fa6268bf01cfdddd84244b2c4ba8d21deafbcacc5119ea60d892", "ref_doc_id": "2f6b3f08-cf99-44da-9369-338a6365b30b"}, "cb6ac9e0-df2d-43d3-8098-51bccd70affc": {"doc_hash": "12515e2f51136850c0f396d84c16630fcb6aa12814011e8990186b9237ce81e8", "ref_doc_id": "2f6b3f08-cf99-44da-9369-338a6365b30b"}, "c303ac3a-0f9e-47fc-9ee3-c304a1211b67": {"doc_hash": "998f6d023ca7061e4e4532657f0b963ba4087aed44ef2763a76ef4c2aab92099", "ref_doc_id": "31f060e9-ff3a-48b7-83c9-724b39d49613"}, "5e77d204-2d3e-469f-a933-4798c6863343": {"doc_hash": "b5a5b568a0842e46f509de3a691f15b79dc771fa1032b7db7aee41abbf6aa41a", "ref_doc_id": "31f060e9-ff3a-48b7-83c9-724b39d49613"}, "2b732bcb-5b93-4522-9b26-7501c5fe7f31": {"doc_hash": "c4551400b635fd0aed770b01474784fec462dd7b5fcf07098021167b64dc446a", "ref_doc_id": "de25f2b7-b7a3-4cf1-80ba-053408f47823"}, "38c7dee0-b302-4558-b0a3-458f6dddfdcb": {"doc_hash": "31aa683e4a438a954a2d19314773aa4f60ec60e00140b5384dae506fea3fc168", "ref_doc_id": "66f4c94e-ebda-460b-b057-e2956e0cc581"}, "4b3dcc69-3fcf-433b-b378-86cb258bb995": {"doc_hash": "21c95ff9a7ca4b2e06cab1aebeea709e8d6e344b8dce17e08a542166946d1e18", "ref_doc_id": "66f4c94e-ebda-460b-b057-e2956e0cc581"}, "e7843b5a-427f-4d2d-9a26-804a34e7d2d6": {"doc_hash": "662065620c0983f7c37476524ff0e6977d83065e9efa27b8b40958c1d36ac675", "ref_doc_id": "1ed27771-4fba-4efb-ba51-eaf25a346d2f"}, "e833b503-49b1-4528-8f30-bc99b6eaf50e": {"doc_hash": "abebe5475791bb3eef72e1f7639304749f836bc80d8b862d9e364c45ebaf7ee8", "ref_doc_id": "1ed27771-4fba-4efb-ba51-eaf25a346d2f"}, "a8c5a05e-dfd3-404f-80fe-f1ad7052bf23": {"doc_hash": "fd7e65f5f33c02205a47caf73a316a4e0d92b1dbde7151a4b01b6cf7b2f009ba", "ref_doc_id": "211288c5-2a13-4149-b0db-ddb5f946ff17"}, "8a430c60-6f3d-4fdc-b214-c8575cbc2516": {"doc_hash": "572ade1dbe28435dcbdacdc54607bdba46730a7ac8d9b9fbb1fde27cfd48114c", "ref_doc_id": "211288c5-2a13-4149-b0db-ddb5f946ff17"}, "c66aa112-8b1b-44d8-a2df-9b48ee326fa8": {"doc_hash": "a07ad7bf6693cb8fa84db186e2529856135f09b444fcebb5937b796ff59a8a3d", "ref_doc_id": "8d726df1-939d-456c-99cc-0ff9bd6fa13f"}, "b8f2cd42-0ed7-4539-af00-e815bf902a5a": {"doc_hash": "8444ae19dffd66425371d07a02735b03229599ac00c6cae4a58dab8c6278696f", "ref_doc_id": "8d726df1-939d-456c-99cc-0ff9bd6fa13f"}, "e780335a-b086-448b-bfd6-c1234b599d90": {"doc_hash": "043db3f5b19de24cf703f135e0153fb1aac78d228fe3776ff88bfc1851645d4d", "ref_doc_id": "e30dacd8-a1b3-4a64-bdf3-07ac52a35a25"}, "74bf5371-f584-4934-a790-ea07bcfd790e": {"doc_hash": "95266e13a0fe1c695614649c5fdb5e5c0a30888f0b544e222f091e1280c4e2a8", "ref_doc_id": "e30dacd8-a1b3-4a64-bdf3-07ac52a35a25"}, "685d07e4-b022-4a17-a856-7717d4daf536": {"doc_hash": "c678a8006d55b1f84401a1bc35cd8df716b1c9e181565a0b848930844ce9fb55", "ref_doc_id": "7aab6e5a-8007-4760-aa2d-c59bdb1d7cdf"}, "1d3cecd9-7eb6-4659-854a-a5024d6fc392": {"doc_hash": "0a22822ff3956b70671105d7341546da07c0fdfde761366df02be5cf05253319", "ref_doc_id": "7aab6e5a-8007-4760-aa2d-c59bdb1d7cdf"}, "cc72ac2f-18f0-4268-8e58-dcb42dc5214e": {"doc_hash": "1b998a1c635a6e6f37ba7f89b315d33196fc279e78f011728807104b713db787", "ref_doc_id": "eb6d4eb8-f779-4455-ab83-71f04ad97cc4"}, "7db51537-9b7c-4734-a6a1-f79558b98f44": {"doc_hash": "e3810f2c1f6fb66107f6961daf816b77de7ebec3233c880e0406e80a8cc87e2e", "ref_doc_id": "eb6d4eb8-f779-4455-ab83-71f04ad97cc4"}, "c605b43d-3828-46d1-82be-b1228d343cd3": {"doc_hash": "e46cc09dab785648d209584f707eb39364f13d6d11a8cdca86c1949b51b4d1c8", "ref_doc_id": "2ff2ec0f-515d-4cf3-a152-beb3dc3f5428"}, "8c2e0021-e472-4709-8c41-fde361665cc2": {"doc_hash": "1715f1cdd7ae61b2362c99038b8bc9991f6b275af2edf64659ccdffd9e465206", "ref_doc_id": "2ff2ec0f-515d-4cf3-a152-beb3dc3f5428"}, "219c7d5c-8799-4a31-bf1b-4b92afd5b816": {"doc_hash": "d0ea5a25c36a689254df9f752aa832c14d6a5f78136cac3c9e3eb52e5f52a6ec", "ref_doc_id": "fd7cb11f-69ff-4a19-bb3d-6eb2a9d7d6ca"}, "49c016ca-6098-4fb8-a072-8ab16dd6c5de": {"doc_hash": "36d3c46b0631f76e75b5ce6ff6e117d1af1daa78d4dec0feacc44cd088e60aab", "ref_doc_id": "fd7cb11f-69ff-4a19-bb3d-6eb2a9d7d6ca"}, "8f83b4b3-47f1-4757-8858-f93f28b838ea": {"doc_hash": "f0607b92c95b1ed8986c1aad2141e90b562feb36f6fc7e95aee6b793b8829fbf", "ref_doc_id": "8c735fee-441e-4c0c-b648-3f8927f54a84"}, "de8ccd1b-e2d3-490a-9d8a-db24a3092b7d": {"doc_hash": "58ae62bcd4262f8e2b2433c8cb6d56e8b68d26b31116bf7893840d7e8ca26ada", "ref_doc_id": "8c735fee-441e-4c0c-b648-3f8927f54a84"}, "d3f8c1af-7e9a-4b67-9c7b-9246b2116036": {"doc_hash": "c1237047e55ea53e5bf0f45b7e95c915cdfd43d101ebf48b27f0e9e40583f9e5", "ref_doc_id": "45814271-f6ec-4320-b1e3-0ab05b675675"}, "2dc58b05-c1c5-4e89-91c1-e72f86e3aa85": {"doc_hash": "cb3bcda5528496156bdf3c86a9a63a95d7577d129b0ba9756fefce5ee533d13c", "ref_doc_id": "c3fc50ad-4f2d-4c95-ae46-5044eb9ce075"}, "4655e0f3-39c6-4dd4-96d8-353eebc675fa": {"doc_hash": "6c31a1f69c709c3c4eeef472a375655def3ab4d5356ac8c863d1bab3fbf3dd11", "ref_doc_id": "6c02fa2c-5fa8-4429-b452-20051d98420a"}, "57523290-bf7c-4d9d-a849-c81ff114c2e3": {"doc_hash": "664ad2a3729945a551a00bae3e56fd9fcfdd495a6b7384a7c34658e1d25329aa", "ref_doc_id": "fa132b5f-9ded-444a-b92a-c1fd59ea8883"}, "984215be-3ca3-4b90-8c25-b9e4c5b5c2a5": {"doc_hash": "4b5e881cd577b765a3add6099da35e24d3e125c2888510a13076fb8c502d83bc", "ref_doc_id": "92cbcd51-6bba-470b-9f79-b1f11f89181d"}, "077ecf39-393c-46f8-9ac6-384df21fdf5f": {"doc_hash": "5b3dde9f5989282cbb33d3e196d83b6725a2aec4a653805de987a02c323c6de6", "ref_doc_id": "4fa2b42e-4e5a-4f5a-8421-38540b1c6c1b"}, "a33f50d1-5428-413a-8894-75726f2ab16f": {"doc_hash": "df2bdea2b098687d334ebace38c2fbb74cb31d6730c76f60bae81c8252c44852", "ref_doc_id": "4fa2b42e-4e5a-4f5a-8421-38540b1c6c1b"}, "7ee9492a-7a04-41fb-bafa-b6d612cc8650": {"doc_hash": "c989a48fb7da024627bfc24962a9440ff940107b893e2c4dbbdf1bb865de4f26", "ref_doc_id": "95d855cc-70e1-4f45-a1ca-bf68dc39041b"}, "599bcf1e-c86f-4247-8db2-2d4fa38ab8fc": {"doc_hash": "4750c4ad9c9dc9c6776f8eb482ce51552165f355c6ca274b27c24dbf1383d5f9", "ref_doc_id": "379ee6e7-e1e5-4cc1-b19d-b6ba9ee3474d"}, "1998f6c9-cfec-43dc-b82a-7ba1e5207fae": {"doc_hash": "eec028016def7c20491d47c7c771a1686780c450faec5f438e94691708b59305", "ref_doc_id": "379ee6e7-e1e5-4cc1-b19d-b6ba9ee3474d"}, "e0617267-0559-4be0-b685-01ede02a1288": {"doc_hash": "c2071253e8853fcff3112b1e199ac528fd43196e7f6bd76471dcea8251f6e39b", "ref_doc_id": "0e469dfe-ab9a-44e9-8705-5c55f9815440"}, "f291389e-20c8-4879-a191-16a10346f8e1": {"doc_hash": "82fe102837a7d6344f045975dd9192a397d72cca0b7c1afd0d596cf813afa7a3", "ref_doc_id": "0531013c-7af7-43f3-80c9-108a054e3641"}, "e97960c3-17be-4375-a3c9-51895b8a47aa": {"doc_hash": "f203933d7e9c03ed7b8f87ebdad9d9f2eb1bf30fe18d6c044d218c65c3cdfbaa", "ref_doc_id": "0531013c-7af7-43f3-80c9-108a054e3641"}, "27c68578-8d05-4f09-9091-d4dd1d868f56": {"doc_hash": "aeaabeb3f250bbba126b04fe18412c23179213fff9a8cd4082ca671a2d308fe2", "ref_doc_id": "f39d25a6-d2f8-41da-a57e-5675bf769a17"}, "90a63ced-3824-4f8b-900c-5962cb1b1d6e": {"doc_hash": "baf296effc09532f1a817669828d780e36c6956a34815e02d3cacfb486557dc4", "ref_doc_id": "35f12626-eb91-4636-9d1e-b11150a477a1"}, "f26be59c-50a6-4d1b-a6e9-b4615d093fc3": {"doc_hash": "5cea3ef7a090578f5e676c4e2b1ebeae2d1aa1ba9347765a74c3df1a2b0b6949", "ref_doc_id": "fb9635b4-1982-4ae3-b466-0d05e732bb67"}, "65c368e9-5405-4029-b69e-aca2923db5d2": {"doc_hash": "4726fc15d7f48f0d9273d24e057fbe64118f68f1ba9a7c9ad8a96605d96ddd0a", "ref_doc_id": "5b7b49de-1f13-4737-a47c-32ac293626f9"}, "d8c75822-2004-4b11-9fc2-58fad2da8cce": {"doc_hash": "93657660469f4cc780fb465c68426b7c568627c68c48f136405394d7259a9bfb", "ref_doc_id": "cd42df60-847b-4bde-b4cc-5306dfde5f1b"}, "594568b9-6160-41fe-8d4b-1c02fb040bd9": {"doc_hash": "5d3fd7a207fb7770f31a59aa13efc5666d45af976a875a24da9a13430c9cb90e", "ref_doc_id": "725f8c93-52b6-48e3-86a2-8c30d1c419b0"}, "25990a46-6ebb-4a08-831d-62367de64ccd": {"doc_hash": "b7b2e4c414e593a5b5048a461a688968fc3be8db21c1d57bee9ca7ab881df87e", "ref_doc_id": "c7c28efb-12e7-47e1-a7fe-7b2fab2f2107"}, "ff2396d2-24ab-4457-b9cd-99b63f9aa420": {"doc_hash": "612ae75e1a6b1494b6d53e703471ddf876d5d1e4a93a24f580b23704e8caee60", "ref_doc_id": "c7c28efb-12e7-47e1-a7fe-7b2fab2f2107"}, "3e982401-b33f-4181-90b7-10b349e28e39": {"doc_hash": "bf7b2abcaa3818a12fbd82c9085e424b293573a639749eb40460f7993ec12e2e", "ref_doc_id": "6d9917be-8db3-4511-b957-1bcc2b4be680"}, "1a2c8590-90ff-4ca1-915a-45629f0eeaf2": {"doc_hash": "438ea858f094990dd5ace549874102096ab6d4a0913b883847e22927ff30157a", "ref_doc_id": "c844d012-1072-4175-91e7-79f4de4f077a"}, "6e22e3fe-324e-4026-aead-1dee8f8aab59": {"doc_hash": "69715634dd87d8c9f88322f349ecb9831675369e75f3c4e267199750b986f1ed", "ref_doc_id": "b851bf06-f4c9-4140-94b6-dd2b06d017f7"}, "284e0294-fcd7-492b-9a69-a2136ca1ed9e": {"doc_hash": "0891563aea63d092408d7bd82a3e03a3de92e5639340c6fa46ebac663647baf3", "ref_doc_id": "53102d6c-a640-4c4f-baad-a18dbcf4efc7"}, "8964255e-b611-434d-95bb-441c4aca752e": {"doc_hash": "9916a4e36b89c43fab7ba2f352fd8e97b1b45b13b82879365520807d64b1c72f", "ref_doc_id": "53102d6c-a640-4c4f-baad-a18dbcf4efc7"}, "d62e0cf2-f0eb-4f5b-ac56-124b06b0a874": {"doc_hash": "a865f2f34c6149c1d241006eac3c73662e168047165d55a5cd62002a0a7f48d5", "ref_doc_id": "3d542561-e275-4984-9121-7a6ad0e6bf52"}, "b212b645-338d-4d26-a9e6-3df4f47a738b": {"doc_hash": "d5de3ecefc3c6c89f00f4eabb2582e828c3144c172a4c92437e5352e2c9c9de5", "ref_doc_id": "3d542561-e275-4984-9121-7a6ad0e6bf52"}, "33307481-bd83-4f83-899b-72fefe367f1d": {"doc_hash": "d35c3c27dbb0afe32745c39e34ae9e2cc8ab63e26351782722a8c7a3f64db23e", "ref_doc_id": "b54e3c4b-2163-4710-8425-e7ea55298923"}, "54257664-c5a5-42d6-86d0-67b5aeef3c71": {"doc_hash": "7feab313c7daebb8724ad49e74a69390c0dff4d96732072a32962800518e00bb", "ref_doc_id": "e5dd65d2-5eda-4373-a9f5-258e655492bf"}, "03dcabff-c43a-4000-ae85-f2e5bed6d276": {"doc_hash": "59be3c6592e732852f5668a4ecb17ba8b3b33b1a0f6c92c0998bbce2dfac22c6", "ref_doc_id": "e5dd65d2-5eda-4373-a9f5-258e655492bf"}, "fd9b2d05-26a2-4463-999d-9740e106b536": {"doc_hash": "9a8d237078bf448230f591f6d8d402c2e2ecd03b9db0e8368de7fec6c21e368e", "ref_doc_id": "1bf610f3-69fc-4de9-ae7c-b738a8f645da"}, "d9df8ad8-ef33-47cb-ae97-8d1696759811": {"doc_hash": "af46d68d35c9c9e382ac0f37929098e0bb84837393f617d75700d4accf30dd49", "ref_doc_id": "1bf610f3-69fc-4de9-ae7c-b738a8f645da"}, "737463ba-c93f-4fdd-93fc-eeaadf7111d3": {"doc_hash": "28aa8ca5925b775d1e41d29dbda13f7bf18161236e7684c9ffb3b00e3e3befda", "ref_doc_id": "bde80ebd-a216-4211-afd9-b0a403478f83"}, "1e091581-56aa-449a-ab80-da628374840e": {"doc_hash": "b5d92b18e621402768dff1612f3c9280cdcb7a6c5d8ddc09ba45297f172d2005", "ref_doc_id": "bde80ebd-a216-4211-afd9-b0a403478f83"}, "8c26a7c8-80cd-4a68-ba33-57259c3d5a38": {"doc_hash": "747f540377dafeb78e136d6bfcd1a8fb71237c51236b106ddb9294d5ddd6c702", "ref_doc_id": "9bb0cde4-e229-43e8-8579-490e9d192022"}, "e33e0277-18f8-4e89-8410-012916491129": {"doc_hash": "f41a3bd1f658ac78c3d4b427ae7e08901df912e395d939d233e84fb7b922a130", "ref_doc_id": "409bb29e-17e8-420e-833e-9d8b18248de1"}, "5c5934bf-0e79-4464-a754-fcbc3eae02aa": {"doc_hash": "9a3f8650de7de3c7293eceae77b470e879c38cd81366f7e2b9d76d6e843438ca", "ref_doc_id": "409bb29e-17e8-420e-833e-9d8b18248de1"}, "4656f740-93db-4aa3-9dd4-2be1a42592f7": {"doc_hash": "38d986503e7cceaabc8d64c55475727ad01d73dcb8079690a23d1a8ba4c3faef", "ref_doc_id": "a31a05fe-7eca-465b-85da-4c39b0936020"}, "9b18740e-2bfd-4708-9bd9-ae593d739ca0": {"doc_hash": "edd31adcc456970d9d43ed19036c3cf36a3a239377cfe6b2b029f8d17c3e9a94", "ref_doc_id": "249f8562-a7a5-40a4-a029-700ee5f1463d"}, "4471ac0f-b54c-44bb-b02d-2977f49a2741": {"doc_hash": "a6023d0c7b2824a61a88497dec9a049798f34cf1c2b6be8509e7a6f6ce1e2194", "ref_doc_id": "d55e4055-e446-4d88-b7cb-b108aab513ed"}, "887e64dd-b920-4466-857d-d700093d6ec0": {"doc_hash": "935ce458e5aeb468b174825a6facb6d9c9928c359befb36177dcaf18f0674d43", "ref_doc_id": "0b61cf34-83e5-4ce7-9f95-da6fd850700d"}, "009a1bab-df08-4165-8996-45bd55b69613": {"doc_hash": "fc9dfd38f42ce5af0e63598945cbec782b35eae79285edabc8c0571d31478d98", "ref_doc_id": "718e8467-1957-4cb9-b710-f7a18ce719ca"}, "fde697f6-1d81-4bd9-8fe8-f767145faa6a": {"doc_hash": "2348c478f3dba292cbbceaa0dab3781b342121f12682ac9ec7a3ea8dbb00f14a", "ref_doc_id": "718e8467-1957-4cb9-b710-f7a18ce719ca"}, "a0e80c68-1ca3-4022-9f8b-10fd06558589": {"doc_hash": "17ce452a7d8e92468afdccb948a9a743f873d3a5958c6ce8eb04c59d18c9fd68", "ref_doc_id": "718e8467-1957-4cb9-b710-f7a18ce719ca"}, "6c660e4a-385f-4979-bcee-989ff420df3e": {"doc_hash": "2ec4e62509c7e2f12cbecb2cd1e1724c70c1af32b8f84f1798c1e16190a7a919", "ref_doc_id": "718e8467-1957-4cb9-b710-f7a18ce719ca"}, "b46d7635-7b6b-4ca3-bbed-c1cecb14b876": {"doc_hash": "92f853bacc2b23a7d0f6f4108771edf33b3b29a5c62f29ae79664cc3fb06ab99", "ref_doc_id": "f84bfaf7-4c6b-48bc-a7f7-8251d9c317fe"}, "a0faf4e5-0f00-43e0-9757-652488968f51": {"doc_hash": "200ac516fb1ac5b80a07de90070a23f84060d4994135e6e57f2a70792409b1f5", "ref_doc_id": "f84bfaf7-4c6b-48bc-a7f7-8251d9c317fe"}, "035b4efb-be0e-4e9d-89b2-fd2185c25560": {"doc_hash": "8d7c55d7c1990671ed68dc82134c8fffd753c3b342572fad3f3d7e351f8b9e3e", "ref_doc_id": "f84bfaf7-4c6b-48bc-a7f7-8251d9c317fe"}, "ea429a41-eada-43d8-85de-173ac3aaa37e": {"doc_hash": "7dbfdccf185fafa44f6bc6424cbfef195f2c80d86666fe1772164a4ea9123668", "ref_doc_id": "f84bfaf7-4c6b-48bc-a7f7-8251d9c317fe"}, "03da3e55-13b3-4f33-8a85-3988c6f51815": {"doc_hash": "19587d8f5a8cff0f627eddf091d26509446622f0f08b5fdd1197efa4ee29f1d0", "ref_doc_id": "920f1b13-bf7c-4e18-a5ce-3c0b6d700a7f"}, "cfc0400c-2177-4553-b7eb-8ca3caa69b43": {"doc_hash": "f0a4ffbc753df4ec593ed95e060dc6c57b722817ec88b34c376ac4381e31d19b", "ref_doc_id": "920f1b13-bf7c-4e18-a5ce-3c0b6d700a7f"}, "e54686e1-12f7-444a-9857-9d9b3443b52a": {"doc_hash": "142b7a52d37e7b8edca2902d73124ded3c95bb9b926f7c5e96c762aeb6388d44", "ref_doc_id": "920f1b13-bf7c-4e18-a5ce-3c0b6d700a7f"}, "6a81adbc-12c6-4168-bb07-352586f3b64f": {"doc_hash": "39b7c870bd8270a6fad0089cafe0725e17a02064db1e988edd1365460d3b5f8e", "ref_doc_id": "920f1b13-bf7c-4e18-a5ce-3c0b6d700a7f"}, "b539a732-b2fd-463a-b408-e1325681b672": {"doc_hash": "2ba1f9f2e061d872551ef85dd2d07fc6ab00d82d1031d03b0ae908ca37a3b063", "ref_doc_id": "a0a69297-355a-4dc3-bb23-e9e52f949efa"}, "845f08f6-836d-4cc9-b44b-3c25a42e54c8": {"doc_hash": "fbc15afb32fa6966b51532dce3c142ed5176bf34376c2c331c06a76a387a21e5", "ref_doc_id": "a0a69297-355a-4dc3-bb23-e9e52f949efa"}, "2b55c0e3-dfd0-4c84-9aa7-5b124898848a": {"doc_hash": "6c50d3462026567a5cb31c78fc70c7f6e0409f170791fc7e6153ecf64c9cec8b", "ref_doc_id": "a0a69297-355a-4dc3-bb23-e9e52f949efa"}, "450eb635-f69a-4acf-8167-9e9f9936ae29": {"doc_hash": "eb820085da4fedb603ea9e7b62fb3084750f08e361058f115aa484da11e1520e", "ref_doc_id": "a0a69297-355a-4dc3-bb23-e9e52f949efa"}, "bbe23339-0b40-4b8f-8e44-88c2eae299fb": {"doc_hash": "2c2846f973c4ee76224388f1e0d2f648e9dc68aefba15597ba5645678e9bcacc", "ref_doc_id": "31cc409f-9256-431c-8121-9c5756707107"}}, "docstore/data": {"43cbcae8-6cc7-4c56-b367-e7d073c73e2e": {"__data__": {"id_": "43cbcae8-6cc7-4c56-b367-e7d073c73e2e", "embedding": null, "metadata": {"page_label": "1", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3300603e-6396-4503-ace0-497dcd8029cf", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "442409102d71f8083d5c176cd680970a326a79b80f4e3bbc2c5db81f28a68937", "class_name": "RelatedNodeInfo"}}, "text": "Retrieval-Augmented Generation for\nKnowledge-Intensive NLP Tasks\nPatrick Lewis\u2020\u2021, Ethan Perez\u22c6,\nAleksandra Piktus\u2020, Fabio Petroni\u2020, Vladimir Karpukhin\u2020, Naman Goyal\u2020, Heinrich K\u00fcttler\u2020,\nMike Lewis\u2020, Wen-tau Yih\u2020, Tim Rockt\u00e4schel\u2020\u2021, Sebastian Riedel\u2020\u2021, Douwe Kiela\u2020\n\u2020Facebook AI Research; \u2021University College London; \u22c6New York University;\nplewis@fb.com\nAbstract\nLarge pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when \ufb01ne-tuned on down-\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\nedge is still limited, and hence on knowledge-intensive tasks, their performance\nlags behind task-speci\ufb01c architectures. Additionally, providing provenance for their\ndecisions and updating their world knowledge remain open research problems. Pre-\ntrained models with a differentiable access mechanism to explicit non-parametric\nmemory have so far been only investigated for extractive downstream tasks. We\nexplore a general-purpose \ufb01ne-tuning recipe for retrieval-augmented generation\n(RAG) \u2014 models which combine pre-trained parametric and non-parametric mem-\nory for language generation. We introduce RAG models where the parametric\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\npare two RAG formulations, one which conditions on the same retrieved passages\nacross the whole generated sequence, and another which can use different passages\nper token. We \ufb01ne-tune and evaluate our models on a wide range of knowledge-\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\noutperforming parametric seq2seq models and task-speci\ufb01c retrieve-and-extract\narchitectures. For language generation tasks, we \ufb01nd that RAG models generate\nmore speci\ufb01c, diverse and factual language than a state-of-the-art parametric-only\nseq2seq baseline.\n1 Introduction\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\nedge from data [47]. They can do so without any access to an external memory, as a parameterized\nimplicit knowledge base [51, 52]. While this development is exciting, such models do have down-\nsides: They cannot easily expand or revise their memory, can\u2019t straightforwardly provide insight into\ntheir predictions, and may produce \u201challucinations\u201d [38]. Hybrid models that combine parametric\nmemory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\ninspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\ncombine masked language models [8] with a differentiable retriever, have shown promising results,\narXiv:2005.11401v4  [cs.CL]  12 Apr 2021", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2900, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b2d001f-00fc-4be0-914f-6d363e110b28": {"__data__": {"id_": "3b2d001f-00fc-4be0-914f-6d363e110b28", "embedding": null, "metadata": {"page_label": "2", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0cd449d0-3faf-43e2-8997-77a60dd8bc6e", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "6af96f779b3ab025a1d8a3558ccd1e0db4b7c047a7e50a3a75b344d54e8d51cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "852669ea-9eaa-4666-959f-feeeef0043d4", "node_type": "1", "metadata": {}, "hash": "1bba58513a7e005efb5d61efaf03da03da7e674659480554fafc481022eaea0b", "class_name": "RelatedNodeInfo"}}, "text": "The\tDivine\nComedy\t(x) q \nQuery \nEncoder \nq(x) \nMIPS p \u03b8 \nGenerator\u00a0p\u03b8\n(Parametric) \nMargin- \nalize \nThis\t14th\tcentury\twork\nis\tdivided\tinto\t3\nsections:\t\"Inferno\",\n\"Purgatorio\"\t&\n\"Paradiso\"\t\t\t\t\t\t\t\t\t(y)\nEnd-to-End Backprop through q  and\u00a0p \u03b8 \nBarack\tObama\twas\nborn\tin\tHawaii.(x)\nFact Veri\ufb01cation: Fact Query\nsupports\t(y)\nQuestion Generation\nFact Veri\ufb01cation:\nLabel Generation\nDocument \nIndex \nDefine\t\"middle\tear\"(x)\nQuestion Answering:\nQuestion Query\nThe\tmiddle\tear\tincludes\nthe\ttympanic\tcavity\tand\nthe\tthree\tossicles.\t\t(y)\nQuestion Answering:\nAnswer GenerationRetriever p\u03b7 \n(Non-Parametric) \nz 4 \nz 3 \nz 2 \nz 1 \nd(z) \nJeopardy Question\nGeneration:\nAnswer Query\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document\nIndex) with a pre-trained seq2seq model (Generator) and \ufb01ne-tune end-to-end. For query x , we use\nMaximum Inner Product Search (MIPS) to \ufb01nd the top-K documents z i . For \ufb01nal prediction y , we\ntreat z as a latent variable and marginalize over seq2seq predictions given different documents.\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric\nand non-parametric memory to the \u201cworkhorse of NLP,\u201d i.e. sequence-to-sequence (seq2seq) models.\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\na general-purpose \ufb01ne-tuning approach which we refer to as retrieval-augmented generation (RAG).\nWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\nretriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on\nthe input, and the seq2seq model (BART [32]) then conditions on these latent documents together with\nthe input to generate the output. We marginalize the latent documents with a top-K approximation,\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\nbasis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG\ncan be \ufb01ne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric\nmemory which are trained from scratch for speci\ufb01c tasks, e.g. memory networks [ 64, 55], stack-\naugmented networks [25] and memory layers [ 30]. In contrast, we explore a setting where both\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\npresent without additional training.\nOur results highlight the bene\ufb01ts of combining parametric and non-parametric memory with genera-\ntion for knowledge-intensive tasks\u2014tasks that humans could not reasonably be expected to perform\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\non open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\nextractive tasks, we \ufb01nd that unconstrained generation outperforms previous extractive approaches.\nFor knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\ngeneration, and we \ufb01nd that our models generate responses that are more factual, speci\ufb01c, and\ndiverse than a BART baseline. For FEVER [56] fact veri\ufb01cation, we achieve results within 4.3% of\nstate-of-the-art pipeline models which use strong retrieval supervision.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3774, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "852669ea-9eaa-4666-959f-feeeef0043d4": {"__data__": {"id_": "852669ea-9eaa-4666-959f-feeeef0043d4", "embedding": null, "metadata": {"page_label": "2", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0cd449d0-3faf-43e2-8997-77a60dd8bc6e", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "6af96f779b3ab025a1d8a3558ccd1e0db4b7c047a7e50a3a75b344d54e8d51cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b2d001f-00fc-4be0-914f-6d363e110b28", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "87da3ecd14b5a355563536b799c427b339a42a19dc4a56548d5cb586333ee263", "class_name": "RelatedNodeInfo"}}, "text": "Our RAG models achieve state-of-the-art results\non open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\nextractive tasks, we \ufb01nd that unconstrained generation outperforms previous extractive approaches.\nFor knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\ngeneration, and we \ufb01nd that our models generate responses that are more factual, speci\ufb01c, and\ndiverse than a BART baseline. For FEVER [56] fact veri\ufb01cation, we achieve results within 4.3% of\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\nthe non-parametric memory can be replaced to update the models\u2019 knowledge as the world changes.1\n2 Methods\nWe explore RAG models, which use the input sequencex to retrieve text documents z and use them\nas additional context when generating the target sequence y . As shown in Figure 1, our models\nleverage two components: (i) a retriever p \u03b7(z |x ) with parameters \u03b7 that returns (top-K truncated)\ndistributions over text passages given a query x and (ii) a generator p \u03b8(y i |x,z,y 1:i \u22121) parametrized\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\ners Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/\nexamples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\n2", "mimetype": "text/plain", "start_char_idx": 3080, "end_char_idx": 4590, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd63402e-3d67-4c55-a6ca-534e31dceb92": {"__data__": {"id_": "dd63402e-3d67-4c55-a6ca-534e31dceb92", "embedding": null, "metadata": {"page_label": "3", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36163862-d97e-44e2-9afa-b6588cc4e49b", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "ecb3ff08dfa1d96d367754c4fa222eab039c6d8407cedd4d8b3ce55d7ad8e80b", "class_name": "RelatedNodeInfo"}}, "text": "by \u03b8that generates a current token based on a context of the previous i\u22121 tokens y1:i\u22121, the original\ninput xand a retrieved passage z.\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.\nWe propose two models that marginalize over the latent documents in different ways to produce a\ndistribution over generated text. In one approach, RAG-Sequence, the model uses the same document\nto predict each target token. The second approach, RAG-Token, can predict each target token based\non a different document. In the following, we formally introduce both models and then describe the\np\u03b7 and p\u03b8 components, as well as the training and decoding procedure.\n2.1 Models\nRAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate\nthe complete sequence. Technically, it treats the retrieved document as a single latent variable that\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the\ntop K documents are retrieved using the retriever, and the generator produces the output sequence\nprobability for each document, which are then marginalized,\npRAG-Sequence(y|x) \u2248\n\u2211\nz\u2208top-k(p(\u00b7|x))\np\u03b7(z|x)p\u03b8(y|x,z) =\n\u2211\nz\u2208top-k(p(\u00b7|x))\np\u03b7(z|x)\nN\u220f\ni\np\u03b8(yi|x,z,y 1:i\u22121)\nRAG-Token Model In the RAG-Token model we can draw a different latent document for each\ntarget token and marginalize accordingly. This allows the generator to choose content from several\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\nretriever, and then the generator produces a distribution for the next output token for each document,\nbefore marginalizing, and repeating the process with the following output token, Formally, we de\ufb01ne:\npRAG-Token(y|x) \u2248\nN\u220f\ni\n\u2211\nz\u2208top-k(p(\u00b7|x))\np\u03b7(z|x)p\u03b8(yi|x,z,y 1:i\u22121)\nFinally, we note that RAG can be used for sequence classi\ufb01cation tasks by considering the target class\nas a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\n2.2 Retriever: DPR\nThe retrieval component p\u03b7(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\np\u03b7(z|x) \u221dexp\n(\nd(z)\u22a4q(x)\n)\nd(z) =BERTd(z), q(x) =BERTq(x)\nwhere d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],\nand q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating\ntop-k(p\u03b7(\u00b7|x)), the list of kdocuments zwith highest prior probability p\u03b7(z|x), is a Maximum Inner\nProduct Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use\na pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\nretriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and\nNatural Questions [29]. We refer to the document index as the non-parametric memory.\n2.3 Generator: BART\nThe generator component p\u03b8(yi|x,z,y 1:i\u22121) could be modelled using any encoder-decoder. We use\nBART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input\nxwith the retrieved content zwhen generating from BART, we simply concatenate them. BART was\npre-trained using a denoising objective and a variety of different noising functions. It has obtained\nstate-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5\nmodels [32]. We refer to the BART generator parameters \u03b8as the parametric memory henceforth.\n2.4 Training\nWe jointly train the retriever and generator components without any direct supervision on what\ndocument should be retrieved. Given a \ufb01ne-tuning training corpus of input/output pairs (xj,yj), we\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d3ef193-bd71-4d92-a043-52f2543179e2": {"__data__": {"id_": "2d3ef193-bd71-4d92-a043-52f2543179e2", "embedding": null, "metadata": {"page_label": "4", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f45d831-0530-4252-970d-3f3cf0636715", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "9ade8e9aa8648e4056ad4de37c49debb4cdd84d50e2a649c128a976b27cfe206", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d037085-21aa-40c4-9af7-09e68658c56c", "node_type": "1", "metadata": {}, "hash": "2a09242ac392501bba75166ea45909eacf9f8b3138fa37a0686f45ad1caaf656", "class_name": "RelatedNodeInfo"}}, "text": "minimize the negative marginal log-likelihood of each target, \u2211\nj\u2212log p(yj|xj) using stochastic\ngradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as\nit requires the document index to be periodically updated as REALM does during pre-training [20].\nWe do not \ufb01nd this step necessary for strong performance, and keep the document encoder (and\nindex) \ufb01xed, only \ufb01ne-tuning the query encoder BERTq and the BART generator.\n2.5 Decoding\nAt test time, RAG-Sequence and RAG-Token require different ways to approximatearg maxyp(y|x).\nRAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\ntor with transition probability: p\u2032\n\u03b8(yi|x,y1:i\u22121) = \u2211\nz\u2208top-k(p(\u00b7|x)) p\u03b7(zi|x)p\u03b8(yi|x,zi,y1:i\u22121) To\ndecode, we can plug p\u2032\n\u03b8(yi|x,y1:i\u22121) into a standard beam decoder.\nRAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-\ntoken likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\neach document z, scoring each hypothesis using p\u03b8(yi|x,z,y 1:i\u22121). This yields a set of hypotheses\nY, some of which may not have appeared in the beams of all documents. To estimate the probability\nof an hypothesis y we run an additional forward pass for each document z for which y does not\nappear in the beam, multiply generator probability with p\u03b7(z|x) and then sum the probabilities across\nbeams for the marginals. We refer to this decoding procedure as \u201cThorough Decoding.\u201d For longer\noutput sequences, |Y|can become large, requiring many forward passes. For more ef\ufb01cient decoding,\nwe can make a further approximation that p\u03b8(y|x,zi) \u22480 where ywas not generated during beam\nsearch from x,zi. This avoids the need to run additional forward passes once the candidate set Y has\nbeen generated. We refer to this decoding procedure as \u201cFast Decoding.\u201d\n3 Experiments\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint\n100-word chunks, to make a total of 21M documents. We use the document encoder to compute an\nembedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical\nNavigable Small World approximation for fast retrieval [37]. During training, we retrieve the top\nkdocuments for each query. We consider k\u2208{5,10}for training and set kfor test time using dev\ndata. We now discuss experimental details for each task.\n3.1 Open-domain Question Answering\nOpen-domain question answering (QA) is an important real-world application and common testbed\nfor knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)\nand train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\nthe popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved\ndocuments, relying primarily on non-parametric knowledge. We also compare to \u201cClosed-Book\nQA\u201d approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead\nrelying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural\nQuestions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As\nCT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG\nmodel. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3691, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d037085-21aa-40c4-9af7-09e68658c56c": {"__data__": {"id_": "1d037085-21aa-40c4-9af7-09e68658c56c", "embedding": null, "metadata": {"page_label": "4", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f45d831-0530-4252-970d-3f3cf0636715", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "9ade8e9aa8648e4056ad4de37c49debb4cdd84d50e2a649c128a976b27cfe206", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d3ef193-bd71-4d92-a043-52f2543179e2", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "85aac0ea0a318c3c699fd4534a779814dcbc134ba644f2d454dc8551d39e0e8e", "class_name": "RelatedNodeInfo"}}, "text": "We also compare to \u201cClosed-Book\nQA\u201d approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead\nrelying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural\nQuestions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As\nCT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG\nmodel. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\n3.2 Abstractive Question Answering\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive\ntext generation. To test RAG\u2019s natural language generation (NLG) in a knowledge-intensive setting,\nwe use the MSMARCO NLG task v2.1 [ 43]. The task consists of questions, ten gold passages\nretrieved from a search engine for each question, and a full sentence answer annotated from the\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\n4", "mimetype": "text/plain", "start_char_idx": 3098, "end_char_idx": 4206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a730939c-ce27-4c35-9d7c-eb55921c6671": {"__data__": {"id_": "a730939c-ce27-4c35-9d7c-eb55921c6671", "embedding": null, "metadata": {"page_label": "5", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3c86b19-ae2a-4cbf-b2dc-cbc196285c11", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "b8b076e3679238b38a7eada70e64297b1060fb53aab2fe2f3feadec9038779e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09763838-24c2-4eb1-9e1b-4eac3aff565e", "node_type": "1", "metadata": {}, "hash": "9a22a7591e1da05d693d2336ecab8538f9ffc89840fdff80eaa834360984c7c4", "class_name": "RelatedNodeInfo"}}, "text": "MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be\nanswered in a way that matches the reference answer without access to the gold passages, such as\n\u201cWhat is the weather in V olcano, CA?\u201d so performance will be lower without using gold passages.\nWe also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,\nRAG can rely on parametric knowledge to generate reasonable responses.\n3.3 Jeopardy Question Generation\nTo evaluate RAG\u2019s generation abilities in a non-QA setting, we study open-domain question gen-\neration. Rather than use questions from standard open-domain QA tasks, which typically consist\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions.\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.\nFor example, \u201cThe World Cup\u201d is the answer to the question \u201cIn 1986 Mexico scored as the \ufb01rst\ncountry to host this international sports competition twice.\u201d As Jeopardy questions are precise,\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\nchallenging knowledge-intensive generation task.\nWe use the splits from SearchQA [ 10], with 100K train, 14K dev, and 27K test examples. As\nthis is a new task, we train a BART model for comparison. Following [67], we evaluate using the\nSQuAD-tuned Q-BLEU-1 metric [ 42]. Q-BLEU is a variant of BLEU with a higher weight for\nmatching entities and has higher correlation with human judgment for question generation than\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\none for speci\ufb01city. We de\ufb01ne factuality as whether a statement can be corroborated by trusted external\nsources, and speci\ufb01city as high mutual dependence between the input and output [ 33]. We follow\nbest practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two\ngenerated questions, one from BART and one from RAG. They are then asked to pick one of four\noptions\u2014quuestion A is better, question B is better, both are good, or neither is good.\n3.4 Fact Veri\ufb01cation\nFEVER [ 56] requires classifying whether a natural language claim is supported or refuted by\nWikipedia, or whether there is not enough information to decide. The task requires retrieving\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\nwhether the claim is true, false, or unveri\ufb01able from Wikipedia alone. FEVER is a retrieval problem\ncoupled with an challenging entailment reasoning task. It also provides an appropriate testbed for\nexploring the RAG models\u2019 ability to handle classi\ufb01cation rather than generation. We map FEVER\nclass labels (supports, refutes, or not enough info) to single output tokens and directly train with\nclaim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on\nretrieved evidence. In many real-world applications, retrieval supervision signals aren\u2019t available, and\nmodels that do not require such supervision will be applicable to a wider range of tasks. We explore\ntwo variants: the standard 3-way classi\ufb01cation task (supports/refutes/not enough info) and the 2-way\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\n4 Results\n4.1 Open-domain Question Answering\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\nthe generation \ufb02exibility of the \u201cclosed-book\u201d (parametric only) approaches and the performance of\n\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\nwithout expensive, specialized \u201csalient span masking\u201d pre-training [20]. It is worth noting that RAG\u2019s\nretriever is initialized using DPR\u2019s retriever, which uses retrieval supervision on Natural Questions\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based \u201ccross-\nencoder\u201d to re-rank documents, along with an extractive reader.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4141, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09763838-24c2-4eb1-9e1b-4eac3aff565e": {"__data__": {"id_": "09763838-24c2-4eb1-9e1b-4eac3aff565e", "embedding": null, "metadata": {"page_label": "5", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3c86b19-ae2a-4cbf-b2dc-cbc196285c11", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "b8b076e3679238b38a7eada70e64297b1060fb53aab2fe2f3feadec9038779e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a730939c-ce27-4c35-9d7c-eb55921c6671", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "0a45e308d4ab5bb806455c8fa4845734498ad63ec03241912633d037639061e7", "class_name": "RelatedNodeInfo"}}, "text": "4 Results\n4.1 Open-domain Question Answering\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\nthe generation \ufb02exibility of the \u201cclosed-book\u201d (parametric only) approaches and the performance of\n\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\nwithout expensive, specialized \u201csalient span masking\u201d pre-training [20]. It is worth noting that RAG\u2019s\nretriever is initialized using DPR\u2019s retriever, which uses retrieval supervision on Natural Questions\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based \u201ccross-\nencoder\u201d to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\nre-ranker nor extractive reader is necessary for state-of-the-art performance.\nThere are several advantages to generating answers even when it is possible to extract them. Docu-\nments with clues about the answer but do not contain the answer verbatim can still contribute towards\na correct answer being generated, which is not possible with standard extractive approaches, leading\n5", "mimetype": "text/plain", "start_char_idx": 3358, "end_char_idx": 4556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5b522c3-0b03-41d8-9c58-af812f9d2cc9": {"__data__": {"id_": "b5b522c3-0b03-41d8-9c58-af812f9d2cc9", "embedding": null, "metadata": {"page_label": "6", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "abd720f7-e7c7-4f0c-90b0-55c5c600a64a", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "78eac3543db985b49a47e38d04f8a7abc25320c78b40fe39387c3f002d05e794", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d378120f-4074-4020-8adf-dd9cb539b3a0", "node_type": "1", "metadata": {}, "hash": "c9796ed1a9fbc44bf2c953f3a0c9dc10d1fdc13e01da5c897c0bd2b6aa3a210e", "class_name": "RelatedNodeInfo"}}, "text": "Table 1: Open-Domain QA Test Scores. For TQA,\nleft column uses the standard test set for Open-\nDomain QA, right column uses the TQA-Wiki\ntest set. See Appendix D for further details.\nModel NQ TQA WQ CT\nClosed\nBook\nT5-11B [52] 34.5 - /50.1 37.4 -\nT5-11B+SSM[52] 36.6 - /60.5 44.7 -\nOpen\nBook\nREALM [20] 40.4 - / - 40.7 46.8\nDPR [26] 41.5 57.9/ - 41.1 50.6\nRAG-Token 44.1 55.2/66.1 45.5 50.0\nRAG-Seq. 44.5 56.8/68.0 45.2 52.2\nTable 2: Generation and classi\ufb01cation Test Scores.\nMS-MARCO SotA is [4], FEVER-3 is [68] and\nFEVER-2 is [ 57] *Uses gold context/evidence.\nBest model without gold access underlined.\nModel Jeopardy MSMARCO FVR3 FVR2\nB-1 QB-1 R-L B-1 Label Acc.\nSotA - - 49.8* 49.9* 76.8 92.2 *\nBART 15.1 19.7 38.2 41.6 64.0 81.1\nRAG-Tok. 17.3 22.2 40.1 41.5 72.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers\neven when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such\ncases for NQ, where an extractive model would score 0%.\n4.2 Abstractive Question Answering\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\nimpressive given that (i) those models access gold passages with speci\ufb01c information required to\ngenerate the reference answer , (ii) many questions are unanswerable without the gold passages, and\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\nfrom our models. Qualitatively, we \ufb01nd that RAG models hallucinate less and generate factually\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\nBART generations (see \u00a74.5).\n4.3 Jeopardy Question Generation\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\nwith both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\npairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\nthan RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and\nBART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\nthe task over a state-of-the-art generation model. Evaluators also \ufb01nd RAG generations to be more\nspeci\ufb01c by a large margin. Table 3 shows typical generations from each model.\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform\nbest because it can generate responses that combine content from several documents. Figure 2 shows\nan example. When generating \u201cSun\u201d, the posterior is high for document 2 which mentions \u201cThe\nSun Also Rises\u201d. Similarly, document 1 dominates the posterior when \u201cA Farewell to Arms\u201d is\ngenerated. Intriguingly, after the \ufb01rst token of each book is generated, the document posterior \ufb02attens.\nThis observation suggests that the generator can complete the titles without depending on speci\ufb01c\ndocuments. In other words, the model\u2019s parametric knowledge is suf\ufb01cient to complete the titles.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3135, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d378120f-4074-4020-8adf-dd9cb539b3a0": {"__data__": {"id_": "d378120f-4074-4020-8adf-dd9cb539b3a0", "embedding": null, "metadata": {"page_label": "6", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "abd720f7-e7c7-4f0c-90b0-55c5c600a64a", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "78eac3543db985b49a47e38d04f8a7abc25320c78b40fe39387c3f002d05e794", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5b522c3-0b03-41d8-9c58-af812f9d2cc9", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "91d0742f47b2ea8732b0df454fe98c905dd28638a57b22b4e3c514ac82909363", "class_name": "RelatedNodeInfo"}}, "text": "Evaluators also \ufb01nd RAG generations to be more\nspeci\ufb01c by a large margin. Table 3 shows typical generations from each model.\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform\nbest because it can generate responses that combine content from several documents. Figure 2 shows\nan example. When generating \u201cSun\u201d, the posterior is high for document 2 which mentions \u201cThe\nSun Also Rises\u201d. Similarly, document 1 dominates the posterior when \u201cA Farewell to Arms\u201d is\ngenerated. Intriguingly, after the \ufb01rst token of each book is generated, the document posterior \ufb02attens.\nThis observation suggests that the generator can complete the titles without depending on speci\ufb01c\ndocuments. In other words, the model\u2019s parametric knowledge is suf\ufb01cient to complete the titles. We\n\ufb01nd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding\"The\nSun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun\nAlso Rises\" indicating the title \"The Sun Also Rises\" is stored in BART\u2019s parameters. Similarly,\nBART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A\nwith \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows\nhow parametric and non-parametric memories work together\u2014the non-parametric component helps\nto guide the generation, drawing out speci\ufb01c knowledge stored in the parametric memory.\n4.4 Fact Veri\ufb01cation\nTable 2 shows our results on FEVER. For 3-way classi\ufb01cation, RAG scores are within 4.3% of\nstate-of-the-art models, which are complex pipeline systems with domain-speci\ufb01c architectures and\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\n6", "mimetype": "text/plain", "start_char_idx": 2333, "end_char_idx": 4110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02110f77-ff94-4534-b3dd-7c4c7c0976c2": {"__data__": {"id_": "02110f77-ff94-4534-b3dd-7c4c7c0976c2", "embedding": null, "metadata": {"page_label": "7", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1aaab2c-7d44-497e-8cc6-6ec01aaf2b67", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "a66238ad15a00535a8833526e2292928efc7de240331969fc8b8225f27e601ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "361afec3-2d22-46ef-8e45-b692f342f90e", "node_type": "1", "metadata": {}, "hash": "70f23b15c7328a70724bac0beec839f3d431c2351858369c7caf97afd16398b2", "class_name": "RelatedNodeInfo"}}, "text": "Document 1: his works are considered classics of American\nliterature ... His wartime experiences formed the basis for his novel\n\u201dA Farewell to Arms\u201d(1929) ...\nDocument 2: ... artists of the 1920s \u201dLost Generation\u201d expatriate\ncommunity. His debut novel,\u201dThe Sun Also Rises\u201d, was published\nin 1926.\nBOS\n\u201d\nTheSunAlso\nR ises\n\u201d is a\nnovel\nby this\nauthor\nof \u201d A\nFarewellto\nArms\n\u201d\nDoc 1\nDoc 2\nDoc 3\nDoc 4\nDoc 5\nFigure 2: RAG-Token document posterior p(zi|x,yi,y\u2212i) for each generated token for input \u201cHem-\ningway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\nwhen generating \u201cA Farewell to Arms\" and for document 2 when generating \u201cThe Sun Also Rises\".\nTable 3: Examples from generation tasks. RAG models generate more speci\ufb01c and factually accurate\nresponses. \u2018?\u2019 indicates factually incorrect responses, * indicates partially correct responses.\nTask Input Model Generation\nMS-\nMARCO\nde\ufb01ne middle\near\nBART ?The middle ear is the part of the ear between the middle ear and the nose.\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\nwhat currency\nneeded in\nscotland\nBART The currency needed in Scotland is Pound sterling.\nRAG-T Pound is the currency needed in Scotland.\nRAG-S The currency needed in Scotland is the pound sterling.\nJeopardy\nQuestion\nGener\n-ation\nWashington\nBART ?This state has the largest number of counties in the U.S.\nRAG-T It\u2019s the only U.S. state named for a U.S. president\nRAG-S It\u2019s the state where you\u2019ll \ufb01nd Mount Rainier National Park\nThe Divine\nComedy\nBART *This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\nRAG-T Dante\u2019s \"Inferno\" is the \ufb01rst part of this epic poem\nRAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"\nFor 2-way classi\ufb01cation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\nevidence in FEVER. We calculate the overlap in article titles between the topkdocuments retrieved\nby RAG and gold evidence annotations. We \ufb01nd that the top retrieved document is from a gold article\nin 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n4.5 Additional Results\nGeneration Diversity Section 4.3 shows that RAG models are more factual and speci\ufb01c than\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\ntotal ngrams generated by different models. Table 5 shows that RAG-Sequence\u2019s generations are\nmore diverse than RAG-Token\u2019s, and both are signi\ufb01cantly more diverse than BART without needing\nany diversity-promoting decoding.\nRetrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\nduring training. As shown in Table 6, learned retrieval improves results for all tasks.\nWe compare RAG\u2019s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace\nRAG\u2019s retriever with a \ufb01xed BM25 system, and use BM25 retrieval scores as logits when calculating\np(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\nheavily entity-centric and thus well-suited for word overlap-based retrieval.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3735, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "361afec3-2d22-46ef-8e45-b692f342f90e": {"__data__": {"id_": "361afec3-2d22-46ef-8e45-b692f342f90e", "embedding": null, "metadata": {"page_label": "7", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1aaab2c-7d44-497e-8cc6-6ec01aaf2b67", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "a66238ad15a00535a8833526e2292928efc7de240331969fc8b8225f27e601ff", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02110f77-ff94-4534-b3dd-7c4c7c0976c2", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "038c8c497ca6404f0cca621a6652e654a5cb9ea853ff302b7b06a851223085ca", "class_name": "RelatedNodeInfo"}}, "text": "Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\nduring training. As shown in Table 6, learned retrieval improves results for all tasks.\nWe compare RAG\u2019s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace\nRAG\u2019s retriever with a \ufb01xed BM25 system, and use BM25 retrieval scores as logits when calculating\np(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\nheavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval\nimproves results on all other tasks, especially for Open-Domain QA, where it is crucial.\nIndex hot-swapping An advantage of non-parametric memory models like RAG is that knowledge\ncan be easily updated at test time. Parametric-only models like T5 or BART need further training to\nupdate their behavior as the world changes. To demonstrate, we build an index using the DrQA [5]\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\n7", "mimetype": "text/plain", "start_char_idx": 3077, "end_char_idx": 4331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72064272-5b7a-4547-b7da-9b1b02c4f638": {"__data__": {"id_": "72064272-5b7a-4547-b7da-9b1b02c4f638", "embedding": null, "metadata": {"page_label": "8", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46b2ca3a-e9c9-4443-9a1e-bdb5ebf5cfa9", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "19e1f6a5ab9933bd459bafe5352307ba5f2dfd0f4304fba31736d62b8063a192", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14a36df0-a23a-4bff-8d9a-bec8352fd24c", "node_type": "1", "metadata": {}, "hash": "23787b2c8798c76518bfdb24fb984853786ba9e6207c58fb6cd26116ef431ecb", "class_name": "RelatedNodeInfo"}}, "text": "Table 4: Human assessments for the Jeopardy\nQuestion Generation Task.\nFactuality Speci\ufb01city\nBART better 7.1% 16.8%\nRAG better 42.7% 37.4%\nBoth good 11.7% 11.8%\nBoth poor 17.7% 6.9%\nNo majority 20.8% 20.1%\nTable 5: Ratio of distinct to total tri-grams for\ngeneration tasks.\nMSMARCO Jeopardy QGen\nGold 89.6% 90.0%\nBART 70.7% 32.4%\nRAG-Token 77.8% 46.8%\nRAG-Seq. 83.5% 53.8%\nTable 6: Ablations on the dev set. As FEVER is a classi\ufb01cation task, both RAG models are equivalent.\nModel NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2\nExact Match B-1 QB-1 R-L B-1 Label Accuracy\nRAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.4 75.1 91.6RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9\nRAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.4 72.9 89.4RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3\nRAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.4 74.5 90.6RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5\nbetween these dates and use a template \u201cWho is {position}?\u201d (e.g. \u201cWho is the President of Peru?\u201d)\nto query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for\n2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched\nindices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).\nThis shows we can update RAG\u2019s world knowledge by simply replacing its non-parametric memory.\nEffect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent\ndocuments, and we do not observe signi\ufb01cant differences in performance between them. We have the\n\ufb02exibility to adjust the number of retrieved documents at test time, which can affect performance and\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\nOpen-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved\ndocuments. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\nRAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\n10 20 30 40 50\nKR e t r i e v e dD o c s\n39\n40\n41\n42\n43\n44NQ Exact Match RAG-Tok\nRAG-Seq\n10 20 30 40 50\nKR e t r i e v e dD o c s\n40\n50\n60\n70\n80NQ Answer Recall @ K\nRAG-Tok\nRAG-Seq\nFixed DPR\nBM25\n10 20 30 40 50\nKR e t r i e v e dD o c s\n48\n50\n52\n54\n56Bleu-1 / Rouge-L score\nRAG-Tok R-L\nRAG-Tok B-1\nRAG-Seq R-L\nRAG-Seq B-1\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-\nmance in NQ.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14a36df0-a23a-4bff-8d9a-bec8352fd24c": {"__data__": {"id_": "14a36df0-a23a-4bff-8d9a-bec8352fd24c", "embedding": null, "metadata": {"page_label": "8", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46b2ca3a-e9c9-4443-9a1e-bdb5ebf5cfa9", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "19e1f6a5ab9933bd459bafe5352307ba5f2dfd0f4304fba31736d62b8063a192", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72064272-5b7a-4547-b7da-9b1b02c4f638", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "bf1a5c880a1acc9ed8545c270281e1f6fad616674ddc674625143a67d3319101", "class_name": "RelatedNodeInfo"}}, "text": "Center: Retrieval recall perfor-\nmance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\n5 Related Work\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of\nNLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29],\nfact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article\ngeneration [36], dialogue [ 41, 65, 9, 13], translation [ 17], and language modeling [ 19, 27]. Our\nwork uni\ufb01es previous successes in incorporating retrieval into individual tasks, showing that a single\nretrieval-based architecture is capable of achieving strong performance across several tasks.\n8", "mimetype": "text/plain", "start_char_idx": 2463, "end_char_idx": 3186, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32e367c7-d4d9-42cf-af1e-fcc2ef7a4916": {"__data__": {"id_": "32e367c7-d4d9-42cf-af1e-fcc2ef7a4916", "embedding": null, "metadata": {"page_label": "9", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3fe1691c-6cb0-40c8-961d-e25dfc04a1fe", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "388597f470f98ba204965fdb3dc49a21568ee9ce4c2370cc98276989f6d45636", "class_name": "RelatedNodeInfo"}}, "text": "General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\nhas been shown to achieve strong performance on various classi\ufb01cation tasks in the GLUE bench-\nmarks [60, 61] after \ufb01ne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained\nlanguage model could achieve strong performance across both discriminative and generative tasks.\nFor further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\nand generative tasks. Our work aims to expand the space of possible tasks with a single, uni\ufb01ed\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\nLearned Retrieval There is signi\ufb01cant work on learning to retrieve documents in information\nretrieval, more recently with pre-trained, neural language models [ 44, 26] similar to ours. Some\nwork optimizes the retrieval module to aid in a speci\ufb01c, downstream task such as question answering,\nusing search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\nachieve strong performance on a single task, while we show that a single retrieval-based architecture\ncan be \ufb01ne-tuned for strong performance on a variety of tasks.\nMemory-based Architectures Our document index can be seen as a large external memory for\nneural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\nwork. Other work improves the ability of dialog models to generate factual text by attending over\nfact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model\u2019s\nmemory by editing the document index. This approach has also been used in knowledge-intensive\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\nrather than end-to-end learnt retrieval [9].\nRetrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\nto provide a \ufb01nal output. These approaches have proved successful in a number of domains including\nMachine Translation [ 18, 22] and Semantic Parsing [21]. Our approach does have several differences,\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content from several\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\ncould represent promising future work.\n6 Discussion\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\nfound that people prefer RAG\u2019s generation over purely parametric BART, \ufb01nding RAG more factual\nand speci\ufb01c. We conducted an thorough investigation of the learned retrieval component, validating\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components\ncan be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\nanother objective. Our work opens up new research directions on how parametric and non-parametric\nmemories interact and how to most effectively combine them, showing promise in being applied to a\nwide variety of NLP tasks.\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4124, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "213385ad-7f81-43e4-b831-83b0a542143b": {"__data__": {"id_": "213385ad-7f81-43e4-b831-83b0a542143b", "embedding": null, "metadata": {"page_label": "10", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b01cfc6-6781-42a8-98b0-3bc0ae981d95", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "b7a084ddcb62ee5eca50b7f32101bd4527d0bee52c75e56aa9dddb6f4265e844", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11d20e68-07f8-49ae-b5ae-4afa2c177498", "node_type": "1", "metadata": {}, "hash": "3c8adaa54dfbb2668adc6ccd69c17a7a84e608fc9de1e644d757ac65caae1423", "class_name": "RelatedNodeInfo"}}, "text": "Broader Impact\nThis work offers several positive societal bene\ufb01ts over previous work: the fact that it is more\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it \u201challucinate\u201d less\nwith generations that are more factual, and offers more control and interpretability. RAG could be\nemployed in a wide variety of scenarios with direct bene\ufb01t to society, for example by endowing it\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\neffective at their jobs.\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\nemployed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\nthe news or on social media; to impersonate others; or to automate the production of spam/phishing\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\ncoming decades [16]. In order to mitigate these risks, AI systems could be employed to \ufb01ght against\nmisleading content and automated spam/phishing.\nAcknowledgments\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\nprogram.\nReferences\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\nReading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:\n//arxiv.org/abs/1611.09268. arXiv: 1611.09268.\n[2] Petr Baudi\u0161 and Jan \u0160ediv`y. Modeling of the question answering task in the yodaqa system. In\nInternational Conference of the Cross-Language Evaluation Forum for European Languages,\npages 222\u2013228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%\n2F978-3-319-24027-5_20 .\n[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase\nfrom Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods\nin Natural Language Processing, pages 1533\u20131544, Seattle, Washington, USA, October 2013.\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\nD13-1160.\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-\ning&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159,\n2020. URL https://arxiv.org/abs/2004.07159.\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 1870\u20131879, Vancouver, Canada,\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\nhttps://www.aclweb.org/anthology/P17-1171.\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\nJonathan Berant. Coarse-to-\ufb01ne question answering for long documents. In Proceedings of the\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 209\u2013220, Vancouver, Canada, July 2017.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11d20e68-07f8-49ae-b5ae-4afa2c177498": {"__data__": {"id_": "11d20e68-07f8-49ae-b5ae-4afa2c177498", "embedding": null, "metadata": {"page_label": "10", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b01cfc6-6781-42a8-98b0-3bc0ae981d95", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "b7a084ddcb62ee5eca50b7f32101bd4527d0bee52c75e56aa9dddb6f4265e844", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "213385ad-7f81-43e4-b831-83b0a542143b", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "690c91c834c2035b73d1834e299b18cba4859909873d3a797a45a8149fea94af", "class_name": "RelatedNodeInfo"}}, "text": "Reading Wikipedia to Answer\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 1870\u20131879, Vancouver, Canada,\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\nhttps://www.aclweb.org/anthology/P17-1171.\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\nJonathan Berant. Coarse-to-\ufb01ne question answering for long documents. In Proceedings of the\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 209\u2013220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\n10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.\n10", "mimetype": "text/plain", "start_char_idx": 3006, "end_char_idx": 3778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c57e6931-2394-43c7-859b-47395aada179": {"__data__": {"id_": "c57e6931-2394-43c7-859b-47395aada179", "embedding": null, "metadata": {"page_label": "11", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "702dbea1-3fd7-4e63-98f3-514d0596c2bb", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "45709a804a8faf32c8f98570063e8fdf78bd405ec5b8720bef7287d1c1fcecd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9630248-3990-4524-94d8-92e12325c13e", "node_type": "1", "metadata": {}, "hash": "d7592b28c98528e9822373e1674ba1c910a38f738d52f742b7c2d23de52ed365", "class_name": "RelatedNodeInfo"}}, "text": "[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-\nhension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723.\narXiv: 1710.10723.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis,\nMinnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\nURL https://www.aclweb.org/anthology/N19-1423.\n[9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-\nard of wikipedia: Knowledge-powered conversational agents. In International Conference on\nLearning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\n[10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun\nCho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\narXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv:\n1704.05179.\n[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pages 889\u2013898, Melbourne, Australia, July 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\nP18-1082.\n[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 3558\u20133567, Florence, Italy, July 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/\nanthology/P19-1346.\n[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers\nwith KNN-based composite memory, 2020. URL https://openreview.net/forum?id=\nH1gx1CNKPH.\n[14] Thibault F\u00e9vry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.\nEntities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202,\n2020. URL https://arxiv.org/abs/2004.07202.\n[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen\ntau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI\nConference on Arti\ufb01cial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/\nAAAI/AAAI18/paper/view/16710.\n[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI\nexceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL\nhttp://arxiv.org/abs/1705.08807.\n[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\nmachine translation. In AAAI Conference on Arti\ufb01cial Intelligence , 2018. URL https:\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3118, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9630248-3990-4524-94d8-92e12325c13e": {"__data__": {"id_": "c9630248-3990-4524-94d8-92e12325c13e", "embedding": null, "metadata": {"page_label": "11", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "702dbea1-3fd7-4e63-98f3-514d0596c2bb", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "45709a804a8faf32c8f98570063e8fdf78bd405ec5b8720bef7287d1c1fcecd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c57e6931-2394-43c7-859b-47395aada179", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "c4055128c30a874ebbc911a5b5bf01643ca53e5d7201c64d07317c7b435e11f7", "class_name": "RelatedNodeInfo"}}, "text": "In AAAI\nConference on Arti\ufb01cial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/\nAAAI/AAAI18/paper/view/16710.\n[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI\nexceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL\nhttp://arxiv.org/abs/1705.08807.\n[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\nmachine translation. In AAAI Conference on Arti\ufb01cial Intelligence , 2018. URL https:\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\n[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\nmachine translation. In 32nd AAAI Conference on Arti\ufb01cial Intelligence, AAAI 2018 , 32nd\nAAAI Conference on Arti\ufb01cial Intelligence, AAAI 2018, pages 5133\u20135140. AAAI press, 2018.\n32nd AAAI Conference on Arti\ufb01cial Intelligence, AAAI 2018 ; Conference date: 02-02-2018\nThrough 07-02-2018.\n[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by\nediting prototypes. Transactions of the Association for Computational Linguistics, 6:437\u2013450,\n2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.\n11", "mimetype": "text/plain", "start_char_idx": 2555, "end_char_idx": 3767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b53c0a7-6c0f-48e9-be31-f72eb7bac96c": {"__data__": {"id_": "2b53c0a7-6c0f-48e9-be31-f72eb7bac96c", "embedding": null, "metadata": {"page_label": "12", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "12ac5c52-43db-47fd-8c86-b6acc1d7b19b", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "b82a1c45fdafeac9a4fb02af931baf4c4ce4cfc1dc0163b6d1e372a6c251eb1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "562dd12c-08d7-42d4-8352-04ce8ff6bc35", "node_type": "1", "metadata": {}, "hash": "34d5aba010554d2e3c9312b274e68c08ffc690f3b61f374d29bbf95c582bb300", "class_name": "RelatedNodeInfo"}}, "text": "[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\nRetrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https:\n//arxiv.org/abs/2002.08909.\n[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A\nretrieve-and-edit framework for predicting structured outputs. In S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\nitors, Advances in Neural Information Processing Systems 31 , pages 10052\u2013\n10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.\npdf.\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-\nedit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 2532\u20132538, Online, July 2020. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/\nanthology/2020.acl-main.228.\n[23] Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with gpus. arXiv\npreprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale\nDistantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 1601\u20131611, Vancouver, Canada, July 2017. Association for Computational Linguistics.\ndoi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.\n[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-\naugmented recurrent nets. In Proceedings of the 28th International Conference on\nNeural Information Processing Systems - Volume 1 , NIPS\u201915, page 190\u2013198, Cam-\nbridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/\n5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets .\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\narXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-\ntion through memorization: Nearest neighbor language models. In International Conference on\nLearning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\nhttp://arxiv.org/abs/1412.6980.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2841, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "562dd12c-08d7-42d4-8352-04ce8ff6bc35": {"__data__": {"id_": "562dd12c-08d7-42d4-8352-04ce8ff6bc35", "embedding": null, "metadata": {"page_label": "12", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "12ac5c52-43db-47fd-8c86-b6acc1d7b19b", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "b82a1c45fdafeac9a4fb02af931baf4c4ce4cfc1dc0163b6d1e372a6c251eb1f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b53c0a7-6c0f-48e9-be31-f72eb7bac96c", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "97b46a36c1b899634b2013a793d989d24622a738ea9f7d9d32cd544abd3950ca", "class_name": "RelatedNodeInfo"}}, "text": "URL https://arxiv.org/abs/2004.04906.\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-\ntion through memorization: Nearest neighbor language models. In International Conference on\nLearning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\nhttp://arxiv.org/abs/1412.6980.\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red\ufb01eld, Michael Collins, Ankur Parikh,\nChris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-\nton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques-\ntion Answering Research. Transactions of the Association of Computational Lin-\nguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/\nnatural-questions/main-1455-kwiatkowski.pdf .\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc\u2019 Aurelio Ranzato, Ludovic Denoyer, and\nHerve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d\u2019 Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-\nformation Processing Systems 32, pages 8548\u20138559. Curran Associates, Inc., 2019. URL http:\n//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf .\n[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\nopen domain question answering. In Proceedings of the 57th Annual Meeting of the Association\n12", "mimetype": "text/plain", "start_char_idx": 2236, "end_char_idx": 3990, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "696bf2ab-3eb5-4680-8d43-d16c9c40bf49": {"__data__": {"id_": "696bf2ab-3eb5-4680-8d43-d16c9c40bf49", "embedding": null, "metadata": {"page_label": "13", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "15cb1b71-56a6-4191-94ca-b2417fc78085", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "934fb0564c4b84c9f453e703aac199f7f59a06da7086fef6fdb2c204c6ade290", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47a6a4a1-753d-4588-bbde-b69906a25140", "node_type": "1", "metadata": {}, "hash": "1d5cdd00f7a05361012368a664c007345dcfbc60a94d1a79e0597ee6a2ee855b", "class_name": "RelatedNodeInfo"}}, "text": "for Computational Linguistics, pages 6086\u20136096, Florence, Italy, July 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/\nanthology/P19-1612.\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and comprehension. arXiv preprint\narXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\nobjective function for neural conversation models. In Proceedings of the 2016 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 110\u2013119, San Diego, California, June 2016. Association for Computational\nLinguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\nN16-1014.\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation\nwith optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL\nhttps://arxiv.org/abs/1909.03087.\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine\ntranslation with joint textual and phonetic embedding. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, pages 3044\u20133049, Florence, Italy,\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL\nhttps://www.aclweb.org/anthology/P19-1291.\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\nand Noam Shazeer. Generating wikipedia by summarizing long sequences. In International\nConference on Learning Representations, 2018. URL https://openreview.net/forum?\nid=Hyg0vbWC-.\n[37] Yury A. Malkov and D. A. Yashunin. Ef\ufb01cient and robust approximate nearest neighbor search\nusing hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 42:824\u2013836, 2016. URL https://arxiv.org/abs/1603.09320.\n[38] Gary Marcus. The next decade in ai: four steps towards robust arti\ufb01cial intelligence. arXiv\npreprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt\u00e4schel, Vassilis\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\nveri\ufb01ability of generated text. arXiv preprint arXiv:1911.03587 , 2019. URL https:\n//arxiv.org/abs/1911.03587.\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\nprecision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-\ning background knowledge for building conversation systems. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing, pages 2322\u20132332, Brus-\nsels, Belgium, October-November 2018.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47a6a4a1-753d-4588-bbde-b69906a25140": {"__data__": {"id_": "47a6a4a1-753d-4588-bbde-b69906a25140", "embedding": null, "metadata": {"page_label": "13", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "15cb1b71-56a6-4191-94ca-b2417fc78085", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "934fb0564c4b84c9f453e703aac199f7f59a06da7086fef6fdb2c204c6ade290", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "696bf2ab-3eb5-4680-8d43-d16c9c40bf49", "node_type": "1", "metadata": {"page_label": "13", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "b5c6443698a00e81a3f23e4bd80687ed3efa237212ab2ab4eb8ea3f9fd999383", "class_name": "RelatedNodeInfo"}}, "text": "URL https:\n//arxiv.org/abs/1911.03587.\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\nprecision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-\ning background knowledge for building conversation systems. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing, pages 2322\u20132332, Brus-\nsels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\n10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\nsystems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, pages 3950\u20133959, Brussels, Belgium, October-November 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/\nanthology/D18-1429.\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\nand Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\nTarek Richard Besold, Antoine Bordes, Artur S. d\u2019Avila Garcez, and Greg Wayne, editors,\nProceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\n13", "mimetype": "text/plain", "start_char_idx": 2559, "end_char_idx": 4010, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9209a8d9-7f9c-442b-965a-3938a956c95c": {"__data__": {"id_": "9209a8d9-7f9c-442b-965a-3938a956c95c", "embedding": null, "metadata": {"page_label": "14", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f7bba5bf-f672-4398-8849-16ba4dd084fc", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "683afed788e26248269e793fe53ef7e6542876bbef11905674399cf21bcce888", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c8061f2-9b95-48b4-a614-19d60f880b64", "node_type": "1", "metadata": {}, "hash": "73980d4b1938174a52fc075b60aac87a1cd249559d10174a7a22c23498a43966", "class_name": "RelatedNodeInfo"}}, "text": "approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing\nSystems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop\nProceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_\n2016_paper9.pdf.\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\narXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings\nof the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics (Demonstrations), pages 48\u201353, Minneapolis, Minnesota, June 2019. Association\nfor Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.\norg/anthology/N19-4009.\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun\nCho. Finding generalizable evidence by learning to convince q&a models. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages\n2402\u20132411, Hong Kong, China, November 2019. Association for Computational Linguistics.\ndoi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\n[47] Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\u20132473, Hong\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/\nD19-1250. URL https://www.aclweb.org/anthology/D19-1250.\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H.\nMiller, and Sebastian Riedel. How context affects language models\u2019 factual predictions. In\nAutomated Knowledge Base Construction, 2020. URL https://openreview.net/forum?\nid=025X0zPfn.\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-\nproving Language Understanding by Generative Pre-Training, 2018. URL\nhttps://s3-us-west-2.amazonaws.com/openai-assets/research-covers/\nlanguage-unsupervised/language_understanding_paper.pdf.\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. Language models are unsupervised multitask learners, 2019. URL\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_\nmodels_are_unsupervised_multitask_learners.pdf.\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uni\ufb01ed\ntext-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\nthe parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/\n2002.08910.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3179, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c8061f2-9b95-48b4-a614-19d60f880b64": {"__data__": {"id_": "9c8061f2-9b95-48b4-a614-19d60f880b64", "embedding": null, "metadata": {"page_label": "14", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f7bba5bf-f672-4398-8849-16ba4dd084fc", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "683afed788e26248269e793fe53ef7e6542876bbef11905674399cf21bcce888", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9209a8d9-7f9c-442b-965a-3938a956c95c", "node_type": "1", "metadata": {"page_label": "14", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "6f0bca805876a95463bd20cd5264e284596838fde217d3ec126f224741a53582", "class_name": "RelatedNodeInfo"}}, "text": "URL\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_\nmodels_are_unsupervised_multitask_learners.pdf.\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uni\ufb01ed\ntext-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\nthe parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/\n2002.08910.\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\nbeyond. Found. Trends Inf. Retr., 3(4):333\u2013389, April 2009. ISSN 1554-0669. doi: 10.1561/\n1500000019. URL https://doi.org/10.1561/1500000019.\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\nArXiv, abs/1908.09203, 2019.\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-\nworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,Advances\nin Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates, Inc., 2015.\nURL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf .\n14", "mimetype": "text/plain", "start_char_idx": 2601, "end_char_idx": 3984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2231e565-3d42-44d2-a8ba-8449cf61ba30": {"__data__": {"id_": "2231e565-3d42-44d2-a8ba-8449cf61ba30", "embedding": null, "metadata": {"page_label": "15", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "04947ad0-6e74-4888-ad68-561644f6ce91", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "613db13869148cf0d718f276fa11a39ddee9eee338163d399dd5f8a75bc1c069", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d251e7a-b243-4039-94b0-d52417ddba74", "node_type": "1", "metadata": {}, "hash": "39f1d29699d69b28fef6a6c1fd9bad875fd383401de4cd84636639e8457820d4", "class_name": "RelatedNodeInfo"}}, "text": "[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\nlarge-scale dataset for fact extraction and VERi\ufb01cation. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pages 809\u2013819, New Orleans, Louisiana,\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL\nhttps://www.aclweb.org/anthology/N18-1074.\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model\nbiases in sentence-pair classi\ufb01cation with elastic weight consolidation. ArXiv, abs/2004.14366,\n2020. URL https://arxiv.org/abs/2004.14366.\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural\nInformation Processing Systems 30, pages 5998\u20136008. Curran Associates, Inc., 2017. URL\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\nCrandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.\nAAAI Conference on Arti\ufb01cial Intelligence, 2018. URL https://www.aaai.org/ocs/index.\nphp/AAAI/AAAI18/paper/view/17329.\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP, pages 353\u2013355, Brussels, Belgium, November 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/\nanthology/W18-5446.\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-\nPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d\\textquotesingle Alch\u00e9-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information\nProcessing Systems 32, pages 3261\u20133275. Curran Associates, Inc., 2019. URL https://\narxiv.org/abs/1905.00537.\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,\nGerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain\nquestion answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\nthe Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, (AAAI-18), the 30th innovative\nApplications of Arti\ufb01cial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\nAdvances in Arti\ufb01cial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\n2018, pages 5981\u20135988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.\nphp/AAAI/AAAI18/paper/view/16712.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3028, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d251e7a-b243-4039-94b0-d52417ddba74": {"__data__": {"id_": "8d251e7a-b243-4039-94b0-d52417ddba74", "embedding": null, "metadata": {"page_label": "15", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "04947ad0-6e74-4888-ad68-561644f6ce91", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "613db13869148cf0d718f276fa11a39ddee9eee338163d399dd5f8a75bc1c069", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2231e565-3d42-44d2-a8ba-8449cf61ba30", "node_type": "1", "metadata": {"page_label": "15", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "3c5f7d52d526763d8f180c9b5c76d96d6d887a9ddb8bf068afce02ef73969c9b", "class_name": "RelatedNodeInfo"}}, "text": "R3: Reinforced ranker-reader for open-domain\nquestion answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\nthe Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, (AAAI-18), the 30th innovative\nApplications of Arti\ufb01cial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\nAdvances in Arti\ufb01cial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\n2018, pages 5981\u20135988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.\nphp/AAAI/AAAI18/paper/view/16712.\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,\nTim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-\nranking in open-domain question answering. In ICLR, 2018. URL https://openreview.\nnet/forum?id=rJl3yM-Ab.\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio\nand Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL\nhttp://arxiv.org/abs/1410.3916.\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and re\ufb01ne: Improved sequence\ngeneration models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd\nInternational Workshop on Search-Oriented Conversational AI, pages 87\u201392, Brussels, Belgium,\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL\nhttps://www.aclweb.org/anthology/W18-5713.\n15", "mimetype": "text/plain", "start_char_idx": 2507, "end_char_idx": 4006, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc4926e3-c0f9-4ca9-8b15-a3bbef9e36d9": {"__data__": {"id_": "cc4926e3-c0f9-4ca9-8b15-a3bbef9e36d9", "embedding": null, "metadata": {"page_label": "16", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "199c7629-974f-40ae-94a7-b516538f02cd", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "078c62bb8a8db88bf0eb267ce24452fe12be71f10c8c963756e62893f1c27916", "class_name": "RelatedNodeInfo"}}, "text": "[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface\u2019s transformers:\nState-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\nsupervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP) , pages 2495\u20132509, Hong Kong, China, Novem-\nber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL\nhttps://www.aclweb.org/anthology/D19-1253.\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and\nJian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019.\nURL https://arxiv.org/abs/1909.03745.\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73e97549-8258-4c51-91ee-03c795f38ad7": {"__data__": {"id_": "73e97549-8258-4c51-91ee-03c795f38ad7", "embedding": null, "metadata": {"page_label": "17", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "48769f2d-0bc5-4887-9019-0f89da8f3a15", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "da5c41ccb996cd8fde8cc642b0d35bf597bbd7e617c004e2ca7f877580a399b7", "class_name": "RelatedNodeInfo"}}, "text": "Appendices for Retrieval-Augmented Generation for\nKnowledge-Intensive NLP Tasks\nA Implementation Details\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.\nFor RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\nThorough Decoding approach since answers are generally short. We use greedy decoding for QA as\nwe did not \ufb01nd beam search improved results. For Open-MSMarco and Jeopardy question generation,\nwe report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\nand we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\nDecoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\nB Human Evaluation\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions\nand a worked example appear when clicking \"view tool guide\".\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position,\nwhich model corresponded to sentence A and sentence B was randomly selected for each example.\nAnnotators were encouraged to research the topic using the internet, and were given detailed instruc-\ntions and worked examples in a full instructions tab. We included some gold sentences in order to\nassess the accuracy of the annotators. Two annotators did not perform well on these examples and\ntheir annotations were removed from the results.\nC Training setup Details\nWe train all RAG models and BART baselines using Fairseq [45].2 We train with mixed precision\n\ufb02oating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though\ntraining and inference can be run on one GPU. We \ufb01nd that doing Maximum Inner Product Search\nwith FAISS is suf\ufb01ciently fast on CPU, so we store document index vectors on CPU, requiring\u223c100\nGB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace\nTransformers [66]3, which achieves equivalent performance to the previous version but is a cleaner\nand easier to use implementation. This version is also open-sourced. We also compress the document\nindex using FAISS\u2019s compression tools, reducing the CPU memory requirement to 36GB. Scripts to\nrun experiments with RAG can be found athttps://github.com/huggingface/transformers/\nblob/master/examples/rag/README.md and an interactive demo of a RAG model can be found\nat https://huggingface.co/rag/\n2https://github.com/pytorch/fairseq\n3https://github.com/huggingface/transformers\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5ae9077-55ab-40aa-a6c1-70da3f751a61": {"__data__": {"id_": "c5ae9077-55ab-40aa-a6c1-70da3f751a61", "embedding": null, "metadata": {"page_label": "18", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e5c821c4-f03f-40b6-8875-c89eeac79c24", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "c1dd9ceb5ab2368995aa8e2bb2a82f634f72b20de33a50cf432bc2f49281d605", "class_name": "RelatedNodeInfo"}}, "text": "D Further Details on Open-Domain QA\nFor open-domain QA, multiple answer annotations are often available for a given question. These\nanswer annotations are exploited by extractive models during training as typically all the answer\nannotations are used to \ufb01nd matches within documents when preparing training data. For RAG, we\nalso make use of multiple annotation examples for Natural Questions and WebQuestions by training\nthe model with each (q,a) pair separately, leading to a small increase in accuracy. For TriviaQA,\nthere are often many valid answers to a given question, some of which are not suitable training targets,\nsuch as emoji or spelling variants. For TriviaQA, we \ufb01lter out answer candidates if they do not occur\nin top 1000 documents for the query.\nCuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres-\nsions, which has been suggested as a reason why it is unsuitable for answer-generation models [20].\nTo overcome this, we use a pre-processing step where we \ufb01rst retrieve the top 1000 documents for\neach query, and use the answer that most frequently matches the regex pattern as the supervision\ntarget. If no matches are found, we resort to a simple heuristic: generate all possible permutations for\neach regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\nTriviaQA Evaluation setups The open-domain QA community customarily uses public develop-\nment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading\ncompehension purposes. We report our results using the datasets splits used in DPR [26], which are\nconsistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public\nTriviaQA Web Development split. Roberts et al.[52] used the TriviaQA of\ufb01cial Wikipedia test set\ninstead. F\u00e9vry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See\nappendix of [14]). We report results on both test sets to enable fair comparison to both approaches.\nWe \ufb01nd that our performance is much higher using the of\ufb01cial Wiki test set, rather than the more\nconventional open-domain test set, which we attribute to the of\ufb01cial Wiki test set questions being\nsimpler to answer from Wikipedia.\nE Further Details on FEVER\nFor FEVER classi\ufb01cation, we follow the practice from [ 32], and \ufb01rst re-generate the claim, and\nthen classify using the representation of the \ufb01nal hidden state, before \ufb01nally marginalizing across\ndocuments to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The\n\ufb01rst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task\nwe explore in the main paper. FEVER\u2019s other sub-task involves extracting sentences from Wikipedia\nas evidence supporting the classi\ufb01cation prediction. As FEVER uses a different Wikipedia dump to\nus, directly tackling this task is not straightforward. We hope to address this in future work.\nF Null Document Probabilities\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order\nto model cases where no useful information could be retrieved for a given input. Here, ifkdocuments\nwere retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null\ndocument, before marginalizing over k+ 1predictions. We explored modelling this null document\nlogit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or\n(iii) a neural network to predict the logit. We did not \ufb01nd that these improved performance, so in\nthe interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents\ncannot always be retrieved, we observe that the model learns to always retrieve a particular set of\ndocuments for questions that are less likely to bene\ufb01t from retrieval, suggesting that null document\nmechanisms may not be necessary for RAG.\nG Parameters\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of\nDPR, with 110M parameters each (although we do not train the document encoder ourselves) and\n406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4243, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2f4b5bb-0be8-4794-b3f9-8054454b1858": {"__data__": {"id_": "f2f4b5bb-0be8-4794-b3f9-8054454b1858", "embedding": null, "metadata": {"page_label": "19", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "072d472c-0b9e-4c76-9399-7b1237cdf258", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "ef6d275f0ec45f92580ef7c2eb0c4746e01422d557fa3a3c0218b213f82566dd", "class_name": "RelatedNodeInfo"}}, "text": "Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\nTask Train Development Test\nNatural Questions 79169 8758 3611\nTriviaQA 78786 8838 11314\nWebQuestions 3418 362 2033\nCuratedTrec 635 134 635\nJeopardy Question Generation 97392 13714 26849\nMS-MARCO 153726 12468 101093*\nFEVER-3-way 145450 10000 10000\nFEVER-2-way 96966 6666 6666\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit \ufb02oating\npoint precision to manage memory and disk footprints.\nH Retrieval Collapse\nIn preliminary experiments, we observed that for some tasks such as story generation [ 11], the\nretrieval component would \u201ccollapse\u201d and learn to retrieve the same documents regardless of the\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\nin less informative gradients for the retriever. Perez et al.[46] also found spurious retrieval results\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\nI Number of instances per dataset\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c75945b-edeb-4716-8c12-3b75e62b5cc0": {"__data__": {"id_": "6c75945b-edeb-4716-8c12-3b75e62b5cc0", "embedding": null, "metadata": {"page_label": "1", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "50278bb3-b938-4698-a028-5b396c83bcdb", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "94d4e9afccc3505a41c24c793cf4219a3310c20a9081a5e20f5e40a7d9a21cc0", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nSELF -RAG : L EARNING TO RETRIEVE , GENERATE , AND\nCRITIQUE THROUGH SELF -REFLECTION\nAkari Asai\u2020, Zeqiu Wu\u2020, Yizhong Wang\u2020\u00a7, Avirup Sil\u2021, Hannaneh Hajishirzi\u2020\u00a7\n\u2020University of Washington \u00a7Allen Institute for AI \u2021IBM Research AI\n{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu, avi@us.ibm.com\nABSTRACT\nDespite their remarkable capabilities, large language models (LLMs) often produce\nresponses containing factual inaccuracies due to their sole reliance on the paramet-\nric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\nhoc approach that augments LMs with retrieval of relevant knowledge, decreases\nsuch issues. However, indiscriminately retrieving and incorporating a fixed number\nof retrieved passages, regardless of whether retrieval is necessary, or passages are\nrelevant, diminishes LM versatility or can lead to unhelpful response generation.\nWe introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\neration (SELF -RAG) that enhances an LM\u2019s quality and factuality through retrieval\nand self-reflection. Our framework trains a single arbitrary LM that adaptively\nretrieves passages on-demand, and generates and reflects on retrieved passages\nand its own generations using special tokens, called reflection tokens. Generating\nreflection tokens makes the LM controllable during the inference phase, enabling it\nto tailor its behavior to diverse task requirements. Experiments show that SELF -\nRAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs\nand retrieval-augmented models on a diverse set of tasks. Specifically, SELF -RAG\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\nreasoning and fact verification tasks, and it shows significant gains in improving\nfactuality and citation accuracy for long-form generations relative to these models.1\n1 I NTRODUCTION\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\nwork introduces Self-Reflective Retrieval-augmented Generation ( SELF -RAG) to improve an\nLLM\u2019s generation quality, including its factual accuracy without hurting its versatility, via on-demand\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\nits own generation process given a task input by generating both task output and intermittent special\ntokens (i.e., reflection tokens). Reflection tokens are categorized into retrieval and critique tokens to\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\ngiven an input prompt and preceding generations, SELF -RAG first determines if augmenting the\ncontinued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\ncalls a retriever model on demand (Step 1). Subsequently,SELF -RAG concurrently processes multiple\nretrieved passages, evaluating their relevance and thengenerating corresponding task outputs (Step\n2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\nof factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\n1Our code and trained models are available at https://selfrag.github.io/.\n1\narXiv:2310.11511v1  [cs.CL]  17 Oct 2023", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65977908-800a-4f24-9295-9fb50c47da63": {"__data__": {"id_": "65977908-800a-4f24-9295-9fb50c47da63", "embedding": null, "metadata": {"page_label": "2", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f6b3f08-cf99-44da-9369-338a6365b30b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "fe39947021b130e579e155834fdb4b3c46b6efe66105324b18307f60b5e09f58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb6ac9e0-df2d-43d3-8098-51bccd70affc", "node_type": "1", "metadata": {}, "hash": "ec75d82c44f654107e5539a76021f80549b0a50315f2d1757e0ea3889b66390e", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nStep 1: Retrieve K documents\nCalifornia was named after a \ufb01ctional \nisland in a Spanish book. \nPrompt How did US states get their names? \nUS states got their names from a variety of \nsources. Eleven states are named after an \nindividual person  (e.g, California was named \nafter Christopher Columbus) . Some states \nincluding Texas and Utah, are named after \nNative American tribe.\nRetrieval-Augmented Generation (RAG) Ours: Self-re\ufb02ective Retrieval-Augmented Generation (Self-RAG) \nPopular names by states. In Texas, \nEmma is a popular baby name. \nOf the \ufb01fty states, eleven are named \nafter an individual person. \nPrompt How did US states get their names? + \nStep 2: Prompt LM with K docs and generate\nRetriever\nLM\nPrompt How did US states get their names? \nUS states got their names from a variety of sources. Retrieve\nStep 1: Retrieve on demand  \nPrompt +  \n11 of 50 state namesRelevant\nStep 2: Generate segment in parallel \ncome from persons. Supported\nIrrelevant Texas is named\nafter a Native American tribe. \nStep 3: Critique outputs and select best segment\norigins in a 16th-century novel \nLas Sergas de Esplandi\u00e1n. \nCalifornia's name has itsRelevant\nPartially\nUS states got their names from a variety of sources. 11 of 50 \nstates names are come from persons.    26 states are named \nafter Native Americans, including Utah. \nPrompt: Write an essay of your best summer vacation\n Prompt: Write an essay of your best summer vacation\nNo Retrieval My best summer vacation is when my family and I embarked on a road trip along \u2026My best\u2026 \n>\nRepeat.\u2026\nNo information in passagesContradictory\n>\nPrompt +  \n Prompt +  \nRetrieve\nFigure 1: Overview of SELF -RAG. SELF -RAG learns to retrieve, critique, and generate text passages\nto enhance overall generation quality, factuality, and verifiability.\nconsistently retrieves a fixed number of documents for generation regardless of the retrieval necessity\n(e.g., the bottom figure example does not require factual knowledge) and never second visits the\ngeneration quality. Moreover, SELF -RAG provides citations for each segment with its self-assessment\nof whether the output is supported by the passage, leading to easier fact verification.\nSELF -RAG trains an arbitrary LM to generate text with reflection tokens by unifying them as the\nnext token prediction from the expanded model vocabulary. We train our generator LM on a diverse\ncollection of text interleaved with reflection tokens and retrieved passages. Reflection tokens, inspired\nby reward models used in reinforcement learning (Ziegler et al., 2019; Ouyang et al., 2022), are\ninserted offline into the original corpus by a trained critic model. This eliminates the need to host a\ncritic model during training, reducing overhead. The critic model, in part, is supervised on a dataset\nof input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e.,\nGPT-4; OpenAI 2023). While we draw inspiration from studies that use control tokens to start and\nguide text generation (Lu et al., 2022; Keskar et al., 2019), our trained LM uses critique tokens to\nassess its own predictions after each generated segment as an integral part of the generation output.\nSELF -RAG further enables a customizable decoding algorithm to satisfy hard or soft constraints,\nwhich are defined by reflection token predictions. In particular, our inference-time algorithm enables\nus to (1) flexibly adjust retrieval frequency for different downstream applications and (2) customize\nmodels\u2019 behaviors to user preferences by leveraging reflection tokens through segment-level beam\nsearch using the weighted linear sum of the reflection token probabilities as segment score.\nEmpirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF -\nRAG significantly outperforms pre-trained and instruction-tuned LLMs that have more parameters and\nwidely adopted RAG approaches with higher citation accuracy. In particular, SELF -RAG outperforms\nretrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois\net al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with\nreflection tokens for overall performance improvements as well as test-time model customizations\n(e.g., balancing the trade-off between citation previsions and completeness).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb6ac9e0-df2d-43d3-8098-51bccd70affc": {"__data__": {"id_": "cb6ac9e0-df2d-43d3-8098-51bccd70affc", "embedding": null, "metadata": {"page_label": "2", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f6b3f08-cf99-44da-9369-338a6365b30b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "fe39947021b130e579e155834fdb4b3c46b6efe66105324b18307f60b5e09f58", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65977908-800a-4f24-9295-9fb50c47da63", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "6e23a690d248fa6268bf01cfdddd84244b2c4ba8d21deafbcacc5119ea60d892", "class_name": "RelatedNodeInfo"}}, "text": "Empirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF -\nRAG significantly outperforms pre-trained and instruction-tuned LLMs that have more parameters and\nwidely adopted RAG approaches with higher citation accuracy. In particular, SELF -RAG outperforms\nretrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois\net al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with\nreflection tokens for overall performance improvements as well as test-time model customizations\n(e.g., balancing the trade-off between citation previsions and completeness).\n2 R ELATED WORK\nRetrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) augments the input\nspace of LMs with retrieved text passages (Guu et al., 2020; Lewis et al., 2020), leading to large\nimprovements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram\net al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number\n2", "mimetype": "text/plain", "start_char_idx": 3690, "end_char_idx": 4761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c303ac3a-0f9e-47fc-9ee3-c304a1211b67": {"__data__": {"id_": "c303ac3a-0f9e-47fc-9ee3-c304a1211b67", "embedding": null, "metadata": {"page_label": "3", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "31f060e9-ff3a-48b7-83c9-724b39d49613", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "22dc2aa6433501b4da4c0f92f0f4a10f54e6c1cd0b540063ef813bb9d3dd969f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e77d204-2d3e-469f-a933-4798c6863343", "node_type": "1", "metadata": {}, "hash": "9b845391d352433ee8bdb5318360676329ff260754189d4abd11830a5e61995e", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nof retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few-\nshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only\nonce at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\non top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named\nentities. Yet, the improved task performance of such approaches often comes at the expense of\nruntime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of\nattributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to\nlearn to use retrieval on-demand for diverse instruction-following queries and introduce controlled\ngeneration guided by reflections tokens to further improve generation quality and attributions.\nConcurrent RAG work. A few concurrent works2 on RAG propose new training or prompting\nstrategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever\nand LM on instruction-tuning datasets in two steps. While we also train our model on diverse\ninstruction-following datasets, SELF -RAG enables retrieval on demand and selection of the best\npossible model output via fine-grained self-reflection, making it widely applicable and more robust\nand controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use\na summarization model to filter out or compress retrieved passages before using them to prompt the\nLM to generate the output. SELF -RAG processes passages in parallel and filters out irrelevant ones\nthrough self-reflection, without relying on external models at inference. Moreover, our self-reflection\nmechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou\net al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks\nand to generate with tree search, guided by LM-generated value scores. While their value function\nsimply indicates an overall score of each generation, SELF -RAG trains to an arbitrary LM to learn to\ngenerate fine-grained self-reflection and customizable inference.\nTraining and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal\nPolicy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven\neffective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce\nfine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique\non retrieval and generation, we train our target LM on task examples augmented with reflection\ntokens from a critic model offline, with a far lower training cost compared to RLHF. In addition,\nreflection tokens in SELF -RAG enable controllable generation at inference, while RLHF focuses on\nhuman preference alignment during training. Other works use general control tokens to guide LM\ngeneration (Lu et al., 2022; Korbak et al., 2023), whileSELF -RAG uses reflection tokens to decide the\nneed for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluation-\nguided decoding framework, but they focus only on reasoning tasks with one evaluation dimension\n(reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala\net al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural\nlanguage feedback and refined task output iteratively, but at the cost of inference efficiency.\n3 S ELF -RAG: L EARNING TO RETRIEVE , GENERATE AND CRITIQUE\nWe introduce Self-Reflective Retrieval-Augmented Generation ( SELF -RAG), shown in Figure 1.\nSELF -RAG is a framework that enhances the quality and factuality of an LLM through retrieval and\nself-reflection, without sacrificing LLM\u2019s original creativity and versatility. Our end-to-end training\nlets an LM M generate text informed by retrieved passages, if needed, and criticize the output by\nlearning to generate special tokens.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e77d204-2d3e-469f-a933-4798c6863343": {"__data__": {"id_": "5e77d204-2d3e-469f-a933-4798c6863343", "embedding": null, "metadata": {"page_label": "3", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "31f060e9-ff3a-48b7-83c9-724b39d49613", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "22dc2aa6433501b4da4c0f92f0f4a10f54e6c1cd0b540063ef813bb9d3dd969f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c303ac3a-0f9e-47fc-9ee3-c304a1211b67", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "998f6d023ca7061e4e4532657f0b963ba4087aed44ef2763a76ef4c2aab92099", "class_name": "RelatedNodeInfo"}}, "text": "Recent work on LLM refinement (Dhuliawala\net al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural\nlanguage feedback and refined task output iteratively, but at the cost of inference efficiency.\n3 S ELF -RAG: L EARNING TO RETRIEVE , GENERATE AND CRITIQUE\nWe introduce Self-Reflective Retrieval-Augmented Generation ( SELF -RAG), shown in Figure 1.\nSELF -RAG is a framework that enhances the quality and factuality of an LLM through retrieval and\nself-reflection, without sacrificing LLM\u2019s original creativity and versatility. Our end-to-end training\nlets an LM M generate text informed by retrieved passages, if needed, and criticize the output by\nlearning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\nor confirm the output\u2019s relevance, support, or completeness. In contrast, common RAG approaches\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\n3.1 P ROBLEM FORMALIZATION AND OVERVIEW\nFormally, given input x, we train M to sequentially generate textual outputs y consisting of multiple\nsegments y = [y1, . . . , yT ], where yt indicates a sequence of tokens for the t-th segment.3 Generated\ntokens in yt include text from the original vocabulary as well as the reflection tokens (Table 1).\n2All work is arXived within a week of this preprint.\n3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\nsegment unit (i.e., sub-sentence).\n3", "mimetype": "text/plain", "start_char_idx": 3373, "end_char_idx": 4899, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b732bcb-5b93-4522-9b26-7501c5fe7f31": {"__data__": {"id_": "2b732bcb-5b93-4522-9b26-7501c5fe7f31", "embedding": null, "metadata": {"page_label": "4", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de25f2b7-b7a3-4cf1-80ba-053408f47823", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "c4551400b635fd0aed770b01474784fec462dd7b5fcf07098021167b64dc446a", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nType Input Output Definitions\nRetrieve x / x, y {yes, no, continue} Decides when to retrieve with R\nISREL x, d {relevant, irrelevant} d provides useful information to solve x.\nISSUP x, d, y {fully supported, partially\nsupported, no support}\nAll of the verification-worthy statement in y\nis supported by d.\nISUSE x, y {5, 4, 3, 2, 1} y is a useful response to x.\nTable 1: Four types of reflection tokens used inSELF -RAG. Each type uses several tokens to represent\nits output values. The bottom three rows are three types of Critique tokens, and the bold text indicates\nthe most desirable critique tokens. x, y, dindicate input, output, and a relevant passage, respectively.\nAlgorithm 1 SELF -RAG Inference\nRequire: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , dN }\n1: Input: input prompt x and preceding generation y<t, Output: next output segment yt\n2: M predicts Retrieve given (x, y<t)\n3: if Retrieve == Yes then\n4: Retrieve relevant text passages D using R given (x, yt\u22121) \u25b7 Retrieve\n5: M predicts ISREL given x, dand yt given x, d, y<t for each d \u2208 D \u25b7 Generate\n6: M predicts ISSUP and ISUSE given x, yt, dfor each d \u2208 D \u25b7 Critique\n7: Rank yt based on ISREL , ISSUP , ISUSE \u25b7 Detailed in Section 3.3\n8: else if Retrieve == No then\n9: Mgen predicts yt given x \u25b7 Generate\n10: Mgen predicts ISUSE given x, yt \u25b7 Critique\nInference overview. Figure 1 and Algorithm 1 present an overview of SELF -RAG at inference. For\nevery x and preceding generation y<t, the model decodes a retrieval token to evaluate the utility\nof retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\nstandard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\npassage\u2019s relevance, the next response segment, and a critique token to evaluate if the information in\nthe response segment is supported by the passage. Finally, a new critique token evaluates the overall\nutility of the response.4 To generate each segment, SELF -RAG processes multiple passages in parallel\nand uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control\n(Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages\nd1 is selected at the first time step since d2 does not provide direct evidence ( ISREL is Irrelevant)\nand d3 output is only partially supported while d1 are fully supported.\nTraining overview. SELF -RAG enables an arbitrary LM to generate text with reflection tokens\nby unifying them as next token predictions from the expanded model vocabulary (i.e., the original\nvocabulary plus reflection tokens). Specifically, we train the generator model M on a curated corpus\nwith interleaving passages retrieved by a retriever R and reflection tokens predicted by a critic model\nC (summarized in Appendix Algorithm 2). We train C to generate reflection tokens for evaluating\nretrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we\nupdate the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we\ntrain the final generator model (M) using the conventional LM objective (Section 3.2.2) to enable\nM to generate reflection tokens by itself without relying on the critic at inference time.\n3.2 S ELF -RAG TRAINING\nHere, we describe the supervised data collection and training of two models, the criticC (Section 3.2.1)\nand the generator M (Section 3.2.2).\n3.2.1 T RAINING THE CRITIC MODEL\nData collection for critic model. Manual annotation of reflection tokens for each segment is\nexpensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively\n4We follow Liu et al. (2023a) in using a \u201cperceived\u201d utility value that is independent of retrieved passages.\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3834, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38c7dee0-b302-4558-b0a3-458f6dddfdcb": {"__data__": {"id_": "38c7dee0-b302-4558-b0a3-458f6dddfdcb", "embedding": null, "metadata": {"page_label": "5", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66f4c94e-ebda-460b-b057-e2956e0cc581", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "25de27f4660a60f521474b693f0f79168ebfa7b5ffd4eb11c9cad28b1d6a88aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b3dcc69-3fcf-433b-b378-86cb258bb995", "node_type": "1", "metadata": {}, "hash": "17ef4a5044c6582a9e569cacea8e83ffbf6f927525a19de84dc8b0f01a657a30", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nInput: How did US states get their names? Input: Write an essay of your best summer vacation\nOutput: My best summer vacation was a magical escape \nto the coastal town of Santorini. The azure waters, \ncharming white-washed building are unforgettable. \nCritic LM\nOutput: 1 of 50 states names come from persons. For instance, Louisiana was named in honor \nof King Louis XIV of France and Georgia was named after King George II. \nRetrieve\nPartially\nAugmented Output:                Retrieve\n<p>LOUISIANA: Named in\n<p>Of the \ufb01fty states, eleven are named after an individual person</p>.  \n             11 of 50 states\u2019 names come from person. Relevant Supported\nhonor of Louis XIV of France.</p>.  Relevant For instance, Louisiana was named after King Louis XIV, and\nUtil: 5Georgia was named after King George II. \nUtil: 5\nAugmented Output:                     My best summer \nvacation was a magical escape to the coastal town of \nSantorini.                     The azure waters, charming white-\nwashed building are unforgettable experience.\nNo Retrieval\nNo Retrieval\nRetriever\nFigure 2: SELF -RAG training examples. The left example does not require retrieval while the right\none requires retrieval; thus, passages are inserted. More examples are in Appendix Table 4.\nused to generate such feedback (Liu et al., 2023b). However, depending on such proprietary LMs\ncan raise API costs and diminish reproducibility (Chen et al., 2023). We create supervised data by\nprompting GPT-4 to generate reflection tokens and then distill their knowledge into an in-houseC.\nFor each group of reflection tokens, we randomly sample instances from the original training data:\n{Xsample, Ysample} \u223c {X, Y}. As different reflection token groups have their own definitions and\ninput, as shown in Table 1, we use different instruction prompts for them. Here, we use Retrieve as\nan example. We prompt GPT-4 with a type-specific instruction (\u201cGiven an instruction, make a\njudgment on whether finding some external documents from the web helps to generate a better\nresponse.\u201d) followed by few-shot demonstrations I the original task input x and output y to predict\nan appropriate reflection token as text: p(r|I, x, y). Manual assessment reveals that GPT-4 reflection\ntoken predictions show high agreement with human evaluations. We collect 4k-20k supervised\ntraining data for each type and combine them to form training data for C. Appendix Section D shows\nthe full list of instructions, and A.1 contains more details and our analysis.\nCritic learning. After we collect training data Dcritic, we initialize C with a pre-trained LM and\ntrain it on Dcritic using a standard conditional language modeling objective, maximizing likelihood:\nmax\nC\nE((x,y),r)\u223cDcritic log pC(r|x, y), rfor reflection tokens. (1)\nThough the initial model can be any pre-trained LM, we use the same one as the generator LM\n(i.e., Llama 2-7B; Touvron et al. 2023) for C initialization. The critic achieves a higher than 90%\nagreement with GPT-4-based predictions on most reflection token categories (Appendix Table 5).\n3.2.2 T RAINING THE GENERATOR MODEL\nData collection for generator. Given an input-output pair (x, y), we augment the original output\ny using the retrieval and critic models to create supervised data that precisely mimics the SELF -\nRAG inference-time process (Section 3.1). For each segment yt \u2208 y, we run C to assess whether\nadditional passages could help to enhance generation. If retrieval is required, the retrieval special\ntoken Retrieve =Yes is added, and R retrieves the top K passages, D. For each passage, C further\nevaluates whether the passage is relevant and predicts ISREL . If a passage is relevant, C further\nevaluates whether the passage supports the model generation and predicts ISSUP . Critique tokens\nISREL and ISSUP are appended after the retrieved passage or generations. At the end of the output, y\n(or yT ), C predicts the overall utility token ISUSE , and an augmented output with reflection tokens\nand the original input pair is added to Dgen. See the example training data in Figure 2.\nGenerator learning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b3dcc69-3fcf-433b-b378-86cb258bb995": {"__data__": {"id_": "4b3dcc69-3fcf-433b-b378-86cb258bb995", "embedding": null, "metadata": {"page_label": "5", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66f4c94e-ebda-460b-b057-e2956e0cc581", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "25de27f4660a60f521474b693f0f79168ebfa7b5ffd4eb11c9cad28b1d6a88aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38c7dee0-b302-4558-b0a3-458f6dddfdcb", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "31aa683e4a438a954a2d19314773aa4f60ec60e00140b5384dae506fea3fc168", "class_name": "RelatedNodeInfo"}}, "text": "For each segment yt \u2208 y, we run C to assess whether\nadditional passages could help to enhance generation. If retrieval is required, the retrieval special\ntoken Retrieve =Yes is added, and R retrieves the top K passages, D. For each passage, C further\nevaluates whether the passage is relevant and predicts ISREL . If a passage is relevant, C further\nevaluates whether the passage supports the model generation and predicts ISSUP . Critique tokens\nISREL and ISSUP are appended after the retrieved passage or generations. At the end of the output, y\n(or yT ), C predicts the overall utility token ISUSE , and an augmented output with reflection tokens\nand the original input pair is added to Dgen. See the example training data in Figure 2.\nGenerator learning. We train the generator model M by training on the curated corpus augmented\nwith reflection tokens Dgen using the standard next token objective:\nmax\nM\nE(x,y,r)\u223cDgen log pM(y, r|x). (2)\nUnlike C training (Eq. 1), M learns to predict the target output as well as the reflection tokens. During\ntraining, we mask out the retrieved text chunks (surrounded by <p> and </p> in Figure 2) for loss\ncalculation and expand the original vocabulary V with a set of reflection tokens { Critique , Retrieve }.\nConnections to prior work on learning with critique. Recent work incorporates additional\ncritique (feedback) during training, e.g., RLHF (Ouyang et al. 2022) via PPO. While PPO relies on\n5", "mimetype": "text/plain", "start_char_idx": 3348, "end_char_idx": 4789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7843b5a-427f-4d2d-9a26-804a34e7d2d6": {"__data__": {"id_": "e7843b5a-427f-4d2d-9a26-804a34e7d2d6", "embedding": null, "metadata": {"page_label": "6", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1ed27771-4fba-4efb-ba51-eaf25a346d2f", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "f72e274a969a631229a21d338259dad4f50463c181cd195c7e1ff5a6a644d239", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e833b503-49b1-4528-8f30-bc99b6eaf50e", "node_type": "1", "metadata": {}, "hash": "af25321c69f5e5446ccaf48443ea5d405943827eb85f979e040126dbd8fc2f1e", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nseparate reward models during training, we compute critique offline and directly insert them into the\ntraining corpus, where the generator LM is trained with a standard LM objective. This significantly\nreduces training costs compared to PPO. Our work also relates to prior work that incorporates special\ntokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). OurSELF -RAG\nlearns to generate special tokens to evaluate its own predictionafter each generated segment, enabling\nthe use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\n3.3 S ELF -RAG INFERENCE\nGenerating reflection tokens to self-evaluate its own output makes SELF -RAG controllable during the\ninference phase, enabling it to tailor its behavior to diverse task requirements. For tasks demanding\nfactual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to\nensure that the output aligns closely with the available evidence. Conversely, in more open-ended\ntasks, like composing a personal experience essay, the emphasis shifts towards retrieving less and\nprioritizing the overall creativity or utility score. In this section, we describe approaches to enforce\ncontrol to meet these distinct objectives during the inference process.\nAdaptive retrieval with threshold.SELF -RAG dynamically decides when to retrieve text passages by\npredicting Retrieve . Alternatively, our framework allows a threshold to be set. Specifically, if the prob-\nability of generating the Retrieve =Yes token normalized over all output tokens in Retrieve surpasses a\ndesignated threshold, we trigger retrieval (details in Appendix Section A.3).\nTree-decoding with critique tokens. At each segment step t, when retrieval is required, based either\non hard or soft conditions, R retrieves K passages, and the generator M processes each passage in\nparallel and outputs K different continuation candidates. We conduct a segment-level beam search\n(with the beam size=B) to obtain the top-B segment continuations at each timestamp t, and return\nthe best sequence at the end of generation. The score of each segment yt with respect to passage d is\nupdated with a critic score S that is the linear weighted sum of the normalized probability of each\nCritique token type. For each critique token group G (e.g., ISREL ), we denote its score at timestamp\nt as sG\nt , and we compute a segment score as follows:\nf(yt, d,Critique ) =p(yt|x, d, y<t)) +S( Critique ), where (3)\nS( Critique ) =\nX\nG\u2208G\nwGsG\nt for G = { ISREL , ISSUP , ISUSE }, (4)\nwhere sG\nt = pt(\u02c6r)PNG\ni=1 pt(ri)\nstands for the generation probability of the most desirable reflection token\n\u02c6r (e.g., ISREL =Relevant) for the critique token type G with NG distinct tokens (that represent\ndifferent possible values for G). The weights wG in Eq. 4 are hyperparameters that can be adjusted\nat inference time to enable customized behaviors at test time. For instance, to ensure that result\ny is mostly supported by evidence, we can set a weight term for the ISSUP score higher, while\nrelatively lowering weights for other aspects. Alternatively, we could further enforce hard constraints\nduring decoding using Critique . Instead of using a soft reward function in Eq. 4, we could explicitly\nfilter out a segment continuation when the model generates an undesirable Critique token (e.g.,\nISSUP =No support) . Balancing the trade-off between multiple preferences has been studied\nin RLHF (Touvron et al., 2023; Wu et al., 2023), which often requires training to change models\u2019\nbehaviors. SELF -RAG tailors an LM with no additional training.\n4 E XPERIMENTS\n4.1 T ASKS AND DATASETS\nWe conduct evaluations of our SELF -RAG and diverse baselines on a range of downstream tasks,\nholistically evaluating outputs with metrics designed to assess overall correctness, factuality, and\nfluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instruc-\ntions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of\nour experiments\u2019 settings, including test-time instructions, are available in the Appendix Section B.1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4177, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e833b503-49b1-4528-8f30-bc99b6eaf50e": {"__data__": {"id_": "e833b503-49b1-4528-8f30-bc99b6eaf50e", "embedding": null, "metadata": {"page_label": "6", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1ed27771-4fba-4efb-ba51-eaf25a346d2f", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "f72e274a969a631229a21d338259dad4f50463c181cd195c7e1ff5a6a644d239", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7843b5a-427f-4d2d-9a26-804a34e7d2d6", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "662065620c0983f7c37476524ff0e6977d83065e9efa27b8b40958c1d36ac675", "class_name": "RelatedNodeInfo"}}, "text": "Balancing the trade-off between multiple preferences has been studied\nin RLHF (Touvron et al., 2023; Wu et al., 2023), which often requires training to change models\u2019\nbehaviors. SELF -RAG tailors an LM with no additional training.\n4 E XPERIMENTS\n4.1 T ASKS AND DATASETS\nWe conduct evaluations of our SELF -RAG and diverse baselines on a range of downstream tasks,\nholistically evaluating outputs with metrics designed to assess overall correctness, factuality, and\nfluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instruc-\ntions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of\nour experiments\u2019 settings, including test-time instructions, are available in the Appendix Section B.1.\nClosed-set tasks include two datasets, i.e., a factverification dataset about public health (PubHealth;\nZhang et al. 2023) and a multiple-choice reasoning dataset created from scientific exams ( ARC-\n6", "mimetype": "text/plain", "start_char_idx": 3405, "end_char_idx": 4379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8c5a05e-dfd3-404f-80fe-f1ad7052bf23": {"__data__": {"id_": "a8c5a05e-dfd3-404f-80fe-f1ad7052bf23", "embedding": null, "metadata": {"page_label": "7", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "211288c5-2a13-4149-b0db-ddb5f946ff17", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "64638a8eabf3a667e1bfa77663ab1580a41636dfac2ba7ce11e1ebabcb89224c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a430c60-6f3d-4fdc-b214-c8575cbc2516", "node_type": "1", "metadata": {}, "hash": "76fafa084d428382043d0199d2542cdf734ce69807548178ffbac675488eea70", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nChallenge; Clark et al. 2018). We use accuracy as an evaluation metric and report on the test set. We\naggregate the answer probabilities of target classes for both of these datasets (Appendix Section B.2).\nShort-form generations tasks include two open-domain question answering (QA) datasets,\nPopQA (Mallen et al., 2023) and TriviaQA-unfiltered (Joshi et al., 2017), where systems need\nto answer arbitrary questions about factual knowledge. For PopQA, we use the long-tail subset,\nconsisting of 1,399 rare entity queries whose monthly Wikipedia page views are less than 100. As the\nTriviaQA-unfiltered (open) test set is not publicly available, we follow prior work\u2019s validation and\ntest split (Min et al., 2019; Guu et al., 2020), using 11,313 test queries for evaluation. We evaluate\nperformance based on whether gold answers are included in the model generations instead of strictly\nrequiring exact matching, following Mallen et al. (2023); Schick et al. (2023).\nLong-form generation tasks include a biography generation task (Min et al., 2023) and a long-form\nQA task ALCE-ASQA Gao et al. (2023); Stelmakh et al. (2022). We use FactScore (Min et al.,\n2023) to evaluate biographies, and we use official metrics of correctness (str-em), fluency based on\nMAUVE (Pillutla et al., 2021), and citation precision and recall (Gao et al., 2023) for ASQA. 5\n4.2 B ASELINES\nBaselines without retrievals. We evaluate strong publicly available pre-trained LLMs,\nLlama27B,13B (Touvron et al., 2023), instruction-tuned models, Alpaca7B,13B (Dubois et al., 2023)\n(our replication based on Llama2); and models trained and reinforced using private data, Chat-\nGPT (Ouyang et al., 2022) and Llama2-chat 13B. For instruction-tuned LMs, we use the official\nsystem prompt or instruction format used during training if publicly available. We also compare our\nmethod to concurrent work, CoVE65B (Dhuliawala et al., 2023), which introduces iterative prompt\nengineering to improve the factuality of LLM generations.\nBaselines with retrievals. We evaluate models augmented with retrieval at test time or during training.\nThe first category includes standard RAG baselines, where an LM (Llama2, Alpaca) generates output\ngiven the query prepended with the top retrieved documents using the same retriever as in our system.\nIt also includes Llama2-FT, where Llama2 is fine-tuned on all training data we use without the\nreflection tokens or retrieved passages. We also report the result of retrieval-augmented baselines\nwith LMs trained with private data: Ret-ChatGPT and Ret-Llama2-chat, which deploy the same\naugmentation technique above, as well as perplexity.ai, an InstructGPT-based production search\nsystem. The second category includes concurrent methods that are trained with retrieved text\npassages, i.e., SAIL (Luo et al., 2023) to instruction-tune an LM on the Alpaca instruction-tuning\ndata with top retrieved documents inserted before instructions, and Toolformer (Schick et al., 2023)\nto pre-train an LM with API calls (e.g., Wikipedia APIs).6\n4.3 E XPERIMENTAL SETTINGS\nTraining data and settings. Our training data consists of diverse instruction-following input-output\npairs. In particular, we sample instances from Open-Instruct processed data (Wang et al., 2023) and\nknowledge-intensive datasets (Petroni et al., 2021; Stelmakh et al., 2022; Mihaylov et al., 2018). In\ntotal, we use 150k instruction-output pairs. We use Llama2 7B and 13B (Touvron et al., 2023) as\nour generator base LM, and we use Llama2 7B as our base critic LM. For the retriever model R, we\nuse off-the-shelf Contriever-MS MARCO (Izacard et al., 2022a) by default and retrieve up to ten\ndocuments for each input.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a430c60-6f3d-4fdc-b214-c8575cbc2516": {"__data__": {"id_": "8a430c60-6f3d-4fdc-b214-c8575cbc2516", "embedding": null, "metadata": {"page_label": "7", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "211288c5-2a13-4149-b0db-ddb5f946ff17", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "64638a8eabf3a667e1bfa77663ab1580a41636dfac2ba7ce11e1ebabcb89224c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8c5a05e-dfd3-404f-80fe-f1ad7052bf23", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "fd7e65f5f33c02205a47caf73a316a4e0d92b1dbde7151a4b01b6cf7b2f009ba", "class_name": "RelatedNodeInfo"}}, "text": "Our training data consists of diverse instruction-following input-output\npairs. In particular, we sample instances from Open-Instruct processed data (Wang et al., 2023) and\nknowledge-intensive datasets (Petroni et al., 2021; Stelmakh et al., 2022; Mihaylov et al., 2018). In\ntotal, we use 150k instruction-output pairs. We use Llama2 7B and 13B (Touvron et al., 2023) as\nour generator base LM, and we use Llama2 7B as our base critic LM. For the retriever model R, we\nuse off-the-shelf Contriever-MS MARCO (Izacard et al., 2022a) by default and retrieve up to ten\ndocuments for each input. More training details are in the Appendix Section B.1.\nInference settings. As a default configuration, we assign the weight terms ISREL , ISSUP , ISUSE\nvalues of 1.0, 1.0 and 0.5, respectively. To encourage frequent retrieval, we set the retrieval threshold\nto 0.2 for most tasks and to 0 for ALCE (Gao et al., 2023) due to citation requirements. We speed\nup inference using vllm (Kwon et al., 2023). At each segment level, we adopt a beam width of 2.\nFor a token-level generation, we use greedy decoding. By default, we use the top five documents\nfrom Contriever-MS MARCO (Izacard et al., 2022a); for biographies and open-domain QA, we\nuse additional top five documents retrieved by a web search engine, following Luo et al. (2023);\nfor ASQA, we use the author-provided top 5 documents by GTR-XXL (Ni et al., 2022) across all\nbaselines for a fair comparison.\n5https://github.com/princeton-nlp/ALCE\n6We report numbers using the results reported in the paper as the implementations are not available.\n7", "mimetype": "text/plain", "start_char_idx": 3096, "end_char_idx": 4687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c66aa112-8b1b-44d8-a2df-9b48ee326fa8": {"__data__": {"id_": "c66aa112-8b1b-44d8-a2df-9b48ee326fa8", "embedding": null, "metadata": {"page_label": "8", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d726df1-939d-456c-99cc-0ff9bd6fa13f", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "865759d071c730ec3190651649ea9d8cb2c645b4ded76c1dd14a5cab720dcb60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8f2cd42-0ed7-4539-af00-e815bf902a5a", "node_type": "1", "metadata": {}, "hash": "bf84b767a00a1691dbffcdac657cbc33627f8db551c5fbbab3193f6812bd0260", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nTable 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among\nnon-proprietary models, and gray-colored bold text indicates the best proprietary model when\nthey outperforms all non-proprietary models. \u2217 indicates concurrent or recent results reported by\nconcurrent work. \u2013 indicates numbers that are not reported by the original papers or are not applicable.\nModels are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\nrouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\nShort-form Closed-set Long-form generations (with citations)\nPopQA TQA Pub ARC Bio ASQA\nLM (acc) (acc) (acc) (acc) (FS) (em) (rg) (mau) (pre) (rec)\nLMs with proprietary data\nLlama2-c13B 20.0 59.3 49.4 38.4 55.9 22.4 29.6 28.6 \u2013 \u2013\nRet-Llama2-c13B 51.8 59.8 52.1 37.9 79.9 32.8 34.8 43.8 19.8 36.1\nChatGPT 29.3 74.3 70.1 75.3 71.8 35.3 36.2 68.8 \u2013 \u2013\nRet-ChatGPT 50.8 65.7 54.7 75.3 \u2013 40.7 39.9 79.7 65.1 76.6\nPerplexity.ai \u2013 \u2013 \u2013 \u2013 71.2 \u2013 \u2013 \u2013 \u2013 \u2013\nBaselines without retrieval\nLlama27B 14.7 30.5 34.2 21.8 44.5 7.9 15.3 19.0 \u2013 \u2013\nAlpaca7B 23.6 54.5 49.8 45.0 45.8 18.8 29.4 61.7 \u2013 \u2013\nLlama213B 14.7 38.5 29.4 29.4 53.4 7.2 12.4 16.0 \u2013 \u2013\nAlpaca13B 24.4 61.3 55.5 54.9 50.2 22.9 32.0 70.6 \u2013 \u2013\nCoVE65B * \u2013 \u2013 \u2013 \u2013 71.2 \u2013 \u2013 \u2013 \u2013 \u2013\nBaselines with retrieval\nToolformer*6B \u2013 48.8 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013\nLlama27B 38.2 42.5 30.0 48.0 78.0 15.2 22.1 32.0 2.9 4.0\nAlpaca7B 46.7 64.1 40.2 48.0 76.6 30.9 33.3 57.9 5.5 7.2\nLlama2-FT7B 48.7 57.3 64.3 65.8 78.2 31.0 35.8 51.2 5.0 7.5\nSAIL*7B \u2013 \u2013 69.2 48.4 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013\nLlama213B 45.7 47.0 30.2 26.0 77.5 16.3 20.5 24.7 2.3 3.6\nAlpaca13B 46.1 66.9 51.1 57.6 77.7 34.8 36.7 56.6 2.0 3.8\nOur SELF -RAG 7B 54.9 66.4 72.4 67.3 81.2 30.0 35.7 74.3 66.9 67.8\nOur SELF -RAG 13B 55.8 69.3 74.5 73.1 80.2 31.7 37.0 71.6 70.3 71.3\n5 R ESULTS AND ANALYSIS\n5.1 M AIN RESULTS\nComparison against baselines without retrieval. Table 2 (top) presents the baselines without\nretrieval.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1971, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8f2cd42-0ed7-4539-af00-e815bf902a5a": {"__data__": {"id_": "b8f2cd42-0ed7-4539-af00-e815bf902a5a", "embedding": null, "metadata": {"page_label": "8", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d726df1-939d-456c-99cc-0ff9bd6fa13f", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "865759d071c730ec3190651649ea9d8cb2c645b4ded76c1dd14a5cab720dcb60", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c66aa112-8b1b-44d8-a2df-9b48ee326fa8", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "a07ad7bf6693cb8fa84db186e2529856135f09b444fcebb5937b796ff59a8a3d", "class_name": "RelatedNodeInfo"}}, "text": "Table 2 (top) presents the baselines without\nretrieval. Our SELF -RAG (bottom two rows) demonstrates a substantial performance advantage\nover supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\nbiography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\na concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\ntask, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\niteratively prompts Llama265B to refine output.\nComparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF -RAG also\noutperforms existing RAG in many tasks, obtaining the best performance among non-proprietary\nLM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio,\npowerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from\ntheir non-retrieval baselines. However, we found that these baselines provide limited solutions for\ntasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth\nand ARC-Challenge, baselines with retrieval do not improve performance notably from their no-\nretrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation\naccuracy. On ASQA, our model shows significantly higher citation precision and recall than all\nmodels except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy\nin this particular task, surpassing smaller LMs. Our SELF -RAG bridges this performance gap, even\noutperforming ChatGPT in citation precision, which measures whether the model-generated claim is\nfully supported by cited evidence. We also found that on the metrics for factual precision, SELF -RAG\n7B occasionally outperforms our 13B due to the tendency of smaller SELF -RAG to often generate\n8", "mimetype": "text/plain", "start_char_idx": 1916, "end_char_idx": 3842, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e780335a-b086-448b-bfd6-c1234b599d90": {"__data__": {"id_": "e780335a-b086-448b-bfd6-c1234b599d90", "embedding": null, "metadata": {"page_label": "9", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e30dacd8-a1b3-4a64-bdf3-07ac52a35a25", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "e4f8b254d44b4006a10f35c17fddce5cd0f58369e85694c0cc9b6409e1117159", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74bf5371-f584-4934-a790-ea07bcfd790e", "node_type": "1", "metadata": {}, "hash": "5f7dd94988b8c5073087cae974bed011060dcb4c3307b737bc2d2caa726f6297", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nPQA Med AS\n(acc) (acc) (em)\nSELF -RAG (50k) 45.5 73.5 32.1\nTraining\nNo Retriever R 43.6 67.8 31.0\nNo Critic C 42.6 72.0 18.1\nTest\nNo retrieval 24.7 73.0 \u2013\nHard constraints 28.3 72.6 \u2013\nRetrieve top1 41.8 73.1 28.6\nRemove ISSUP 44.1 73.2 30.6\n(a) Ablation\n1 2\n70.0\n70.5Precision\n1 2\nWeight for IsSupport\n90\n95Mauve\n (b) Customization\n0.0 0.2 0.4 0.6\n0.98\n0.99\n0.99\n1.00Accuracy\nPubHealth\n0.0 0.2 0.4 0.6\nRetrieval Threshold\n0.6\n0.8\n1.0Accuracy\nPopQA\n0.0\n0.5\n1.0\nFrequency\n0.25\n0.50\n0.75\n1.00\nFrequency\n (c) Retrieval\nFigure 3: Analysis on SELF -RAG: (a) Ablation studies for key components of SELF -RAG training\nand inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\nMauve (fluency). (c) Retrieval frequency and normalized accuracy on PubHealth and PopQA.\nprecisely grounded yet shorter outputs. Llama2-FT 7B, which is the baseline LM trained on the same\ninstruction-output pairs as SELF -RAG without retrieval or self-reflection and is retrieval-augmented\nat test time only, lags behind SELF -RAG. This result indicates S ELF -RAG gains are not solely from\ntraining data and demonstrate the effectiveness of SELF -RAG framework.\n5.2 A NALYSIS\nAblation studies. We conduct a set of ablations of our framework to identify which factors play\nkey roles. We evaluate two model variants trained differently than our model: No Retriever trains an\nLM using the standard instruction-following method given instruction-output pairs, without retrieved\npassages; No Critic trains an LM trained with input-output pairs that are always augmented with the\ntop one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\nwe use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\nSAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\nretrieval during inference; Hard constraints indicates the model performance that retrieves when\nRetrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\ntop one document only, similar to standard RAG approaches;Remove ISSUP indicates the model\nperformance that removes ISSUP score only during critique-guided beam search in Eq. 4. In this\nablation experiment, we use a training instance size of 50k for a more efficient exploration of training\nvariations. Later in this section, we conduct an analysis of the effect of training data size. We conduct\nthe ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\non sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\nWe show in Table 3a the ablation results. The top part of the table shows results for training ablations,\nand the bottom part is for inference ablations. We see that all components play important roles. We\nalso observe a large performance gap between SELF -RAG and No Retriever or Critic baselines across\ntasks, indicating that training an LM with those models largely contributes to the performance gain of\nSELF -RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\nRAG approaches causes a large drop in PopQA and ASQA, and removing ISSUP during the beam\nsearch results hurts performance on ASQA. This demonstrates the effectiveness of SELF -RAG\u2019s\ncapabilities of carefully selecting generations based fine-grained multiple criterion, instead of naively\nusing all of the top passages from the retrieval model or solely depending on relevance scores.\nEffects of inference-time customization.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74bf5371-f584-4934-a790-ea07bcfd790e": {"__data__": {"id_": "74bf5371-f584-4934-a790-ea07bcfd790e", "embedding": null, "metadata": {"page_label": "9", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e30dacd8-a1b3-4a64-bdf3-07ac52a35a25", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "e4f8b254d44b4006a10f35c17fddce5cd0f58369e85694c0cc9b6409e1117159", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e780335a-b086-448b-bfd6-c1234b599d90", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "043db3f5b19de24cf703f135e0153fb1aac78d228fe3776ff88bfc1851645d4d", "class_name": "RelatedNodeInfo"}}, "text": "We show in Table 3a the ablation results. The top part of the table shows results for training ablations,\nand the bottom part is for inference ablations. We see that all components play important roles. We\nalso observe a large performance gap between SELF -RAG and No Retriever or Critic baselines across\ntasks, indicating that training an LM with those models largely contributes to the performance gain of\nSELF -RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\nRAG approaches causes a large drop in PopQA and ASQA, and removing ISSUP during the beam\nsearch results hurts performance on ASQA. This demonstrates the effectiveness of SELF -RAG\u2019s\ncapabilities of carefully selecting generations based fine-grained multiple criterion, instead of naively\nusing all of the top passages from the retrieval model or solely depending on relevance scores.\nEffects of inference-time customization. One key benefit of our proposed framework is that it\nenables us to control how much each critique type affects the final generation sampling. We analyze\nthe effects of different parameter weights on the top of our 7B model during inference time on\nASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing\nthe weighting term for ISSUP , which criticizes how supported the output is by the text passage. As\nthe figure shows, increasing the weight leads to positive effects on the models\u2019 citation precision\nsince this puts more emphasis on whether model generation is supported by the evidence. On the\n9", "mimetype": "text/plain", "start_char_idx": 2672, "end_char_idx": 4245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "685d07e4-b022-4a17-a856-7717d4daf536": {"__data__": {"id_": "685d07e4-b022-4a17-a856-7717d4daf536", "embedding": null, "metadata": {"page_label": "10", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7aab6e5a-8007-4760-aa2d-c59bdb1d7cdf", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "5572624d4328fa6d7a46ab3c9b938dcf8b2c76918dfa472e1e43c54fe5c083af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d3cecd9-7eb6-4659-854a-a5024d6fc392", "node_type": "1", "metadata": {}, "hash": "f71f5e28185bbd330366543edc11b12f604d048fd74fb5dae965b3f2cf2b087f", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\n0 50 100 150\nNum of training (k)\n35\n40\n45\n50\n55Perfomance\n(a) PopQA\n0 100\nNum of training (k)\n71\n72\n73\n (b) PubHealth\n0 100\nNum of training (k)\n40\n60\n (c) ASQA (prec)\nPop Bio.\nS & P 92.5 70.0\nISREL 95.0 90.0\nISSUP 90.0 85.0\n(d) Human evaluation on PopQA\nand Bio generation.\nFigure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect\nof the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)\nHuman analysis on SELF -RAG outputs as well as reflection tokens.\ncontrary, a larger weight results in lower MAUVE scores: when generation gets longer and more\nfluent, there are often more claims that are not fully supported by citations, consistent with findings\nby Liu et al. (2023a). Our framework lets practitioners choose and customize models\u2019 behaviors at\ntest time by adjusting such parameters without requiring additional training.\nEfficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval\noccurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects\noverall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers\nof threshold \u03b4 (larger \u03b4 results in less retrieval) on PubHealth and PopQA. Figure 3c shows that\nthe model\u2019s retrieval frequencies dramatically change on both datasets. as \u03b4 varies. On one hand,\nperformance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\nEffects of training data size. We conduct an analysis of how the data scale affects the model\u2019s\nperformance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\n150k training instances, and fine-tune four SELF -RAG 7B variants on those subsets. Then, we compare\nthe model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF -\nRAG trained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models\u2019\nperformance trained on different amount of data. Across all datasets, increasing data size often shows\nupward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do\nnot observed such significant improvements on Llama2-FT7B when increasing the training data from\n50k to 150k. These results also indicate that further expanding the training data of SELF -RAG may\nlead to further improvements, although in this work we limit our training data size to 150k.\nHuman evaluations. We conduct small human evaluations on SELF -RAG outputs, as well as the\nreliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio\nresults. Following Menick et al. (2022), human annotators evaluate S&P, which indicates whether\nthe model output is plausible (i.e., the output is a reasonable and on-topic response to the question\nas if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to\nverify the validity of the answer). For S&P, we do not consider the instances where SELF -RAG\npredicts irrelevant or no support. We then ask our annotators whether the model-predicted\nreflection tokens about ISREL and ISSUP match their inspections (e.g., whether the fully supported\noutput is supported by the cited evidence). Human annotators find SELF -RAG answers are often\nplausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\nconsistent with Menick et al. (2022). Human annotators also find ISREL and ISSUP reflection token\npredictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\nexamples and explanations on assessments.\n6 C ONCLUSION\nThis work introduces SELF -RAG, a new framework to enhance the quality and factuality of LLMs\nthrough retrieval on demand and self-reflection.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3837, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d3cecd9-7eb6-4659-854a-a5024d6fc392": {"__data__": {"id_": "1d3cecd9-7eb6-4659-854a-a5024d6fc392", "embedding": null, "metadata": {"page_label": "10", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7aab6e5a-8007-4760-aa2d-c59bdb1d7cdf", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "5572624d4328fa6d7a46ab3c9b938dcf8b2c76918dfa472e1e43c54fe5c083af", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "685d07e4-b022-4a17-a856-7717d4daf536", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "c678a8006d55b1f84401a1bc35cd8df716b1c9e181565a0b848930844ce9fb55", "class_name": "RelatedNodeInfo"}}, "text": "For S&P, we do not consider the instances where SELF -RAG\npredicts irrelevant or no support. We then ask our annotators whether the model-predicted\nreflection tokens about ISREL and ISSUP match their inspections (e.g., whether the fully supported\noutput is supported by the cited evidence). Human annotators find SELF -RAG answers are often\nplausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\nconsistent with Menick et al. (2022). Human annotators also find ISREL and ISSUP reflection token\npredictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\nexamples and explanations on assessments.\n6 C ONCLUSION\nThis work introduces SELF -RAG, a new framework to enhance the quality and factuality of LLMs\nthrough retrieval on demand and self-reflection. SELF -RAG trains an LM to learn to retrieve, generate,\nand critique text passages and its own generation by predicting the next tokens from its original\nvocabulary as well as newly added special tokens, called reflection tokens.SELF -RAG further enables\nthe tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\nsix tasks using multiple metrics demonstrate that SELF -RAG significantly outperforms LLMs with\nmore parameters or with conventional retrieval-augmented generation approaches.\n10", "mimetype": "text/plain", "start_char_idx": 3006, "end_char_idx": 4374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc72ac2f-18f0-4268-8e58-dcb42dc5214e": {"__data__": {"id_": "cc72ac2f-18f0-4268-8e58-dcb42dc5214e", "embedding": null, "metadata": {"page_label": "11", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eb6d4eb8-f779-4455-ab83-71f04ad97cc4", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "8c65987194ddbaf1d6195fd778a6742ccb9ec6d33338d585bd4b7d8a7ea6b0fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7db51537-9b7c-4734-a6a1-f79558b98f44", "node_type": "1", "metadata": {}, "hash": "7acf18561742051a223ef0b251dd71d2ea14393432ecf3db3a3ad21744fa3585", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nETHICAL CONCERNS\nThis work aims to improve the factuality of LLM outputs, the lack of which continues to cause nu-\nmerous real-world problems (e.g., spread of misinformation and provision of incorrect and dangerous\nadvice). While our method shows significant improvements in terms of performance, factuality, and\ncitation accuracy, it can still generate outputs that are not fully supported by the citations. We hope\nthat explicit self-reflection and fine-grained attribution may help users verify factual errors in the\nmodel outputs.\nACKNOWLEDGMENTS\nWe thank Sewon Min, Scott Wen-tau Yih, Sean Welleck, and Kawin Ethayarajh for fruitful discussions\nin the early stages of this work. We thank Sewon Min, Joongwon (Daniel) Kim, and Sandy Kaplan\nfor valuable feedback on the paper, and Tianyu Gao and Weijia Shi for their help on evaluations.\nAkari Asai is supported by the IBM Fellowship. We thank Stability AI for providing computing\nto train and evaluate the LMs in this work, and Microsoft Accelerate Foundation Models Research\nProgram for the access to OpenAI APIs. This work was funded in part by the DARPA MCS program\nthrough NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and gifts from AI2.\nREFERENCES\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learn-\ning to retrieve reasoning paths over wikipedia graph for question answering. In International\nConference on Learning Representations, 2020. URL https://openreview.net/forum?\nid=SJgVHkrYDH.\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and appli-\ncations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Tutorial), 2023a. URL https://aclanthology.org/2023.acl-tutorials.6.\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh\nHajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. In Findings of the Associ-\nation for Computational Linguistics , 2023b. URL https://aclanthology.org/2023.\nfindings-acl.225.\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob\nEisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. Attributed question answering:\nEvaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037,\n2022. URL https://arxiv.org/abs/2212.08037.\nLingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt\u2019s behavior changing over time? arXiv\npreprint arXiv:2307.09009, 2023. URL https://arxiv.org/abs/2307.09009.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. In Advances in Neural Information Processing Systems,\n2022. URL https://openreview.net/forum?id=H4DqfPSibmx.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. Chain-of-verification reduces hallucination in large language models.arXiv preprint\narXiv:2309.11495, 2023. URL https://arxiv.org/abs/2309.11495.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7db51537-9b7c-4734-a6a1-f79558b98f44": {"__data__": {"id_": "7db51537-9b7c-4734-a6a1-f79558b98f44", "embedding": null, "metadata": {"page_label": "11", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eb6d4eb8-f779-4455-ab83-71f04ad97cc4", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "8c65987194ddbaf1d6195fd778a6742ccb9ec6d33338d585bd4b7d8a7ea6b0fa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc72ac2f-18f0-4268-8e58-dcb42dc5214e", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "1b998a1c635a6e6f37ba7f89b315d33196fc279e78f011728807104b713db787", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. In Advances in Neural Information Processing Systems,\n2022. URL https://openreview.net/forum?id=H4DqfPSibmx.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. Chain-of-verification reduces hallucination in large language models.arXiv preprint\narXiv:2309.11495, 2023. URL https://arxiv.org/abs/2309.11495.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of\nwikipedia: Knowledge-powered conversational agents. In International Conference on Learning\nRepresentations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that\n11", "mimetype": "text/plain", "start_char_idx": 2744, "end_char_idx": 3781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c605b43d-3828-46d1-82be-b1228d343cd3": {"__data__": {"id_": "c605b43d-3828-46d1-82be-b1228d343cd3", "embedding": null, "metadata": {"page_label": "12", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ff2ec0f-515d-4cf3-a152-beb3dc3f5428", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "af50b89c27654da3274d265c67135462168dff75563a9f1ff2caabf4c20abd5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c2e0021-e472-4709-8c41-fde361665cc2", "node_type": "1", "metadata": {}, "hash": "38aac9087c2330d0b9fcad329a003e39b79195779351435b86e560e67433781b", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nlearn from human feedback. arXiv preprint arXiv:2305.14387, 2023. URL https://arxiv.\norg/abs/2305.14387.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate\ntext with citations. arXiv preprint arXiv:2305.14627, 2023. URL https://arxiv.org/abs/\n2305.14627.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented\nlanguage model pre-training. In International Conference on Machine Learning , 2020. URL\nhttps://dl.acm.org/doi/pdf/10.5555/3524938.3525306.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand\nJoulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning.\nTransactions on Machine Learning Research , 2022a. URL https://openreview.net/\nforum?id=jKN1pXi7b0.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\nretrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022b. URL https:\n//arxiv.org/abs/2208.03299.\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint\narXiv:2305.06983, 2023. URL https://arxiv.org/abs/2305.06983.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017. URL\nhttps://aclanthology.org/P17-1147.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher.\nCtrl: A conditional transformer language model for controllable generation. arXiv preprint\narXiv:1909.05858, 2019. URL https://arxiv.org/abs/1909.05858.\nTomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason\nPhang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences.\nIn International Conference on Machine Learning, 2023. URL https://openreview.net/\nforum?id=AT8Iw8KOeC.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and\nSlav Petrov. Natural questions: A benchmark for question answering research. Transactions of\nthe Association for Computational Linguistics, 2019. URL https://aclanthology.org/\nQ19-1026.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles, 2023. URL https://arxiv.org/abs/2309.06180.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c2e0021-e472-4709-8c41-fde361665cc2": {"__data__": {"id_": "8c2e0021-e472-4709-8c41-fde361665cc2", "embedding": null, "metadata": {"page_label": "12", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ff2ec0f-515d-4cf3-a152-beb3dc3f5428", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "af50b89c27654da3274d265c67135462168dff75563a9f1ff2caabf4c20abd5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c605b43d-3828-46d1-82be-b1228d343cd3", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "e46cc09dab785648d209584f707eb39364f13d6d11a8cdca86c1949b51b4d1c8", "class_name": "RelatedNodeInfo"}}, "text": "Natural questions: A benchmark for question answering research. Transactions of\nthe Association for Computational Linguistics, 2019. URL https://aclanthology.org/\nQ19-1026.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles, 2023. URL https://arxiv.org/abs/2309.06180.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K\u00a8uttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00a8aschel, Sebastian Riedel, and Douwe Kiela.\nRetrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, 2020. URL https://proceedings.neurips.cc/paper/\n2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf.\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez,\nJacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. Ra-dit: Retrieval-\naugmented dual instruction tuning, 2023. URL https://arxiv.org/abs/2310.01352.\nNelson F Liu, Tianyi Zhang, and Percy Liang. Evaluating verifiability in generative search engines.\narXiv preprint arXiv:2304.09848, 2023a. URL https://arxiv.org/abs/2304.09848.\n12", "mimetype": "text/plain", "start_char_idx": 2498, "end_char_idx": 3871, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "219c7d5c-8799-4a31-bf1b-4b92afd5b816": {"__data__": {"id_": "219c7d5c-8799-4a31-bf1b-4b92afd5b816", "embedding": null, "metadata": {"page_label": "13", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd7cb11f-69ff-4a19-bb3d-6eb2a9d7d6ca", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "bda8550a9fb3bee322138e7c54a49b33b967b1f506018397089b7a2a17ec617d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49c016ca-6098-4fb8-a072-8ab16dd6c5de", "node_type": "1", "metadata": {}, "hash": "0d77a87710f5fe495b044a1f573e63b6923de95163efcb7f7140f08759dbd52b", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg\nevaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023b.\nURL https://arxiv.org/abs/2303.16634.\nXiming Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Am-\nmanabrolu, and Yejin Choi. QUARK: Controllable text generation with reinforced unlearning.\nIn Advances in Neural Information Processing Systems, 2022. URL https://openreview.\nnet/forum?id=5HaIds3ux5O.\nHongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox,\nHelen Meng, and James Glass. Sail: Search-augmented instruction learning. arXiv preprint\narXiv:2305.15225, 2023. URL https://arxiv.org/abs/2305.15225.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-\nrefine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. URL\nhttps://arxiv.org/abs/2303.17651.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.\nWhen not to trust language models: Investigating effectiveness of parametric and non-parametric\nmemories. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , 2023. URL https://aclanthology.org/2023.\nacl-long.546.\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching\nlanguage models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.\nURL https://arxiv.org/abs/2203.11147.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. InProceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing, 2018. URL https://aclanthology.\norg/D18-1260.\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. A discrete hard EM approach\nfor weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), 2019. URL https://aclanthology.org/\nD19-1284.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. arXiv preprint arXiv:2305.14251, 2023. URL https:\n//arxiv.org/abs/2305.14251.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49c016ca-6098-4fb8-a072-8ab16dd6c5de": {"__data__": {"id_": "49c016ca-6098-4fb8-a072-8ab16dd6c5de", "embedding": null, "metadata": {"page_label": "13", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd7cb11f-69ff-4a19-bb3d-6eb2a9d7d6ca", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "bda8550a9fb3bee322138e7c54a49b33b967b1f506018397089b7a2a17ec617d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "219c7d5c-8799-4a31-bf1b-4b92afd5b816", "node_type": "1", "metadata": {"page_label": "13", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "d0ea5a25c36a689254df9f752aa832c14d6a5f78136cac3c9e3eb52e5f52a6ec", "class_name": "RelatedNodeInfo"}}, "text": "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. arXiv preprint arXiv:2305.14251, 2023. URL https:\n//arxiv.org/abs/2305.14251.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. URL https:\n//arxiv.org/abs/2112.09332.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao,\nYi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable\nretrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, 2022. URL https://aclanthology.org/2022.emnlp-main.669.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https://arxiv.\norg/abs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\n13", "mimetype": "text/plain", "start_char_idx": 2490, "end_char_idx": 3828, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f83b4b3-47f1-4757-8858-f93f28b838ea": {"__data__": {"id_": "8f83b4b3-47f1-4757-8858-f93f28b838ea", "embedding": null, "metadata": {"page_label": "14", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c735fee-441e-4c0c-b648-3f8927f54a84", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "b46f5ef54d65cb1b4131d9706c72ca537e5dbb021fb1b7271cdbb36aca239357", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de8ccd1b-e2d3-490a-9d8a-db24a3092b7d", "node_type": "1", "metadata": {}, "hash": "55511a5673e25ef1ac0bf4dd332a08cf6d91023bb159b9ab235a1bbdb08ed5ee", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nRyan Lowe. Training language models to follow instructions with human feedback. InAdvances in\nNeural Information Processing Systems, 2022. URL https://openreview.net/forum?\nid=TG8KACxEON.\nDebjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West,\nand Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint\narXiv:2304.01904, 2023. URL https://arxiv.org/abs/2304.01904.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00a8aschel,\nand Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Proceedings\nof the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2021. URL https://aclanthology.org/\n2021.naacl-main.200.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi,\nand Zaid Harchaoui. MAUVE: Measuring the gap between neural text and human text using\ndivergence frontiers. In Advances in Neural Information Processing Systems, 2021. URL https:\n//openreview.net/forum?id=Tqx7nJp7PR.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations\ntoward training trillion parameter models. In Proceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Analysis, 2020. URL https://dl.acm.\norg/doi/10.5555/3433701.3433727.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and\nYoav Shoham. In-context retrieval-augmented language models. Transactions of the Association\nfor Computational Linguistics, 2023. URL https://arxiv.org/abs/2302.00083.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,\nAbheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao,\nStella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training\nenables zero-shot task generalization. In International Conference on Learning Representations,\n2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023. URL https://arxiv.org/abs/2302.\n04761.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017. URL https://arxiv.org/\nabs/1707.06347.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de8ccd1b-e2d3-490a-9d8a-db24a3092b7d": {"__data__": {"id_": "de8ccd1b-e2d3-490a-9d8a-db24a3092b7d", "embedding": null, "metadata": {"page_label": "14", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c735fee-441e-4c0c-b648-3f8927f54a84", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "b46f5ef54d65cb1b4131d9706c72ca537e5dbb021fb1b7271cdbb36aca239357", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f83b4b3-47f1-4757-8858-f93f28b838ea", "node_type": "1", "metadata": {"page_label": "14", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "f0607b92c95b1ed8986c1aad2141e90b562feb36f6fc7e95aee6b793b8829fbf", "class_name": "RelatedNodeInfo"}}, "text": "In International Conference on Learning Representations,\n2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023. URL https://arxiv.org/abs/2302.\n04761.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017. URL https://arxiv.org/\nabs/1707.06347.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael\nSch\u00a8arli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\nIn Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\n//proceedings.mlr.press/v202/shi23a.html.\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: Factoid questions meet long-\nform answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, 2022. URL https://aclanthology.org/2022.emnlp-main.566.\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-\nscale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), 2018. URL https://aclanthology.org/N18-1074.\n14", "mimetype": "text/plain", "start_char_idx": 2482, "end_char_idx": 4004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3f8c1af-7e9a-4b67-9c7b-9246b2116036": {"__data__": {"id_": "d3f8c1af-7e9a-4b67-9c7b-9246b2116036", "embedding": null, "metadata": {"page_label": "15", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45814271-f6ec-4320-b1e3-0ab05b675675", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "c1237047e55ea53e5bf0f45b7e95c915cdfd43d101ebf48b27f0e9e40583f9e5", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. URL https://arxiv.\norg/abs/2307.09288.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go?\nexploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023.\nURL https://arxiv.org/abs/2306.04751.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. InInternational\nConference on Learning Representations, 2022. URL https://openreview.net/forum?\nid=gEZrGCozdqR.\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A\nSmith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better\nrewards for language model training. arXiv preprint arXiv:2306.01693, 2023. URL https:\n//arxiv.org/abs/2306.01693.\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decom-\nposition enhances reasoning via self-evaluation guided decoding.arXiv preprint arXiv:2305.00633,\n2023. URL https://arxiv.org/abs/2305.00633.\nFangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with\ncompression and selective augmentation, 2023. URL https://arxiv.org/abs/2310.\n04408.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language\nmodels robust to irrelevant context, 2023. URL https://arxiv.org/abs/2310.01558.\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. Automatic evaluation of\nattribution by large language models. arXiv preprint arXiv:2305.06311, 2023. URL https:\n//arxiv.org/abs/2305.06311.\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen,\nXixin Wu, Danny Fox, Helen Meng, and James Glass. Interpretable unified language checking.\narXiv preprint arXiv:2304.03728, 2023. URL https://arxiv.org/abs/2304.03728.\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language\nagent tree search unifies reasoning acting and planning in language models, 2023. URL https:\n//arxiv.org/abs/2310.04406.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv\npreprint arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593.\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2dc58b05-c1c5-4e89-91c1-e72f86e3aa85": {"__data__": {"id_": "2dc58b05-c1c5-4e89-91c1-e72f86e3aa85", "embedding": null, "metadata": {"page_label": "16", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c3fc50ad-4f2d-4c95-ae46-5044eb9ce075", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "cb3bcda5528496156bdf3c86a9a63a95d7577d129b0ba9756fefce5ee533d13c", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nAPPENDIX\nA S ELF -RAG Details 17\nA.1 Reflection Tokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nA.2 S ELF -RAG Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nA.3 S ELF -RAG Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nB Experimental Details 19\nB.1 More Details of Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nB.2 More Details of Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nC Results 20\nC.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nC.2 Human Evaluation Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nC.3 Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nD Full List of Instructions and Demonstrations for GPT-4 21\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 895, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4655e0f3-39c6-4dd4-96d8-353eebc675fa": {"__data__": {"id_": "4655e0f3-39c6-4dd4-96d8-353eebc675fa", "embedding": null, "metadata": {"page_label": "17", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c02fa2c-5fa8-4429-b452-20051d98420a", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "6c31a1f69c709c3c4eeef472a375655def3ab4d5356ac8c863d1bab3fbf3dd11", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nA S ELF -RAG DETAILS\nA.1 R EFLECTION TOKENS .\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and\noutput tokens. The first three aspects will be provided at each segment level, while the final aspect is\nonly given at each output level.\n\u2022 Retrieval-on-demand ( Retrieve ): Given an input and previous-step generation (if applicable),\nan LM determines whether the continuation requires factual grounding. No indicates retrieval\nis unnecessary as the sequence does not require factual grounding or may not be enhanced by\nknowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\nto use evidence, which indicates that a model can continue to use the evidence retrieved\npreviously. For instance, a passage may contain rich factual information, and thus SELF -RAG\ngenerates multiple segments based on the passage.\n\u2022 Relevant ( ISREL ): Retrieved knowledge may not be always relevant to the input. This aspect\nindicates whether the evidence provides useful information (Relevant) or not (Irrelevant).\n\u2022 Supported ( ISSUP ): Attribution is the concept of whether the output is fully supported by\ncertain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much infor-\nmation in the output is entailed by the evidence. We evaluate attributions in three scale, Fully\nsupported, Partially supported, and No support / Contradictory, follow-\ning Yue et al. (2023); Nakano et al. (2021).\n\u2022 Useful ( ISUSE ): Following the definitions from Liu et al. (2023a), we define the perceived utility\nas whether the response is a helpful and informative answer to the query, independently from\nwhether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022).\nFor usefulness, we use a five-scale evaluation (1 is the lowest and 5 is the highest).\nDetails of GPT-4-based data collections. We use the instruction and demonstration pairs to prompt\nGPT-4, listed in Section D. Following an official recommendation, we separate instructions and\noutputs with \u201c##\u201d. We use the temperature 1 and set the maximum output token counts to be 200. We\ndiscard instances where GPT-4 does not follow the designated output formats or output sequences\nthat do not match our expected category names. As a result, we collected 1,2594 for Retrieve , 11,181\nfor ISSUP , 19,317 for relevance, 3,831 for utility.\nManual analysis of the GPT-4 predictions. The authors of this paper manually assess randomly\nsampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given\nthe same instruction, demonstrations, and test instances. We found our assessments show high\nagreement with GPT-4 predictions, especially for relevance (95%), retrieval necessity (95%), and\nthe degree of support (90%). Agreement was slightly lower in usefulness (80%), mostly due to the\ndisagreement between 1 and 2 or 4 and 5.\nA.2 S ELF -RAG TRAINING\nOverview of training. Algorithm 2 provides a high-level overview of our training.\nFull list of seed datasets. To sample diverse input-output pairs, we sample instances of the Open-\nInstruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca,\nOpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledge-\nintensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al.,\n2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stel-\nmakh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov et al.,\n2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\nPerformance of the Critic C. We evaluate the accuracy of reward predictions by splitting GPT-4\ngenerated feedback into training, development, and test sets. The accuracy of the reward model is\nas follows. Table 5 shows the model performance of predicting GPT-4 judgments. As you can see,\noverall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57523290-bf7c-4d9d-a849-c81ff114c2e3": {"__data__": {"id_": "57523290-bf7c-4d9d-a849-c81ff114c2e3", "embedding": null, "metadata": {"page_label": "18", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa132b5f-9ded-444a-b92a-c1fd59ea8883", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "664ad2a3729945a551a00bae3e56fd9fcfdd495a6b7384a7c34658e1d25329aa", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nAlgorithm 2 SELF -RAG Training\n1: Input input-output data D = {X, Y}, generator M, C \u03b8\n2: Initialize C with a pre-trained LM\n3: Sample data {Xsample, Ysample} \u223c {X, Y} \u25b7 Training Critic LM (Section 3.2.1)\n4: for (x, y) \u2208 (Xsample, Ysample) do \u25b7 Data collections for C\n5: Prompt GPT-4 to collect a reflection token r for (x, y)\n6: Add {(x, y, r)} to Dcritic\n7: Update C with next token prediction loss \u25b7 Critic learning; Eq. 1\n8: Initialize M with a pre-trained LM \u25b7 Training Generator LM (Section 3.2.2)\n9: for (x, y) \u2208 (X, Y) do \u25b7 Data collection for M with Dcritic\n10: Run C to predict r given (x, y)\n11: Add (x, y, r) to Dgen\n12: Update M on Dgen with next token prediction loss \u25b7 Generator LM learning; Eq. 2\nDataset name category Data source the number of instances\nGPT-4 Alpaca Instruction-following Open-Instruct 26,168\nStanford Alpaca Instruction-following Open-Instruct 25,153\nFLAN-V2 Instruction-following Open-Instruct 17,817\nShareGPT Instruction-following Open-Instruct 13,406\nOpen Assistant 1 Instruction-following Open-Instruct 9,464\nWizard of Wikipedia Knowledge-intensive KILT 17,367\nNatural Questions Knowledge-intensive KILT 15,535\nFEVER Knowledge-intensive KILT 9,966\nOpenBoookQA Knowledge-intensive HF Dataset 4,699\nArc-Easy Knowledge-intensive HF Dataset 2,147\nASQA Knowledge-intensive ASQA 3,897\nTable 3: The generator LM M training data statistics.\nbase LM Retrieve ISSUP ISREL ISUSE\nLlama2-7B 93.8 93.5 80.2 73.5\nFLAN-3B 85.6 73.1 82.0 72.1\nFigure 5: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\nWhile our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei\net al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final\nreward predictions. In most aspects, our reward model shows higher than 80% accuracy, indicating\nthe powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively\nlower performance on ISUSE , this is because both models often confuse between the two highest\ncases (5 and 4), where human annotators can also disagree.\nDetails of M data creation. Here, we provide detailed data creation procedures. Algorithm 3\nsummarizes the process. Here we set yt to y for simplification. Once we train the critic model, we\nfirst run it on input data from the aforementioned datasets, to predict whether retrieval is needed or\nnot. For the instances where the critic predicts Retrieve =No, we only predict the ISUSE given input\nand output. For the instances where the critic predicts Retrieve =Yes, we first retrieve passages using\nthe input and the entire output as queries, to find passages that are relevant to the entire output. We\nthen split output sentences using Spacy.7 For each sentence, we run C to predict whether the retrieval\nis necessary or not, given the input, preceding segments, and the initial retrieved passage. IfC predicts\nRetrieve =No, then do not insert any paragraph at the tth segment. If C predicts Retrieve =Yes, then\nwe use the original input and the tth segment as a retrieval query to find relevant passages for the\nt-th segment. For each retrieved passage, we predict ISREL and ISSUP . If there is any passage and\ncontinuation with ISREL =Relevant and ISSUP =Fully Supported / ISSUP =Partially\n7https://spacy.io/\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "984215be-3ca3-4b90-8c25-b9e4c5b5c2a5": {"__data__": {"id_": "984215be-3ca3-4b90-8c25-b9e4c5b5c2a5", "embedding": null, "metadata": {"page_label": "19", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92cbcd51-6bba-470b-9f79-b1f11f89181d", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "4b5e881cd577b765a3add6099da35e24d3e125c2888510a13076fb8c502d83bc", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nSupported, then we sample it as the continuation. If there is more than one passage satisfying this\ncriterion, we use the one with the highest retrieval score. If there are only ISREL =Irrelevant or\nISSUP =No Support passages, we randomly sample one passage.\nAlgorithm 3 Mgen Data creation\n1: Input Input-output data D = X, Y\n2: for (x, y) \u2208 {X, Y} do\n3: Given (x, y) C predicts Retrieve\n4: if Retrieve is predicted then\n5: Retrieve relevant passages D using R given (x, y) \u25b7 Retrieve passages\n6: for d \u2208 D do\n7: C predicts ISREL for each d \u25b7 Predict relevance of passages\n8: C predicts ISSUP for each (y, d) \u25b7 Predict supports of outputs\n9: C predicts ISUSE for each d \u25b7 Predict overall utility (t = T only)\n10: Sample d\n11: else if Retrieve is not predicted then\n12: C predicts ISUSE given x, y\nAdd augmented (x, y, d, r) to Dgen\nTraining examples. Table 4 show several training examples used forM training.\nA.3 S ELF -RAG INFERENCE\nDetails of beam-search score calculations. We first compute scores for each critique type by\ntaking the normalized probabilities of desirable tokens. For ISREL , we compute the score as follows:\ns( ISREL ) = p( ISREL = RELEVANT )\np( ISREL = RELEVANT ) +p( ISREL = IRRELEVANT ).\nFor ISSUP , we compute the score as follows:\ns( ISREL ) =p( ISSUP = FULLY)\nS + 0.5 \u00d7 p( ISSUP = PARTIALLY )\nS ,\nwhere S = P\nt\u2208{FULLY,PARTIALLY ,NO} p( ISSUP = t). For ISUSE where we have a five-scale score, we\ncompute the weighted sum of the scores. We assigns weighted scores of w = {\u22121, \u22120.5, 0, 0.5, 1}\nto the tokens ISUSE ={1, 2, 3, 4, 5}, and compute the final scores as follows:\ns( ISUSE ) =\n5X\ni\nwi\np( ISUSE = i)\nS ,\nwhere S = P\nt\u2208{1,2,3,4,5} p( ISUSE = t).\nDetails of adaptive retrieval. For retrieval based on soft constraints, we trigger retrieval if the\nfollowing condition is satisfied:\np( Retrieve = YES)\np( Retrieve = YES) +p(p( Retrieve = NO) > \u03b4.\nB E XPERIMENTAL DETAILS\nB.1 M ORE DETAILS OF TRAINING\nMore details of training and computations. We use 4 Nvidia A100 with 80GB memory to train\nour models. All models are trained for 3 epochs with a batch size of 128, a peak learning rate of 2e-5\nwith 3% warmup steps, and linear decay afterward. We set the maximum token length to be 2,048\nfor the 7B model, and 1,524 for the 13B model due to the memory constraint. We use Deepspeed\nstage 3 (Rajbhandari et al., 2020) to conduct multi-GPU distributed training, with training precision\nBfloat16 enabled. FlashAttention (Dao et al., 2022) is used to make the long-context training more\nefficient. We run inference of our trained models using 1-2 Quadro RTX 6000 GPUs with 24GB\nmemory.\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2622, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "077ecf39-393c-46f8-9ac6-384df21fdf5f": {"__data__": {"id_": "077ecf39-393c-46f8-9ac6-384df21fdf5f", "embedding": null, "metadata": {"page_label": "20", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fa2b42e-4e5a-4f5a-8421-38540b1c6c1b", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "5a6c72c3bf9fc79b1c597adce2745e4b0a9d0997b975ee9f4ac3f8370d1e0d0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a33f50d1-5428-413a-8894-75726f2ab16f", "node_type": "1", "metadata": {}, "hash": "e09da5bb207f2b496cb5197afab6fa7b66d88c5f00ee533b2758d1138a7a66ab", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nB.2 M ORE DETAILS OF EVALUATIONS\nRetrieval setup details. By default, we use Contriever-MS MARCO to retrieve the top five\ndocuments from Wikipedia, and use official Wikipedia embeddings based on 2018 English Wikipedia.\nOn PopQA, where question and answer pairs are created based on WikiData in 2022, we found\nthat the 2018 Wikipedia sometimes lacks articles about some entities that have been more recently\nadded to Wikipedia. Therefore, for PopQA, we used the December 2020 preprocessed Wikipedia\ncorpus provided by Izacard et al. (2022b) and generated document embeddings. 8 The issues of\nperformance variance from different Wikipedia dumps have been reported by prior work (Asai et al.,\n2020; Izacard et al., 2022b). Yet, we observe limited effectiveness of such off-the-shelf retrieval\nmodels trained primarily on knowledge-intensive tasks for open-ended generation (e.g., instruction\nfollowing). Recent or concurrent work studies instruction-tuning of retrieval systems (Asai et al.,\n2023b) or joint training of retrieval and LM components (Lin et al., 2023), while we leave exploring\nthe effectivess of such appraoches for future work. For bio generation and open-domain QA tasks,\nwe additionally retrieve five documents using Google Programmable Search9 and search documents\nfrom English Wikipedia. As this API only provides snippets, we retrieve Wikipedia introductory\nparagraphs for the corresponding entities.\nDetailed experimental settings for individual datasets. For OpenQA datasets, we set the max-\nimum new token number to 100 tokens. For closed-set tasks (PubHealth and ARC-C), we set the\nmaximum new token length to 50 for all baselines. For SELF -RAG inference on PubHealth and\nARC-C, instead of determining the output with the highest score 4 as in other tasks, we aggregate the\nscores for each option and select the answer option with the highest score. We found in zero-shot\nsettings of fact checking, some LLMs can generate capitalized class labels (e.g., True) while our\ngold labels are lower-cased. Therefore, across different LMs, for fact checking, we lowercase the\npredictions. In multiple choice tasks, we found some models generate answers in slightly different\nways (e.g., (A) instead of A). We slightly modify instructions for each LLM to avoid such format\nviolations, and further conduct string matching between each candidate and model predictions if\nformat violations still remain. After that processing, in closed set tasks, model predictions match\none of the gold classes in almost all cases. For ALCE, we found that Llama2-chat tend to generate\nsignificantly lower outputs than other models (e.g., on average, their output is nearly 100 token, while\nChatGPT generates 40 tokens on average), resulting in inflated str-em scores. We limit the maximum\ngeneration length to 100 tokens for all baselines to avoid this issue, rather than the original 300\ntokens in the ALCE paper. Consequently, all of the baseline output length is within 30-60 tokens.\nFor FactScore, we set the maximum new token length to 500 for baselines and 200 for SELF -RAG at\neach segment level.\nTask-specific instructions. Table 5 shows the list of the instructions used during evaluations. For\nOpen-domain QA, we do not provide explicit instructions.\nC R ESULTS\nC.1 A NALYSIS\nReliance on parametric- and non-parametric memories. We conduct analysis on how frequently\nmodel answers come from retrieved passages (non-parametric memories) or their own parametric\nmemories. On two open-domain QA datasets, TriviaQA and PopQA, we conduct the following\nanalysis: 1) sample query models successfully answer correctly, 2) for each query in this group,\ncheck whether the matched ground-truth answer is a sub-string of the retrieved passage or not. We\nevaluate SELF -RAG 7B, Alpaca 7B, Alpaca 13B, and Llama2-Chat-13B. We found that SELF -RAG\nsignificantly less frequently generates answers that are not included in the provided evidence; in\nparticular, in Alpaca 30B, 20% of the correct predictions are not included in the provided passages,\nfollowed by Llama2-chat 13B (18%) and Alpaca (15%), while it is only 2% in SELF -RAG.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4135, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a33f50d1-5428-413a-8894-75726f2ab16f": {"__data__": {"id_": "a33f50d1-5428-413a-8894-75726f2ab16f", "embedding": null, "metadata": {"page_label": "20", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fa2b42e-4e5a-4f5a-8421-38540b1c6c1b", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "5a6c72c3bf9fc79b1c597adce2745e4b0a9d0997b975ee9f4ac3f8370d1e0d0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "077ecf39-393c-46f8-9ac6-384df21fdf5f", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "5b3dde9f5989282cbb33d3e196d83b6725a2aec4a653805de987a02c323c6de6", "class_name": "RelatedNodeInfo"}}, "text": "On two open-domain QA datasets, TriviaQA and PopQA, we conduct the following\nanalysis: 1) sample query models successfully answer correctly, 2) for each query in this group,\ncheck whether the matched ground-truth answer is a sub-string of the retrieved passage or not. We\nevaluate SELF -RAG 7B, Alpaca 7B, Alpaca 13B, and Llama2-Chat-13B. We found that SELF -RAG\nsignificantly less frequently generates answers that are not included in the provided evidence; in\nparticular, in Alpaca 30B, 20% of the correct predictions are not included in the provided passages,\nfollowed by Llama2-chat 13B (18%) and Alpaca (15%), while it is only 2% in SELF -RAG. When\nretrieved passages are not relevant, SELF -RAG generates ISREL =Irrelevant, indicating that the\nfollowing answers may not be factually grounded, while those instruction-tuned models continue to\ngenerate plausible answers.\n8https://github.com/facebookresearch/atlas\n9https://programmablesearchengine.google.com/about/\n20", "mimetype": "text/plain", "start_char_idx": 3487, "end_char_idx": 4460, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ee9492a-7a04-41fb-bafa-b6d612cc8650": {"__data__": {"id_": "7ee9492a-7a04-41fb-bafa-b6d612cc8650", "embedding": null, "metadata": {"page_label": "21", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95d855cc-70e1-4f45-a1ca-bf68dc39041b", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "c989a48fb7da024627bfc24962a9440ff940107b893e2c4dbbdf1bb865de4f26", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nC.2 H UMAN EVALUATION EXAMPLES\nTable 6 shows examples with human evaluations on S&P and correctness of ISREL and ISSUP\nreflection tokens.\nC.3 Q UALITATIVE EXAMPLES\nTable 7 shows several examples predicted by ourSELF -RAG (13B). The first example is the model\noutput to an ASQA question. The first reference states that Emperor Constantine made Sunday a\nday of rest from labor, and further the second citation supports the fact that the official adoption\nof Sunday as a day of rest by Constantine in AD 321. In the second example, the model predicts\nContradictory to the first output as the output says the person has served as the CEO since 2010,\nwhile the passage says he stepped down as CEO in 2015. Indicating those factual contradictions\nas reflection tokens enables to enforcement of hard control and also verification of model outputs\neasily. In the third example, while the generation is mostly correct, SELF -RAG predicts Partially\nSupport to the statement listing the name of the songs, as they were not explicitly mentioned.\nD F ULL LIST OF INSTRUCTIONS AND DEMONSTRATIONS FOR GPT-4\nHere, we show the instructions and demonstrations used to prompt GPT-4 to collect reflection tokens.\nTable 8 shows the instructions and demonstrations for the initial retrieval token. Table 9 shows\nthe instruction and demonstrations used to collect the three-way output tokens for Retrieve given\ninstruction, preceding sentences, and previously retrieved passages. Due to the longer demonstration\nand test input, we only use a single demonstration. Table 10 shows an instruction and demonstrations\nused to collect the three-way output tokens for ISREL . Table 11 shows an instruction and demon-\nstrations used to collect the three-way output tokens for ISREL . Table 12 shows an instruction and\ndemonstrations used to collect the five-way output tokens for ISUSE .\n21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "599bcf1e-c86f-4247-8db2-2d4fa38ab8fc": {"__data__": {"id_": "599bcf1e-c86f-4247-8db2-2d4fa38ab8fc", "embedding": null, "metadata": {"page_label": "22", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379ee6e7-e1e5-4cc1-b19d-b6ba9ee3474d", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "7a5e49643296f786cf952eb97ed7b7d0d94769534958316f84d84b22d37c5326", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1998f6c9-cfec-43dc-b82a-7ba1e5207fae", "node_type": "1", "metadata": {}, "hash": "0991e74b59479d5687f52d8ee98a05979b1c48156de705670234080dc7a53d62", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nInput Identify the two types of computer memory. (from gpt4 alpaca)\nOutput [ Retrieve =Yes]<p>Computer memory Computer memory \u2013 In computing, memory refers to the\ncomputer hardware integrated circuits that store information for immediate use in a computer; it is synonymous\nwith the term \u201cprimary storage\u201d. Computer memory operates at a high speed, for example random-access memory\n(RAM), as a distinction from storage that provides slow-to-access information but offers higher capacities. If\nneeded, contents of the computer memory can be transferred to secondary storage; a very common way of doing\nthis is through a memory management technique called \u201cvirtual memory\u201d. An archaic synonym for memory is\nstore. The term \u201cmemory\u201d, meaning \u201cprimary storage\u201d or \u201cmain memory\u201d, is<p>[ ISREL =Relevant]Primary\nmemory, also known as main memory, is the memory that the computer can directly access and manipulate.\n[ ISSUP =Fully Supported] [ Retrieve =Yes]<p>Computer memory\u2013often associated with addressable\nsemiconductor memory, i.e. integrated circuits consisting of silicon-based transistors, used for example as\nprimary storage but also other purposes in computers and other digital electronic devices. There are two main\nkinds of semiconductor memory, volatile and non-volatile. Examples of non-volatile memory are flash memory\n(used as secondary memory) and ROM, PROM, EPROM and EEPROM memory (used for storing firmware\nsuch as BIOS). Examples of volatile memory are primary storage, which is typically dynamic random-access\nmemory (DRAM), and fast CPU cache memory, which is typically static random-access memory (SRAM)\nthat is fast but energy-consuming, offering lower memory<p>[ ISREL =Relevant]Primary memory can be\naccessed quickly, but it is volatile, meaning that the data is lost if the power is turned off.[ ISSUP =Partially\nSupported]...[ ISUSE :5]\nInput Give an example of a situation where you have to delegate tasks (from gpt4 alpaca)\nOutput [ Retrieve =No]The manager realizes that there is an extensive list of tasks that need to be completed in\norder to make the event a success.[ Retrieve =Yes]<p>Delegation\u2013Delegation is the assignment of any responsi-\nbility or authority to another person (normally from a manager to a subordinate) to carry out specific activities. It\nis one of the core concepts of management leadership. However, the person who delegated the work remains\naccountable for the outcome of the delegated work. Delegation empowers a subordinate to make decisions, i.e. it\nis a shifting of decision-making authority from one organizational level to a lower one. Delegation, if properly\ndone, is not fabrication. The opposite of effective delegation is micromanagement, where a manager provides too\nmuch input,<p>[ ISREL =Relevant]This includes tasks such as creating a guest list, designing and sending in-\nvitations, booking a venue, coordinating with vendors, planning the event\u00b4s program, and monitoring the budget.[\nISSUP =Partially Supported][ Retrieve =No]Since it is not possible for the manager to handle everything\nalone, they must delegate tasks to their team members.[ Retrieve =No]The manager would then assign each\nteam member specific responsibilities based on their individual skillsets and strengths.[ Retrieve =Yes]<p>Joint\napplication design \u2013 Subject Matter Experts: These are the business users, the IS professionals, and the outside\nexperts that will be needed for a successful workshop. This group is the backbone of the meeting<p>[ ISREL\n=Irrelevant]For instance, the team\u00b4s graphic designer might be tasked with designing the invitations, while\nthe event coordinator would be in charge of planning the program and working with vendors...[ Retrieve =No]By\ndelegating tasks, the manager can ensure that all necessary tasks are completed in a timely manner, while also\nempowering their team members to take on responsibilities and contribute to the success of the event.[ ISUSE\n=5]\nInput Synthesize a poem based on this prompt:\u201cThe sun is setting\u201d (from stanford alpaca)\nOutput [ Retrieve =No]The stars begin to twinkle bright the trees, scattered throughout the night;\n[ Retrieve =No]The stars begin to twinkle bright the trees, scattered throughout the night [ Retrieve =No] They\nflicker like a silent dance, A peaceful moment in its trance. [ Retrieve =No] They flicker like a silent dance, A\npeaceful moment in its trance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1998f6c9-cfec-43dc-b82a-7ba1e5207fae": {"__data__": {"id_": "1998f6c9-cfec-43dc-b82a-7ba1e5207fae", "embedding": null, "metadata": {"page_label": "22", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379ee6e7-e1e5-4cc1-b19d-b6ba9ee3474d", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "7a5e49643296f786cf952eb97ed7b7d0d94769534958316f84d84b22d37c5326", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "599bcf1e-c86f-4247-8db2-2d4fa38ab8fc", "node_type": "1", "metadata": {"page_label": "22", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "4750c4ad9c9dc9c6776f8eb482ce51552165f355c6ca274b27c24dbf1383d5f9", "class_name": "RelatedNodeInfo"}}, "text": "[ ISUSE\n=5]\nInput Synthesize a poem based on this prompt:\u201cThe sun is setting\u201d (from stanford alpaca)\nOutput [ Retrieve =No]The stars begin to twinkle bright the trees, scattered throughout the night;\n[ Retrieve =No]The stars begin to twinkle bright the trees, scattered throughout the night [ Retrieve =No] They\nflicker like a silent dance, A peaceful moment in its trance. [ Retrieve =No] They flicker like a silent dance, A\npeaceful moment in its trance. [ Retrieve =No] A chorus of crickets sing a lullaby, To every creature that roams\nthe sky [ ISUSE =5]\nInput when did walking dead season 7 come out (from nq)\nOutput [ Retrieve =Yes]<p>The Walking Dead (season 7)\u2013The Walking Dead (season 7) The seventh season of\n\u201cThe Walking Dead\u201d, an American post-apocalyptic horror television series on AMC, premiered on October 23,\n2016, and concluded on April 2, 2017, consisting of 16 episodes. Developed for television by Frank Darabont,\nthe series is based on the eponymous series of comic books by Robert Kirkman, Tony Moore, and Charlie Adlard.\n...<p>[ ISREL =Relevant]October 23 , 2016[ ISSUP =Fully Supported][ ISUSE =5]\nInput Given a chat history separated by new lines, generates an informative, knowledgeable and engaging\nresponse: Hello, I love German shepherd dogs and you? \u2013 My favorite dog, they are officially known as German\nShepherd Dogs? \u2013 Yes, I have a German shepherd in my house and she is pregnant (from wow)\nOutput hey also are easily trained, obedient, and intelligent.[ Retrieve =Yes]<p>German Shepherd \u2013 a simple\ndifference of opinion, it is the fundamental issue of the breed\u00b4s essential conformation and movement. ... They\nare known for being easy to train and good for performing tasks and following instructions. They are especially\nwell known for their police work, being used for tracking criminals, patrolling troubled areas and detecting and\nholding suspects. <p>[ ISREL =Relevant]Because of this, they are often used for disability assistance and\nfor police work. [ ISSUP =Partially Supported][ ISUSE =2]\nTable 4: List of the training examples.\n22", "mimetype": "text/plain", "start_char_idx": 3936, "end_char_idx": 6013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0617267-0559-4be0-b685-01ede02a1288": {"__data__": {"id_": "e0617267-0559-4be0-b685-01ede02a1288", "embedding": null, "metadata": {"page_label": "23", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e469dfe-ab9a-44e9-8705-5c55f9815440", "node_type": "4", "metadata": {"page_label": "23", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "c2071253e8853fcff3112b1e199ac528fd43196e7f6bd76471dcea8251f6e39b", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nDataset Instruction\nARC-C Given four answer candidates, A, B, C and D, choose the best answer choice. Please answer\nwith the capitalized alphabet only, without adding any extra phrase or period.\nPubHealth Is the following statement correct or not? Say true if it\u2019s correct; otherwise, say false. Don\u2019t\ncapitalize or add periods, just say \u201ctrue\u201d or \u201cfalse\u201d.\nBio Generation Tell me a bio about [Person Name]\nASQA (baseline) Instruction: Write an accurate, engaging, and concise answer for the given question using only\nthe provided search results (some of which might be irrelevant) and cite them properly. Use\nan unbiased and journalistic tone. Always cite for any factual claim. When citing several\nsearch results, use [1][2][3]. Cite at least one document and at most three documents in each\nsentence. If multiple documents support the sentence, only cite a minimum sufficient subset of\nthe documents.\nASQA (ours) Answer the following question. The question may be ambiguous and have multiple correct\nanswers, and in that case, you have to provide a long-form answer including all correct answers.\nTable 5: Full list of instructions used during zero-shot evaluations. For open-domain QA, we don\u2019t\nuse any task specific instruction and simply use the original questions as input query.\n23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f291389e-20c8-4879-a191-16a10346f8e1": {"__data__": {"id_": "f291389e-20c8-4879-a191-16a10346f8e1", "embedding": null, "metadata": {"page_label": "24", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0531013c-7af7-43f3-80c9-108a054e3641", "node_type": "4", "metadata": {"page_label": "24", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "a4298d79a426b2bc235911dd9096ca410c156a751b5f032852ceececb1878b41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e97960c3-17be-4375-a3c9-51895b8a47aa", "node_type": "1", "metadata": {}, "hash": "22598ad63ecffd818df70533ba65d520147d4547a994b14d75fc4f8f0804aff3", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nInput What genre is A Question and Answer Guide to Astronomy? (PopQA)\nOutput [ Retrieve =Yes] <p>Reference [1]<p>[ ISREL =Relevant] A Question and Answer Guide to As-\ntronomy is a non-fiction book that falls under the genre of Astronomy and Cosmology. [ ISSUP =Fully\nSupported][ ISUSE =5]\nReference [1] A Question and Answer Guide to Astronomy is a book about astronomy and cosmology, and is\nintended for a general audience. The book was written by Pierre-Yves Bely, Carol Christian, and Jean-Rene\nRoy, and published in English by Cambridge University Press in 2010. It was originally written in French. The\ncontent within the book is written using a question and answer format. It contains some 250 questions, which\nThe Science Teacher states each are answered with a concise and well-formulated essay that is informative and\nreadable. The Science Teacher review goes on to state that many of the answers given in the book are little gems\nof science writing. The Science Teacher summarizes by stating that each question is likely to be thought of by a\nstudent, and that the answers are informative, well constructed, and thorough. The book covers information about\nthe planets, the Earth, the Universe, practical astronomy, history, and awkward questions such as astronomy in\nthe Bible, UFOs, and aliens. Also covered are subjects such as the Big Bang, comprehension of large numbers,\nand the Moon illusion.\nS&P Supported and Plausible.\nCorrectness of ISREL and ISSUP : Both ISREL and ISSUP are correct, as the Reference [1] provides\ndetailed information of the book including the genre, and the model output is fully supported by the reference.\nInput Who is the author of The Lie? (PopQA)\nOutput [ Retrieve =Yes] <p>Reference [1]<p>[ ISREL =Relevant] The author of The Lie is Sam Harris.[\nISSUP =Fully Supported][ ISUSE =5]\nReference [1] Lying is a 2011 long-form essay book by American author and neuroscience expert Sam Harris.\nHarris argues that we can radically simplify our lives and improve society by merely telling the truth in situations\nwhere others often lie.\nS&P Plausible but not supported.\nCorrectness of ISREL and ISSUP Both ISREL and ISSUP are incorrect, as the Reference [1] is about a\ndifferent book whose title is similar to The Lie.\nInput Tell me a bio about Ronaldinho (Bio)\nOutput [ Retrieve =Yes] <p>Reference [1]<p>[ ISREL =Relevant] Ronaldinho is a retired professional\nfootballer from Brazil.[ ISSUP =Fully Supported][ Retrieve =Continue]He played as an attacking\nmidfielder and was known for his technical skills, creativity, dribbling ability, and accuracy from free\nkicks[ Retrieve =Continue]He won two FIFA World Player of the Year awards and a Ballon d\u2019Or.[ ISUSE =5]\nReference [1] Ronaldo de Assis Moreira (born 21 March 1980), commonly known as Ronaldinho Ga\u00b4ucho or\nsimply Ronaldinho, is a Brazilian retired professional footballer who played mostly as an attacking midfielder,\nbut was also deployed as a winger. Considered one of the best players of his generation and regarded by many as\none of the greatest of all time, Ronaldinho won two FIFA World Player of the Year awards and a Ballon d\u2019Or. A\nglobal icon of the sport, he was renowned for his technical skills, creativity, dribbling ability and accuracy from\nfree-kicks, as well as his use of tricks, feints, no-look passes and overhead...\nS&P Plausible and fully supported.\nCorrectness of ISREL and ISSUP Both ISREL and ISSUP are correct.\nInput Tell me a bio about Sergei Bodrov (Bio)\nOutput [ Retrieve =Yes] <p>Reference [1] <p>[ ISREL =Relevant] Sergei Vladimirovich Bodrov\n(born June 28, 1948) is a Russian film director, screenwriter, and producer.[ ISSUP =Fully\nSupported][ Retrieve =Continue]He was born in Moscow, Russia.[ Retrieve =Continue]He is the presi-\ndent of the Russian Guild of Film Producers.[ ISUSE =5]\nReference [1] Sergei Vladimirovich Bodrov (born June 28, 1948) is a Russian film director, screenwriter, and\nproducer. In 2003 he was the President of the Jury at the 25th Moscow International Film Festival.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e97960c3-17be-4375-a3c9-51895b8a47aa": {"__data__": {"id_": "e97960c3-17be-4375-a3c9-51895b8a47aa", "embedding": null, "metadata": {"page_label": "24", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0531013c-7af7-43f3-80c9-108a054e3641", "node_type": "4", "metadata": {"page_label": "24", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "a4298d79a426b2bc235911dd9096ca410c156a751b5f032852ceececb1878b41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f291389e-20c8-4879-a191-16a10346f8e1", "node_type": "1", "metadata": {"page_label": "24", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "82fe102837a7d6344f045975dd9192a397d72cca0b7c1afd0d596cf813afa7a3", "class_name": "RelatedNodeInfo"}}, "text": "Correctness of ISREL and ISSUP Both ISREL and ISSUP are correct.\nInput Tell me a bio about Sergei Bodrov (Bio)\nOutput [ Retrieve =Yes] <p>Reference [1] <p>[ ISREL =Relevant] Sergei Vladimirovich Bodrov\n(born June 28, 1948) is a Russian film director, screenwriter, and producer.[ ISSUP =Fully\nSupported][ Retrieve =Continue]He was born in Moscow, Russia.[ Retrieve =Continue]He is the presi-\ndent of the Russian Guild of Film Producers.[ ISUSE =5]\nReference [1] Sergei Vladimirovich Bodrov (born June 28, 1948) is a Russian film director, screenwriter, and\nproducer. In 2003 he was the President of the Jury at the 25th Moscow International Film Festival.\nS&P Plausible but only partially supported. The the Reference [1] does not explicitly mention the person is born\nin Moscow and there is no description about the Russian Guild of Film Producers.\nCorrectness of ISREL and ISSUP ISREL and ISSUP are correct as the reference document is highly relevant\nand the first sentence is fully supported.\nTable 6: Examples of human evaluation results.\n24", "mimetype": "text/plain", "start_char_idx": 3378, "end_char_idx": 4424, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27c68578-8d05-4f09-9091-d4dd1d868f56": {"__data__": {"id_": "27c68578-8d05-4f09-9091-d4dd1d868f56", "embedding": null, "metadata": {"page_label": "25", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f39d25a6-d2f8-41da-a57e-5675bf769a17", "node_type": "4", "metadata": {"page_label": "25", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "aeaabeb3f250bbba126b04fe18412c23179213fff9a8cd4082ca671a2d308fe2", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nInput When did sunday became the seventh day of the week (asqa)\nOutput [ Retrieve =Yes] <p>Reference [1]<p>[ ISREL =Relevant] In 321 AD, the Roman Emperor Con-\nstantine the Great made Sunday a day of rest from labor, stating: \u201cOn the venerable Day of the Sun let the\nmagistrates and people residing in cities rest, and let all workshops be closed\u201d.[ ISSUP =Fully Supported]\n[ Retrieve =Yes] <p>Reference [2]<p>[ ISREL =Relevant] The official adoption of Sunday as a day of\nrest by Constantine the Great in AD 321 made Sunday the seventh day of the week. [2][ ISSUP =Fully\nSupported][ ISUSE =5]\nReference [1] Emperor Constantine then enacted the first Sunday Laws, for \u201cthe venerable Day of the Sun\u201d\nin 321 A.D. On March 7, 321, the Roman emperor Constantine issued a decree making Sunday a day of rest\nfrom labor, stating: We find a good description of this in sources such as the book Rest Days which states:\nEarly Christian observance of both the spiritual seventh-day sabbath and a Lord\u00b4s Day assembly is evidenced in\nIgnatius\u00b4s letter to the Magnesians ca. 110.\nReference [2] Despite the official adoption of Sunday as a day of rest by Constantine, the seven-day week and\nthe nundial cycle continued to be used side-by-side until at least the Calendar of 354 and probably later. ... The\nfact that the canon had to be issued at all is an indication that adoption of Constantine\u2019s decree of 321 was still\nnot universal ...\nInput Tell me a bio about Rory Tapner. (bio generation)\nOutput <p>Reference [1]<p>[ ISREL =Relevant] Rory Tapner is a British businessman who has served as\nthe Chief Executive Officer of Coutts, a private banking and wealth management company, since 2010.[ ISSUP\n=Contradictory], Reference [2]<p>[ ISREL =Relevant] Rory Tapner was born on 30 September 1959\nin London, England.[ ISSUP =Fully Supported]\nReference [1] Rory Tapner Rory Tapner is a UK businessman who, in September 2010 was appointed CEO of\nCoutts, the Wealth Management division of Royal Bank of Scotland Group. In February 2015 he stepped down\nas CEO when he was replaced in the role by Alison Rose\nReference [2] Holdings (from August 2016); and the Council of the University of Buckingham (from July 2017).\nRory Tapner Rory Tapner (born 30 September 1959) is a UK businessman ...\nInput Tell me a bio about G. Venugopa. (bio generation)\nOutput (ranked 1) [ Retrieve =Yes] <p>Reference [1]<p>[ ISREL =Relevant] G. Venugopal is a popu-\nlar playback singer in the Malayalam film industry. [ Retrieve =Continue] He has sung over 300 songs in\nMalayalam, Tamil, and Hindi movies.[ ISSUP =Partially Supported] [ Retrieve =Yes] <p>Reference\n[2]<p>G. Venugopal was born on December 10, 1960 in India.[ ISSUP =Fully Supported]\nReference [1] \u2019G. Venugopal G Venugopal (born 10 December 1960) is an Indian playback singer known for his\nwork in Malayalam films. He started his singing career in the film \u201cOdaruthammaava Aalariyaam\u201d (1984). Since\nthen he has sung in more than 300 films; and has over 500 private albums to his credit. The many awards he has\nwon include the coveted Best Playback Singer (Kerala State) which he won thrice, the Film Critics award and\nthe Kerala Kaumudi Gallop Poll award (twice). He won The Kerala State award for Best Playback singer (2004)\nthird time for the song \u201daadedee..aadaadedee\u201d from the film \u201dUllam\u201d.Venugopal\u2019\nReference [2] Kerala State Film Awards: Kerala Film Critics Award Kerala State Government Award (Profes-\nsional Drama): Asianet Film Awards: Kerala Kaumadi Gallup Poll Award: Academic G. Venugopal G Venugopal\n(born 10 December 1960) is an Indian playback singer known for his work in Malayalam films.\nTable 7: Examples of outputs.\n25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90a63ced-3824-4f8b-900c-5962cb1b1d6e": {"__data__": {"id_": "90a63ced-3824-4f8b-900c-5962cb1b1d6e", "embedding": null, "metadata": {"page_label": "26", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35f12626-eb91-4636-9d1e-b11150a477a1", "node_type": "4", "metadata": {"page_label": "26", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "baf296effc09532f1a817669828d780e36c6956a34815e02d3cacfb486557dc4", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nInstructions\nGiven an instruction, please make a judgment on whether finding some external documents\nfrom the web (e.g., Wikipedia) helps to generate a better response. Please answer [Yes] or\n[No] and write an explanation.\nDemonstrations\nInstruction Give three tips for staying healthy.\nNeed retrieval? [Yes]\nExplanation There might be some online sources listing three tips for staying healthy or\nsome reliable sources to explain the effects of different behaviors on health. So retrieving\ndocuments is helpful to improve the response to this query.\nInstruction Describe a time when you had to make a difficult decision.\nNeed retrieval? [No]\nExplanation This instruction is asking about some personal experience and thus it does not\nrequire one to find some external documents.\nInstruction Write a short story in third person narration about a protagonist who\nhas to make an important career decision.\nNeed retrieval? [No]\nExplanation This instruction asks us to write a short story, which does not require external\nevidence to verify.\nInstruction What is the capital of France?\nNeed retrieval? [Yes]\nExplanation While the instruction simply asks us to answer the capital of France, which is a\nwidely known fact, retrieving web documents for this question can still help.\nInstruction Find the area of a circle given its radius. Radius = 4\nNeed retrieval? [No]\nExplanation This is a math question and although we may be able to find some documents\ndescribing a formula, it is unlikely to find a document exactly mentioning the answer.\nInstruction Arrange the words in the given sentence to form a grammatically cor-\nrect sentence. quickly the brown fox jumped\nNeed retrieval? [No]\nExplanation This task doesn\u2019t require any external evidence, as it is a simple grammatical\nquestion.\nInstruction Explain the process of cellular respiration in plants.\nNeed retrieval? [Yes]\nExplanation This instruction asks for a detailed description of a scientific concept, and is\nhighly likely that we can find a reliable and useful document to support the response.\nTable 8: Instructions and demonstrations for Retrieve aspect given the input only.\n26", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2146, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f26be59c-50a6-4d1b-a6e9-b4615d093fc3": {"__data__": {"id_": "f26be59c-50a6-4d1b-a6e9-b4615d093fc3", "embedding": null, "metadata": {"page_label": "27", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb9635b4-1982-4ae3-b466-0d05e732bb67", "node_type": "4", "metadata": {"page_label": "27", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "5cea3ef7a090578f5e676c4e2b1ebeae2d1aa1ba9347765a74c3df1a2b0b6949", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nInstructions\nYou will be provided with an instruction, evidence, output sentence, and preceding sentences\n(optional). If the preceding sentence is given, the output should be the sentence that follows\nthose preceding sentences. Your task is to determine whether the information in the output\nsentence can be fully verified by the evidence or if it requires further external verification.\nThere are three cases:\n- If the output sentence can be verified solely with the evidence, then respond with [Continue\nto Use Evidence].\n- If the sentence doesn\u2019t require any factual verification (e.g., a subjective sentence or a\nsentence about common sense), then respond with [No Retrieval].\n- If additional information is needed to verify the output sentence, respond with [Retrieval].\nPlease provide explanations for your judgments.\nInstruction Explain the use of word embeddings in Natural Language Processing.\nPreceding sentences Word embeddings are one of the most powerful tools available for\nNatural Language Processing (NLP). They are mathematical representations of words or\nphrases in a vector space, allowing similarities between words and the context in which they\nare used to be measured.\nEvidence: Word embedding\nWord embedding is the collective name for a set of language modeling and feature learning\ntechniques in natural language processing (NLP) where words or phrases from the vocabulary\nare mapped to vectors of real numbers. Conceptually it involves a mathematical embedding\nfrom a space with one dimension per word to a continuous vector space with a much lower\ndimension. Output: Word embeddings are useful for tasks such as sentiment analysis, text\nclassification, predicting the next word in a sequence, and understanding synonyms and\nanalogies.\nRating [Retrieval]\nExplanation The output discusses the applications of word embeddings, while the evidence\nonly discusses the definitions of word embeddings and how they work. Therefore, we need to\nretrieve other evidence to verify whether the output is correct or not.\nTable 9: Instructions and demonstrations for Retrieve aspect given the input, preceding generations,\nand retrieved passages.\n27", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65c368e9-5405-4029-b69e-aca2923db5d2": {"__data__": {"id_": "65c368e9-5405-4029-b69e-aca2923db5d2", "embedding": null, "metadata": {"page_label": "28", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b7b49de-1f13-4737-a47c-32ac293626f9", "node_type": "4", "metadata": {"page_label": "28", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "4726fc15d7f48f0d9273d24e057fbe64118f68f1ba9a7c9ad8a96605d96ddd0a", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nInstructions\nYou\u2019ll be provided with an instruction, along with evidence and possibly some preceding\nsentences. When there are preceding sentences, your focus should be on the sentence that\ncomes after them. Your job is to determine if the evidence is relevant to the initial instruction\nand the preceding context, and provides useful information to complete the task described in\nthe instruction. If the evidence meets this requirement, respond with [Relevant]; otherwise,\ngenerate [Irrelevant].\nInstruction Given four answer options, A, B, C, and D, choose the best answer.\nInput Earth\u2019s rotating causes\nA: the cycling of AM and PM\nB: the creation of volcanic eruptions\nC: the cycling of the tides\nD: the creation of gravity\nEvidence Rotation causes the day-night cycle which also creates a corresponding cycle of\ntemperature and humidity creates a corresponding cycle of temperature and humidity. Sea\nlevel rises and falls twice a day as the earth rotates.\nRating [Relevant]\nExplanation The evidence explicitly mentions that the rotation causes a day-night cycle, as\ndescribed in the answer option A.\nInstruction age to run for US House of Representatives\nEvidence The Constitution sets three qualifications for service in the U.S. Senate: age (at\nleast thirty years of age); U.S. citizenship (at least nine years); and residency in the state a\nsenator represents at the time of election.\nRating [Irrelevant]\nExplanation The evidence only discusses the ages to run for the US Senate, not for the\nHouse of Representatives.\nTable 10: Instructions and demonstrations for ISREL aspect given the input only.\n28", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1618, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8c75822-2004-4b11-9fc2-58fad2da8cce": {"__data__": {"id_": "d8c75822-2004-4b11-9fc2-58fad2da8cce", "embedding": null, "metadata": {"page_label": "29", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd42df60-847b-4bde-b4cc-5306dfde5f1b", "node_type": "4", "metadata": {"page_label": "29", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "93657660469f4cc780fb465c68426b7c568627c68c48f136405394d7259a9bfb", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nInstructions\nYou will receive an instruction, evidence, and output, and optional preceding sentences. If the\npreceding sentence is given, the output should be the sentence that follows those preceding\nsentences. Your task is to evaluate if the output is fully supported by the information provided\nin the evidence.\nUse the following entailment scale to generate a score:\n- [Fully supported] - All information in output is supported by the evidence, or extractions\nfrom the evidence. This is only applicable when the output and part of the evidence are\nalmost identical.\n- [Partially supported] - The output is supported by the evidence to some extent, but there\nis major information in the output that is not discussed in the evidence. For example, if an\ninstruction asks about two concepts and the evidence only discusses either of them, it should\nbe considered a [Partially supported].\n- [No support / Contradictory] - The output completely ignores evidence, is unrelated to the\nevidence, or contradicts the evidence. This can also happen if the evidence is irrelevant to the\ninstruction.\nMake sure to not use any external information/knowledge to judge whether the out-\nput is true or not. Only check whether the output is supported by the evidence, and not\nwhether the output follows the instructions or not.\nInstruction Explain the use of word embeddings in Natural Language Processing.\nPreceding sentences Word embeddings are one of the most powerful tools available for\nNatural Language Processing (NLP). They are mathematical representations of words or\nphrases in a vector space, allowing similarities between words and the context in which they\nare used to be measured.\nOutput Word embeddings are useful for tasks such as sentiment analysis, text classification,\npredicting the next word in a sequence, and understanding synonyms and analogies.\nEvidence Word embedding\nWord embedding is the collective name for a set of language modeling and feature learning\ntechniques in natural language processing (NLP) where words or phrases from the vocabulary\nare mapped to vectors of real numbers. Conceptually it involves a mathematical embedding\nfrom a space with one dimension per word to a continuous vector space with a much lower\ndimension. Methods to generate this mapping include neural networks, dimensionality\nreduction on the word co-occurrence matrix, probabilistic models, explainable knowledge\nbase method, and explicit representation in terms of the context in which words appear. Word\nand phrase embeddings, when used as the underlying input representation, have been shown\nto boost the performance in NLP tasks such as syntactic parsing, sentiment analysis, next\ntoken predictions as well and analogy detection.\nScore [Fully supported]\nExplanation The output sentence discusses the application of word embeddings, and the\nevidence mentions all of the applications syntactic parsing, sentiment analysis, next token\npredictions as well as analogy detection as the applications. Therefore, the score should be\n[Fully supported].\nTable 11: Instructions and demonstrations for ISSUP tokens.\n29", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3116, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "594568b9-6160-41fe-8d4b-1c02fb040bd9": {"__data__": {"id_": "594568b9-6160-41fe-8d4b-1c02fb040bd9", "embedding": null, "metadata": {"page_label": "30", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "725f8c93-52b6-48e3-86a2-8c30d1c419b0", "node_type": "4", "metadata": {"page_label": "30", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "5d3fd7a207fb7770f31a59aa13efc5666d45af976a875a24da9a13430c9cb90e", "class_name": "RelatedNodeInfo"}}, "text": "Preprint.\nInstructions\nGiven an instruction and an output, rate whether the response appears to be a helpful and\ninformative answer to the query, from 1 (lowest) - 5 (highest). We call this score perceived\nutility. The detailed criterion is as follows: 5: The response provides a complete, highly\ndetailed, and informative response to the query, fully satisfying the information needs. 4: The\nresponse mostly fulfills the need in the query, while there can be some minor improvements\nsuch as discussing more detailed information, having better structure of the response, or im-\nproving coherence. 3: The response is acceptable, but some major additions or improvements\nare needed to satisfy users\u2019 needs. 2: The response still addresses the main request, but it is\nnot complete or not relevant to the query. 1: The response is barely on-topic or completely\nirrelevant.\nInstruction Who is the current prime minister of the UK as of 2023?\nOutput Boris Johnson was the prime minister of the UK from 2019 - 2022.\nPerceived utility 2\nExplanation While the output provides a factually correct statement about the UK prime\nminister from 2019 to 2022, this instruction asks who the prime minister is as of 2023, so it\ndoesn\u2019t answer the instruction. Therefore, the utility is 2.\nInstruction Given a description of a travel destination, recommend 10 tourist attrac-\ntions with detailed descriptions of each. The travel destination is Tokyo, Japan.\nOutput Tokyo is a vibrant city full of exciting tourist attractions. Some must-see sights\ninclude the Tokyo Skytree, Tokyo Disneyland, Sensoji Temple, Meiji Shrine, Tsukiji Fish\nMarket, Harajuku, and Shinjuku Gyoen.\nPerceived utility 3\nExplanation This output doesn\u2019t provide descriptions of each attraction and the number of\nthe attractions is also less than 10. While this output partially answers the instructions, it\ndoesn\u2019t match the instructions strictly.\nTable 12: Instructions and demonstrations for ISUSE tokens.\n30", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1963, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25990a46-6ebb-4a08-831d-62367de64ccd": {"__data__": {"id_": "25990a46-6ebb-4a08-831d-62367de64ccd", "embedding": null, "metadata": {"page_label": "1", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c7c28efb-12e7-47e1-a7fe-7b2fab2f2107", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "dc278ac6027e288d4d0cbb482db7fa2fc878b10fe65afd4f5f144c48e38889bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff2396d2-24ab-4457-b9cd-99b63f9aa420", "node_type": "1", "metadata": {}, "hash": "7d3e19a62df1404df4c2c0bd5913715c6317176708b157333af44558e15b6296", "class_name": "RelatedNodeInfo"}}, "text": "1\nRetrieval-Augmented Generation for Large\nLanguage Models: A Survey\nYunfan Gaoa, Yun Xiongb, Xinyu Gao b, Kangxiang Jia b, Jinliu Pan b, Yuxi Bic, Yi Dai a, Jiawei Sun a, Meng\nWangc, and Haofen Wang a,c\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\ncCollege of Design and Innovation, Tongji University\nAbstract\u2014Large Language Models (LLMs) showcase impres-\nsive capabilities but encounter challenges like hallucination,\noutdated knowledge, and non-transparent, untraceable reasoning\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\nas a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the\ngeneration, particularly for knowledge-intensive tasks, and allows\nfor continuous knowledge updates and integration of domain-\nspecific information. RAG synergistically merges LLMs\u2019 intrin-\nsic knowledge with the vast, dynamic repositories of external\ndatabases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\nIt meticulously scrutinizes the tripartite foundation of RAG\nframeworks, which includes the retrieval, the generation and the\naugmentation techniques. The paper highlights the state-of-the-\nart technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG\nsystems. Furthermore, this paper introduces up-to-date evalua-\ntion framework and benchmark. At the end, this article delineates\nthe challenges currently faced and points out prospective avenues\nfor research and development 1.\nIndex Terms\u2014Large language model, retrieval-augmented gen-\neration, natural language processing, information retrieval\nI. I NTRODUCTION\nL\nARGE language models (LLMs) have achieved remark-\nable success, though they still face significant limitations,\nespecially in domain-specific or knowledge-intensive tasks [1],\nnotably producing \u201challucinations\u201d [2] when handling queries\nbeyond their training data or requiring current information. To\novercome challenges, Retrieval-Augmented Generation (RAG)\nenhances LLMs by retrieving relevant document chunks from\nexternal knowledge base through semantic similarity calcu-\nlation. By referencing external knowledge, RAG effectively\nreduces the problem of generating factually incorrect content.\nIts integration into LLMs has resulted in widespread adoption,\nestablishing RAG as a key technology in advancing chatbots\nand enhancing the suitability of LLMs for real-world applica-\ntions.\nRAG technology has rapidly developed in recent years, and\nthe technology tree summarizing related research is shown\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\n1Resources are available at https://github.com/Tongji-KGLLM/\nRAG-Survey\nin Figure 1. The development trajectory of RAG in the era\nof large models exhibits several distinct stage characteristics.\nInitially, RAG\u2019s inception coincided with the rise of the\nTransformer architecture, focusing on enhancing language\nmodels by incorporating additional knowledge through Pre-\nTraining Models (PTM). This early stage was characterized\nby foundational work aimed at refining pre-training techniques\n[3]\u2013[5].The subsequent arrival of ChatGPT [6] marked a\npivotal moment, with LLM demonstrating powerful in context\nlearning (ICL) capabilities. RAG research shifted towards\nproviding better information for LLMs to answer more com-\nplex and knowledge-intensive tasks during the inference stage,\nleading to rapid development in RAG studies. As research\nprogressed, the enhancement of RAG was no longer limited\nto the inference stage but began to incorporate more with LLM\nfine-tuning techniques.\nThe burgeoning field of RAG has experienced swift growth,\nyet it has not been accompanied by a systematic synthesis that\ncould clarify its broader trajectory. This survey endeavors to\nfill this gap by mapping out the RAG process and charting\nits evolution and anticipated future paths, with a focus on the\nintegration of RAG within LLMs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff2396d2-24ab-4457-b9cd-99b63f9aa420": {"__data__": {"id_": "ff2396d2-24ab-4457-b9cd-99b63f9aa420", "embedding": null, "metadata": {"page_label": "1", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c7c28efb-12e7-47e1-a7fe-7b2fab2f2107", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "dc278ac6027e288d4d0cbb482db7fa2fc878b10fe65afd4f5f144c48e38889bd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25990a46-6ebb-4a08-831d-62367de64ccd", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "b7b2e4c414e593a5b5048a461a688968fc3be8db21c1d57bee9ca7ab881df87e", "class_name": "RelatedNodeInfo"}}, "text": "RAG research shifted towards\nproviding better information for LLMs to answer more com-\nplex and knowledge-intensive tasks during the inference stage,\nleading to rapid development in RAG studies. As research\nprogressed, the enhancement of RAG was no longer limited\nto the inference stage but began to incorporate more with LLM\nfine-tuning techniques.\nThe burgeoning field of RAG has experienced swift growth,\nyet it has not been accompanied by a systematic synthesis that\ncould clarify its broader trajectory. This survey endeavors to\nfill this gap by mapping out the RAG process and charting\nits evolution and anticipated future paths, with a focus on the\nintegration of RAG within LLMs. This paper considers both\ntechnical paradigms and research methods, summarizing three\nmain research paradigms from over 100 RAG studies, and\nanalyzing key technologies in the core stages of \u201cRetrieval,\u201d\n\u201cGeneration,\u201d and \u201cAugmentation.\u201d On the other hand, current\nresearch tends to focus more on methods, lacking analysis and\nsummarization of how to evaluate RAG. This paper compre-\nhensively reviews the downstream tasks, datasets, benchmarks,\nand evaluation methods applicable to RAG. Overall, this\npaper sets out to meticulously compile and categorize the\nfoundational technical concepts, historical progression, and\nthe spectrum of RAG methodologies and applications that\nhave emerged post-LLMs. It is designed to equip readers and\nprofessionals with a detailed and structured understanding of\nboth large models and RAG. It aims to illuminate the evolution\nof retrieval augmentation techniques, assess the strengths and\nweaknesses of various approaches in their respective contexts,\nand speculate on upcoming trends and innovations.\nOur contributions are as follows:\n\u2022 In this survey, we present a thorough and systematic\nreview of the state-of-the-art RAG methods, delineating\nits evolution through paradigms including naive RAG,\narXiv:2312.10997v5  [cs.CL]  27 Mar 2024", "mimetype": "text/plain", "start_char_idx": 3494, "end_char_idx": 5457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e982401-b33f-4181-90b7-10b349e28e39": {"__data__": {"id_": "3e982401-b33f-4181-90b7-10b349e28e39", "embedding": null, "metadata": {"page_label": "2", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d9917be-8db3-4511-b957-1bcc2b4be680", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "bf7b2abcaa3818a12fbd82c9085e424b293573a639749eb40460f7993ec12e2e", "class_name": "RelatedNodeInfo"}}, "text": "2\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\nadvanced RAG, and modular RAG. This review contex-\ntualizes the broader scope of RAG research within the\nlandscape of LLMs.\n\u2022 We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\nof \u201cRetrieval\u201d, \u201cGeneration\u201d and \u201cAugmentation\u201d, and\ndelve into their synergies, elucidating how these com-\nponents intricately collaborate to form a cohesive and\neffective RAG framework.\n\u2022 We have summarized the current assessment methods of\nRAG, covering 26 tasks, nearly 50 datasets, outlining\nthe evaluation objectives and metrics, as well as the\ncurrent evaluation benchmarks and tools. Additionally,\nwe anticipate future directions for RAG, emphasizing\npotential enhancements to tackle current challenges.\nThe paper unfolds as follows: Section II introduces the\nmain concept and current paradigms of RAG. The following\nthree sections explore core components\u2014\u201cRetrieval\u201d, \u201cGen-\neration\u201d and \u201cAugmentation\u201d, respectively. Section III focuses\non optimization methods in retrieval,including indexing, query\nand embedding optimization. Section IV concentrates on post-\nretrieval process and LLM fine-tuning in generation. Section V\nanalyzes the three augmentation processes. Section VI focuses\non RAG\u2019s downstream tasks and evaluation system. Sec-\ntion VII mainly discusses the challenges that RAG currently\nfaces and its future development directions. At last, the paper\nconcludes in Section VIII.\nII. O VERVIEW OF RAG\nA typical application of RAG is illustrated in Figure 2.\nHere, a user poses a question to ChatGPT about a recent,\nwidely discussed news. Given ChatGPT\u2019s reliance on pre-\ntraining data, it initially lacks the capacity to provide up-\ndates on recent developments. RAG bridges this information\ngap by sourcing and incorporating knowledge from external\ndatabases. In this case, it gathers relevant news articles related\nto the user\u2019s query. These articles, combined with the original\nquestion, form a comprehensive prompt that empowers LLMs\nto generate a well-informed answer.\nThe RAG research paradigm is continuously evolving, and\nwe categorize it into three stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\nof the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\nA. Naive RAG\nThe Naive RAG research paradigm represents the earli-\nest methodology, which gained prominence shortly after the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a2c8590-90ff-4ca1-915a-45629f0eeaf2": {"__data__": {"id_": "1a2c8590-90ff-4ca1-915a-45629f0eeaf2", "embedding": null, "metadata": {"page_label": "3", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c844d012-1072-4175-91e7-79f4de4f077a", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "438ea858f094990dd5ace549874102096ab6d4a0913b883847e22927ff30157a", "class_name": "RelatedNodeInfo"}}, "text": "3\nFig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks,\nencoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)\nGeneration. Input the original question and the retrieved chunks together into LLM to generate the final answer.\nwidespread adoption of ChatGPT. The Naive RAG follows\na traditional process that includes indexing, retrieval, and\ngeneration, which is also characterized as a \u201cRetrieve-Read\u201d\nframework [7].\nIndexing starts with the cleaning and extraction of raw data\nin diverse formats like PDF, HTML, Word, and Markdown,\nwhich is then converted into a uniform plain text format. To\naccommodate the context limitations of language models, text\nis segmented into smaller, digestible chunks. Chunks are then\nencoded into vector representations using an embedding model\nand stored in vector database. This step is crucial for enabling\nefficient similarity searches in the subsequent retrieval phase.\nRetrieval. Upon receipt of a user query, the RAG system\nemploys the same encoding model utilized during the indexing\nphase to transform the query into a vector representation.\nIt then computes the similarity scores between the query\nvector and the vector of chunks within the indexed corpus.\nThe system prioritizes and retrieves the top K chunks that\ndemonstrate the greatest similarity to the query. These chunks\nare subsequently used as the expanded context in prompt.\nGeneration. The posed query and selected documents are\nsynthesized into a coherent prompt to which a large language\nmodel is tasked with formulating a response. The model\u2019s\napproach to answering may vary depending on task-specific\ncriteria, allowing it to either draw upon its inherent parametric\nknowledge or restrict its responses to the information con-\ntained within the provided documents. In cases of ongoing\ndialogues, any existing conversational history can be integrated\ninto the prompt, enabling the model to engage in multi-turn\ndialogue interactions effectively.\nHowever, Naive RAG encounters notable drawbacks:\nRetrieval Challenges . The retrieval phase often struggles\nwith precision and recall, leading to the selection of misaligned\nor irrelevant chunks, and the missing of crucial information.\nGeneration Difficulties. In generating responses, the model\nmay face the issue of hallucination, where it produces con-\ntent not supported by the retrieved context. This phase can\nalso suffer from irrelevance, toxicity, or bias in the outputs,\ndetracting from the quality and reliability of the responses.\nAugmentation Hurdles . Integrating retrieved information\nwith the different task can be challenging, sometimes resulting\nin disjointed or incoherent outputs. The process may also\nencounter redundancy when similar information is retrieved\nfrom multiple sources, leading to repetitive responses. Deter-\nmining the significance and relevance of various passages and\nensuring stylistic and tonal consistency add further complexity.\nFacing complex issues, a single retrieval based on the original\nquery may not suffice to acquire adequate context information.\nMoreover, there\u2019s a concern that generation models might\noverly rely on augmented information, leading to outputs that\nsimply echo retrieved content without adding insightful or\nsynthesized information.\nB. Advanced RAG\nAdvanced RAG introduces specific improvements to over-\ncome the limitations of Naive RAG. Focusing on enhancing re-\ntrieval quality, it employs pre-retrieval and post-retrieval strate-\ngies. To tackle the indexing issues, Advanced RAG refines\nits indexing techniques through the use of a sliding window\napproach, fine-grained segmentation, and the incorporation of\nmetadata. Additionally, it incorporates several optimization\nmethods to streamline the retrieval process [8].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e22e3fe-324e-4026-aead-1dee8f8aab59": {"__data__": {"id_": "6e22e3fe-324e-4026-aead-1dee8f8aab59", "embedding": null, "metadata": {"page_label": "4", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b851bf06-f4c9-4140-94b6-dd2b06d017f7", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "69715634dd87d8c9f88322f349ecb9831675369e75f3c4e267199750b986f1ed", "class_name": "RelatedNodeInfo"}}, "text": "4\nFig. 3. Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\nAdvanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\nchain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the\nintroduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\ngeneration; it includes methods such as iterative and adaptive retrieval.\nPre-retrieval process. In this stage, the primary focus is\non optimizing the indexing structure and the original query.\nThe goal of optimizing indexing is to enhance the quality of\nthe content being indexed. This involves strategies: enhancing\ndata granularity, optimizing index structures, adding metadata,\nalignment optimization, and mixed retrieval. While the goal\nof query optimization is to make the user\u2019s original question\nclearer and more suitable for the retrieval task. Common\nmethods include query rewriting query transformation, query\nexpansion and other techniques [7], [9]\u2013[11].\nPost-Retrieval Process. Once relevant context is retrieved,\nit\u2019s crucial to integrate it effectively with the query. The main\nmethods in post-retrieval process include rerank chunks and\ncontext compressing. Re-ranking the retrieved information to\nrelocate the most relevant content to the edges of the prompt is\na key strategy. This concept has been implemented in frame-\nworks such as LlamaIndex 2, LangChain3, and HayStack [12].\nFeeding all relevant documents directly into LLMs can lead\nto information overload, diluting the focus on key details with\nirrelevant content.To mitigate this, post-retrieval efforts con-\ncentrate on selecting the essential information, emphasizing\ncritical sections, and shortening the context to be processed.\n2https://www.llamaindex.ai\n3https://www.langchain.com/\nC. Modular RAG\nThe modular RAG architecture advances beyond the for-\nmer two RAG paradigms, offering enhanced adaptability and\nversatility. It incorporates diverse strategies for improving its\ncomponents, such as adding a search module for similarity\nsearches and refining the retriever through fine-tuning. Inno-\nvations like restructured RAG modules [13] and rearranged\nRAG pipelines [14] have been introduced to tackle specific\nchallenges. The shift towards a modular RAG approach is\nbecoming prevalent, supporting both sequential processing and\nintegrated end-to-end training across its components. Despite\nits distinctiveness, Modular RAG builds upon the foundational\nprinciples of Advanced and Naive RAG, illustrating a progres-\nsion and refinement within the RAG family.\n1) New Modules: The Modular RAG framework introduces\nadditional specialized components to enhance retrieval and\nprocessing capabilities. The Search module adapts to spe-\ncific scenarios, enabling direct searches across various data\nsources like search engines, databases, and knowledge graphs,\nusing LLM-generated code and query languages [15]. RAG-\nFusion addresses traditional search limitations by employing\na multi-query strategy that expands user queries into diverse\nperspectives, utilizing parallel vector searches and intelligent\nre-ranking to uncover both explicit and transformative knowl-\nedge [16]. The Memory module leverages the LLM\u2019s memory\nto guide retrieval, creating an unbounded memory pool that", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "284e0294-fcd7-492b-9a69-a2136ca1ed9e": {"__data__": {"id_": "284e0294-fcd7-492b-9a69-a2136ca1ed9e", "embedding": null, "metadata": {"page_label": "5", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "53102d6c-a640-4c4f-baad-a18dbcf4efc7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "18b3e5ddf55f407b46ddd9a95bae8ce7133b31d546264befaacc6548c06e81f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8964255e-b611-434d-95bb-441c4aca752e", "node_type": "1", "metadata": {}, "hash": "714ea17c5c2020144f5a275a6326bf6f6d58eadcebe51374d30079d744efdbe3", "class_name": "RelatedNodeInfo"}}, "text": "5\naligns the text more closely with data distribution through iter-\native self-enhancement [17], [18]. Routing in the RAG system\nnavigates through diverse data sources, selecting the optimal\npathway for a query, whether it involves summarization,\nspecific database searches, or merging different information\nstreams [19]. The Predict module aims to reduce redundancy\nand noise by generating context directly through the LLM,\nensuring relevance and accuracy [13]. Lastly, the Task Adapter\nmodule tailors RAG to various downstream tasks, automating\nprompt retrieval for zero-shot inputs and creating task-specific\nretrievers through few-shot query generation [20], [21] .This\ncomprehensive approach not only streamlines the retrieval pro-\ncess but also significantly improves the quality and relevance\nof the information retrieved, catering to a wide array of tasks\nand queries with enhanced precision and flexibility.\n2) New Patterns: Modular RAG offers remarkable adapt-\nability by allowing module substitution or reconfiguration\nto address specific challenges. This goes beyond the fixed\nstructures of Naive and Advanced RAG, characterized by a\nsimple \u201cRetrieve\u201d and \u201cRead\u201d mechanism. Moreover, Modular\nRAG expands this flexibility by integrating new modules or\nadjusting interaction flow among existing ones, enhancing its\napplicability across different tasks.\nInnovations such as the Rewrite-Retrieve-Read [7]model\nleverage the LLM\u2019s capabilities to refine retrieval queries\nthrough a rewriting module and a LM-feedback mechanism\nto update rewriting model., improving task performance.\nSimilarly, approaches like Generate-Read [13] replace tradi-\ntional retrieval with LLM-generated content, while Recite-\nRead [22] emphasizes retrieval from model weights, enhanc-\ning the model\u2019s ability to handle knowledge-intensive tasks.\nHybrid retrieval strategies integrate keyword, semantic, and\nvector searches to cater to diverse queries. Additionally, em-\nploying sub-queries and hypothetical document embeddings\n(HyDE) [11] seeks to improve retrieval relevance by focusing\non embedding similarities between generated answers and real\ndocuments.\nAdjustments in module arrangement and interaction, such\nas the Demonstrate-Search-Predict (DSP) [23] framework\nand the iterative Retrieve-Read-Retrieve-Read flow of ITER-\nRETGEN [14], showcase the dynamic use of module out-\nputs to bolster another module\u2019s functionality, illustrating a\nsophisticated understanding of enhancing module synergy.\nThe flexible orchestration of Modular RAG Flow showcases\nthe benefits of adaptive retrieval through techniques such as\nFLARE [24] and Self-RAG [25]. This approach transcends\nthe fixed RAG retrieval process by evaluating the necessity\nof retrieval based on different scenarios. Another benefit of\na flexible architecture is that the RAG system can more\neasily integrate with other technologies (such as fine-tuning\nor reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\nD. RAG vs Fine-tuning\nThe augmentation of LLMs has attracted considerable atten-\ntion due to their growing prevalence. Among the optimization\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt engineering. Each method has distinct charac-\nteristics as illustrated in Figure 4. We used a quadrant chart to\nillustrate the differences among three methods in two dimen-\nsions: external knowledge requirements and model adaption\nrequirements. Prompt engineering leverages a model\u2019s inherent\ncapabilities with minimum necessity for external knowledge\nand model adaption. RAG can be likened to providing a model\nwith a tailored textbook for information retrieval, ideal for pre-\ncise information retrieval tasks. In contrast, FT is comparable\nto a student internalizing knowledge over time, suitable for\nscenarios requiring replication of specific structures, styles, or\nformats.\nRAG excels in dynamic environments by offering real-\ntime knowledge updates and effective utilization of external\nknowledge sources with high interpretability. However, it\ncomes with higher latency and ethical considerations regarding\ndata retrieval. On the other hand, FT is more static, requiring\nretraining for updates but enabling deep customization of the\nmodel\u2019s behavior and style. It demands significant compu-\ntational resources for dataset preparation and training, and\nwhile it can reduce hallucinations, it may face challenges with\nunfamiliar data.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8964255e-b611-434d-95bb-441c4aca752e": {"__data__": {"id_": "8964255e-b611-434d-95bb-441c4aca752e", "embedding": null, "metadata": {"page_label": "5", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "53102d6c-a640-4c4f-baad-a18dbcf4efc7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "18b3e5ddf55f407b46ddd9a95bae8ce7133b31d546264befaacc6548c06e81f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "284e0294-fcd7-492b-9a69-a2136ca1ed9e", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "0891563aea63d092408d7bd82a3e03a3de92e5639340c6fa46ebac663647baf3", "class_name": "RelatedNodeInfo"}}, "text": "Prompt engineering leverages a model\u2019s inherent\ncapabilities with minimum necessity for external knowledge\nand model adaption. RAG can be likened to providing a model\nwith a tailored textbook for information retrieval, ideal for pre-\ncise information retrieval tasks. In contrast, FT is comparable\nto a student internalizing knowledge over time, suitable for\nscenarios requiring replication of specific structures, styles, or\nformats.\nRAG excels in dynamic environments by offering real-\ntime knowledge updates and effective utilization of external\nknowledge sources with high interpretability. However, it\ncomes with higher latency and ethical considerations regarding\ndata retrieval. On the other hand, FT is more static, requiring\nretraining for updates but enabling deep customization of the\nmodel\u2019s behavior and style. It demands significant compu-\ntational resources for dataset preparation and training, and\nwhile it can reduce hallucinations, it may face challenges with\nunfamiliar data.\nIn multiple evaluations of their performance on various\nknowledge-intensive tasks across different topics, [28] re-\nvealed that while unsupervised fine-tuning shows some im-\nprovement, RAG consistently outperforms it, for both exist-\ning knowledge encountered during training and entirely new\nknowledge. Additionally, it was found that LLMs struggle\nto learn new factual information through unsupervised fine-\ntuning. The choice between RAG and FT depends on the\nspecific needs for data dynamics, customization, and com-\nputational capabilities in the application context. RAG and\nFT are not mutually exclusive and can complement each\nother, enhancing a model\u2019s capabilities at different levels.\nIn some instances, their combined use may lead to optimal\nperformance. The optimization process involving RAG and FT\nmay require multiple iterations to achieve satisfactory results.\nIII. R ETRIEVAL\nIn the context of RAG, it is crucial to efficiently retrieve\nrelevant documents from the data source. There are several\nkey issues involved, such as the retrieval source, retrieval\ngranularity, pre-processing of the retrieval, and selection of\nthe corresponding embedding model.\nA. Retrieval Source\nRAG relies on external knowledge to enhance LLMs, while\nthe type of retrieval source and the granularity of retrieval\nunits both affect the final generation results.\n1) Data Structure: Initially, text is s the mainstream source\nof retrieval. Subsequently, the retrieval source expanded to in-\nclude semi-structured data (PDF) and structured data (Knowl-\nedge Graph, KG) for enhancement. In addition to retrieving\nfrom original external sources, there is also a growing trend in\nrecent researches towards utilizing content generated by LLMs\nthemselves for retrieval and enhancement purposes.", "mimetype": "text/plain", "start_char_idx": 3584, "end_char_idx": 6363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d62e0cf2-f0eb-4f5b-ac56-124b06b0a874": {"__data__": {"id_": "d62e0cf2-f0eb-4f5b-ac56-124b06b0a874", "embedding": null, "metadata": {"page_label": "6", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d542561-e275-4984-9121-7a6ad0e6bf52", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "ae3bf9892627bfc219270cdde4030228633440e31165847171338bb27a99bbca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b212b645-338d-4d26-a9e6-3df4f47a738b", "node_type": "1", "metadata": {}, "hash": "44df125f16125945d28589c9b3a5fa268ea93ab82d6ee6d9fd6a88757a4bd44e", "class_name": "RelatedNodeInfo"}}, "text": "6\nTABLE I\nSUMMARY OF RAG METHODS\nMethod Retrieval Source Retrieval\nData Type\nRetrieval\nGranularity\nAugmentation\nStage\nRetrieval\nprocess\nCoG [29] Wikipedia Text Phrase Pre-training Iterative\nDenseX [30] FactoidWiki Text Proposition Inference Once\nEAR [31] Dataset-base Text Sentence Tuning Once\nUPRISE [20] Dataset-base Text Sentence Tuning Once\nRAST [32] Dataset-base Text Sentence Tuning Once\nSelf-Mem [17] Dataset-base Text Sentence Tuning Iterative\nFLARE [24] Search Engine,Wikipedia Text Sentence Tuning Adaptive\nPGRA [33] Wikipedia Text Sentence Inference Once\nFILCO [34] Wikipedia Text Sentence Inference Once\nRADA [35] Dataset-base Text Sentence Inference Once\nFilter-rerank [36] Synthesized dataset Text Sentence Inference Once\nR-GQA [37] Dataset-base Text Sentence Pair Tuning Once\nLLM-R [38] Dataset-base Text Sentence Pair Inference Iterative\nTIGER [39] Dataset-base Text Item-base Pre-training Once\nLM-Indexer [40] Dataset-base Text Item-base Tuning Once\nBEQUE [9] Dataset-base Text Item-base Tuning Once\nCT-RAG [41] Synthesized dataset Text Item-base Tuning Once\nAtlas [42] Wikipedia, Common Crawl Text Chunk Pre-training Iterative\nRA VEN [43] Wikipedia Text Chunk Pre-training Once\nRETRO++ [44] Pre-training Corpus Text Chunk Pre-training Iterative\nINSTRUCTRETRO [45] Pre-training corpus Text Chunk Pre-training Iterative\nRRR [7] Search Engine Text Chunk Tuning Once\nRA-e2e [46] Dataset-base Text Chunk Tuning Once\nPROMPTAGATOR [21] BEIR Text Chunk Tuning Once\nAAR [47] MSMARCO,Wikipedia Text Chunk Tuning Once\nRA-DIT [27] Common Crawl,Wikipedia Text Chunk Tuning Once\nRAG-Robust [48] Wikipedia Text Chunk Tuning Once\nRA-Long-Form [49] Dataset-base Text Chunk Tuning Once\nCoN [50] Wikipedia Text Chunk Tuning Once\nSelf-RAG [25] Wikipedia Text Chunk Tuning Adaptive\nBGM [26] Wikipedia Text Chunk Inference Once\nCoQ [51] Wikipedia Text Chunk Inference Iterative\nToken-Elimination [52] Wikipedia Text Chunk Inference Once\nPaperQA [53] Arxiv,Online Database,PubMed Text Chunk Inference Iterative\nNoiseRAG [54] FactoidWiki Text Chunk Inference Once\nIAG [55] Search Engine,Wikipedia Text Chunk Inference Once\nNoMIRACL [56] Wikipedia Text Chunk Inference Once\nToC [57] Search Engine,Wikipedia Text Chunk Inference Recursive\nSKR [58] Dataset-base,Wikipedia Text Chunk Inference Adaptive\nITRG [59] Wikipedia Text Chunk Inference Iterative\nRAG-LongContext [60] Dataset-base Text Chunk Inference Once\nITER-RETGEN [14] Wikipedia Text Chunk Inference Iterative\nIRCoT [61] Wikipedia Text Chunk Inference Recursive\nLLM-Knowledge-Boundary [62] Wikipedia Text Chunk Inference Once\nRAPTOR [63] Dataset-base Text Chunk Inference Recursive\nRECITE [22] LLMs Text Chunk Inference Once\nICRALM [64] Pile,Wikipedia Text Chunk Inference Iterative\nRetrieve-and-Sample [65] Dataset-base Text Doc Tuning Once\nZemi [66] C4 Text Doc Tuning Once\nCRAG [67] Arxiv Text Doc Inference Once\n1-PAGER [68] Wikipedia Text Doc Inference Iterative\nPRCA [69] Dataset-base Text Doc Inference Once\nQLM-Doc-ranking [70] Dataset-base Text Doc Inference Once\nRecomp [71] Wikipedia Text Doc Inference Once\nDSP [23] Wikipedia Text Doc Inference Iterative\nRePLUG [72] Pile Text Doc Inference Once\nARM-RAG [73] Dataset-base Text Doc Inference Iterative\nGenRead [13] LLMs Text Doc Inference Iterative\nUniMS-RAG [74] Dataset-base Text Multi Tuning Once\nCREA-ICL [19] Dataset-base Crosslingual,Text Sentence Inference Once\nPKG [75] LLM Tabular,Text Chunk Inference Once\nSANTA [76] Dataset-base Code,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3458, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b212b645-338d-4d26-a9e6-3df4f47a738b": {"__data__": {"id_": "b212b645-338d-4d26-a9e6-3df4f47a738b", "embedding": null, "metadata": {"page_label": "6", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d542561-e275-4984-9121-7a6ad0e6bf52", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "ae3bf9892627bfc219270cdde4030228633440e31165847171338bb27a99bbca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d62e0cf2-f0eb-4f5b-ac56-124b06b0a874", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "a865f2f34c6149c1d241006eac3c73662e168047165d55a5cd62002a0a7f48d5", "class_name": "RelatedNodeInfo"}}, "text": "Text Sentence Inference Once\nPKG [75] LLM Tabular,Text Chunk Inference Once\nSANTA [76] Dataset-base Code,Text Item Pre-training Once\nSURGE [77] Freebase KG Sub-Graph Tuning Once\nMK-ToD [78] Dataset-base KG Entity Tuning Once\nDual-Feedback-ToD [79] Dataset-base KG Entity Sequence Tuning Once\nKnowledGPT [15] Dataset-base KG Triplet Inference Muti-time\nFABULA [80] Dataset-base,Graph KG Entity Inference Once\nHyKGE [81] CMeKG KG Entity Inference Once\nKALMV [82] Wikipedia KG Triplet Inference Iterative\nRoG [83] Freebase KG Triplet Inference Iterative\nG-Retriever [84] Dataset-base TextGraph Sub-Graph Inference Once", "mimetype": "text/plain", "start_char_idx": 3353, "end_char_idx": 3968, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33307481-bd83-4f83-899b-72fefe367f1d": {"__data__": {"id_": "33307481-bd83-4f83-899b-72fefe367f1d", "embedding": null, "metadata": {"page_label": "7", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b54e3c4b-2163-4710-8425-e7ea55298923", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "d35c3c27dbb0afe32745c39e34ae9e2cc8ab63e26351782722a8c7a3f64db23e", "class_name": "RelatedNodeInfo"}}, "text": "7\nFig. 4. RAG compared with other model optimization methods in the aspects of \u201cExternal Knowledge Required\u201d and \u201cModel Adaption Required\u201d. Prompt\nEngineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on\nthe other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research\nprogresses, Modular RAG has become more integrated with fine-tuning techniques.\nUnstructured Data , such as text, is the most widely used\nretrieval source, which are mainly gathered from corpus. For\nopen-domain question-answering (ODQA) tasks, the primary\nretrieval sources are Wikipedia Dump with the current major\nversions including HotpotQA 4 (1st October , 2017), DPR5 (20\nDecember, 2018). In addition to encyclopedic data, common\nunstructured data includes cross-lingual text [19] and domain-\nspecific data (such as medical [67]and legal domains [29]).\nSemi-structured data. typically refers to data that contains a\ncombination of text and table information, such as PDF. Han-\ndling semi-structured data poses challenges for conventional\nRAG systems due to two main reasons. Firstly, text splitting\nprocesses may inadvertently separate tables, leading to data\ncorruption during retrieval. Secondly, incorporating tables into\nthe data can complicate semantic similarity searches. When\ndealing with semi-structured data, one approach involves lever-\naging the code capabilities of LLMs to execute Text-2-SQL\nqueries on tables within databases, such as TableGPT [85].\nAlternatively, tables can be transformed into text format for\nfurther analysis using text-based methods [75]. However, both\nof these methods are not optimal solutions, indicating substan-\ntial research opportunities in this area.\nStructured data , such as knowledge graphs (KGs) [86] ,\nwhich are typically verified and can provide more precise in-\nformation. KnowledGPT [15] generates KB search queries and\nstores knowledge in a personalized base, enhancing the RAG\nmodel\u2019s knowledge richness. In response to the limitations of\nLLMs in understanding and answering questions about textual\ngraphs, G-Retriever [84] integrates Graph Neural Networks\n4https://hotpotqa.github.io/wiki-readme.html\n5https://github.com/facebookresearch/DPR\n(GNNs), LLMs and RAG, enhancing graph comprehension\nand question-answering capabilities through soft prompting\nof the LLM, and employs the Prize-Collecting Steiner Tree\n(PCST) optimization problem for targeted graph retrieval. On\nthe contrary, it requires additional effort to build, validate,\nand maintain structured databases. On the contrary, it requires\nadditional effort to build, validate, and maintain structured\ndatabases.\nLLMs-Generated Content. Addressing the limitations of\nexternal auxiliary information in RAG, some research has\nfocused on exploiting LLMs\u2019 internal knowledge. SKR [58]\nclassifies questions as known or unknown, applying retrieval\nenhancement selectively. GenRead [13] replaces the retriever\nwith an LLM generator, finding that LLM-generated contexts\noften contain more accurate answers due to better alignment\nwith the pre-training objectives of causal language modeling.\nSelfmem [17] iteratively creates an unbounded memory pool\nwith a retrieval-enhanced generator, using a memory selec-\ntor to choose outputs that serve as dual problems to the\noriginal question, thus self-enhancing the generative model.\nThese methodologies underscore the breadth of innovative\ndata source utilization in RAG, striving to improve model\nperformance and task effectiveness.\n2) Retrieval Granularity: Another important factor besides\nthe data format of the retrieval source is the granularity of\nthe retrieved data. Coarse-grained retrieval units theoretically\ncan provide more relevant information for the problem, but\nthey may also contain redundant content, which could distract\nthe retriever and language models in downstream tasks [50],\n[87]. On the other hand, fine-grained retrieval unit granularity\nincreases the burden of retrieval and does not guarantee seman-\ntic integrity and meeting the required knowledge. Choosing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54257664-c5a5-42d6-86d0-67b5aeef3c71": {"__data__": {"id_": "54257664-c5a5-42d6-86d0-67b5aeef3c71", "embedding": null, "metadata": {"page_label": "8", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e5dd65d2-5eda-4373-a9f5-258e655492bf", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "ccbf3c4420edca8343d4a731b4e732a2d3fa0ef3d473fa064d2c9d14ad4326ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03dcabff-c43a-4000-ae85-f2e5bed6d276", "node_type": "1", "metadata": {}, "hash": "8f2a5abf5bff943c8a153e623f5db064e623bc8615e8d37546bc31aa2e41f1aa", "class_name": "RelatedNodeInfo"}}, "text": "8\nthe appropriate retrieval granularity during inference can be\na simple and effective strategy to improve the retrieval and\ndownstream task performance of dense retrievers.\nIn text, retrieval granularity ranges from fine to coarse,\nincluding Token, Phrase, Sentence, Proposition, Chunks, Doc-\nument. Among them, DenseX [30]proposed the concept of\nusing propositions as retrieval units. Propositions are defined\nas atomic expressions in the text, each encapsulating a unique\nfactual segment and presented in a concise, self-contained nat-\nural language format. This approach aims to enhance retrieval\nprecision and relevance. On the Knowledge Graph (KG),\nretrieval granularity includes Entity, Triplet, and sub-Graph.\nThe granularity of retrieval can also be adapted to downstream\ntasks, such as retrieving Item IDs [40]in recommendation tasks\nand Sentence pairs [38]. Detailed information is illustrated in\nTable I.\nB. Indexing Optimization\nIn the Indexing phase, documents will be processed, seg-\nmented, and transformed into Embeddings to be stored in a\nvector database. The quality of index construction determines\nwhether the correct context can be obtained in the retrieval\nphase.\n1) Chunking Strategy: The most common method is to split\nthe document into chunks on a fixed number of tokens (e.g.,\n100, 256, 512) [88]. Larger chunks can capture more context,\nbut they also generate more noise, requiring longer processing\ntime and higher costs. While smaller chunks may not fully\nconvey the necessary context, they do have less noise. How-\never, chunks leads to truncation within sentences, prompting\nthe optimization of a recursive splits and sliding window meth-\nods, enabling layered retrieval by merging globally related\ninformation across multiple retrieval processes [89]. Never-\ntheless, these approaches still cannot strike a balance between\nsemantic completeness and context length. Therefore, methods\nlike Small2Big have been proposed, where sentences (small)\nare used as the retrieval unit, and the preceding and following\nsentences are provided as (big) context to LLMs [90].\n2) Metadata Attachments: Chunks can be enriched with\nmetadata information such as page number, file name, au-\nthor,category timestamp. Subsequently, retrieval can be filtered\nbased on this metadata, limiting the scope of the retrieval.\nAssigning different weights to document timestamps during\nretrieval can achieve time-aware RAG, ensuring the freshness\nof knowledge and avoiding outdated information.\nIn addition to extracting metadata from the original doc-\numents, metadata can also be artificially constructed. For\nexample, adding summaries of paragraph, as well as intro-\nducing hypothetical questions. This method is also known as\nReverse HyDE. Specifically, using LLM to generate questions\nthat can be answered by the document, then calculating the\nsimilarity between the original question and the hypothetical\nquestion during retrieval to reduce the semantic gap between\nthe question and the answer.\n3) Structural Index: One effective method for enhancing\ninformation retrieval is to establish a hierarchical structure for\nthe documents. By constructing In structure, RAG system can\nexpedite the retrieval and processing of pertinent data.\nHierarchical index structure . File are arranged in parent-\nchild relationships, with chunks linked to them. Data sum-\nmaries are stored at each node, aiding in the swift traversal\nof data and assisting the RAG system in determining which\nchunks to extract. This approach can also mitigate the illusion\ncaused by block extraction issues.\nKnowledge Graph index . Utilize KG in constructing the\nhierarchical structure of documents contributes to maintaining\nconsistency. It delineates the connections between different\nconcepts and entities, markedly reducing the potential for\nillusions. Another advantage is the transformation of the\ninformation retrieval process into instructions that LLM can\ncomprehend, thereby enhancing the accuracy of knowledge\nretrieval and enabling LLM to generate contextually coherent\nresponses, thus improving the overall efficiency of the RAG\nsystem. To capture the logical relationship between document\ncontent and structure, KGP [91] proposed a method of building\nan index between multiple documents using KG. This KG\nconsists of nodes (representing paragraphs or structures in the\ndocuments, such as pages and tables) and edges (indicating\nsemantic/lexical similarity between paragraphs or relationships\nwithin the document structure), effectively addressing knowl-\nedge retrieval and reasoning problems in a multi-document\nenvironment.\nC. Query Optimization\nOne of the primary challenges with Naive RAG is its\ndirect reliance on the user\u2019s original query as the basis for\nretrieval.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4758, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03dcabff-c43a-4000-ae85-f2e5bed6d276": {"__data__": {"id_": "03dcabff-c43a-4000-ae85-f2e5bed6d276", "embedding": null, "metadata": {"page_label": "8", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e5dd65d2-5eda-4373-a9f5-258e655492bf", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "ccbf3c4420edca8343d4a731b4e732a2d3fa0ef3d473fa064d2c9d14ad4326ca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54257664-c5a5-42d6-86d0-67b5aeef3c71", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "7feab313c7daebb8724ad49e74a69390c0dff4d96732072a32962800518e00bb", "class_name": "RelatedNodeInfo"}}, "text": "Another advantage is the transformation of the\ninformation retrieval process into instructions that LLM can\ncomprehend, thereby enhancing the accuracy of knowledge\nretrieval and enabling LLM to generate contextually coherent\nresponses, thus improving the overall efficiency of the RAG\nsystem. To capture the logical relationship between document\ncontent and structure, KGP [91] proposed a method of building\nan index between multiple documents using KG. This KG\nconsists of nodes (representing paragraphs or structures in the\ndocuments, such as pages and tables) and edges (indicating\nsemantic/lexical similarity between paragraphs or relationships\nwithin the document structure), effectively addressing knowl-\nedge retrieval and reasoning problems in a multi-document\nenvironment.\nC. Query Optimization\nOne of the primary challenges with Naive RAG is its\ndirect reliance on the user\u2019s original query as the basis for\nretrieval. Formulating a precise and clear question is difficult,\nand imprudent queries result in subpar retrieval effectiveness.\nSometimes, the question itself is complex, and the language\nis not well-organized. Another difficulty lies in language\ncomplexity ambiguity. Language models often struggle when\ndealing with specialized vocabulary or ambiguous abbrevi-\nations with multiple meanings. For instance, they may not\ndiscern whether \u201cLLM\u201d refers to large language model or a\nMaster of Laws in a legal context.\n1) Query Expansion: Expanding a single query into mul-\ntiple queries enriches the content of the query, providing\nfurther context to address any lack of specific nuances, thereby\nensuring the optimal relevance of the generated answers.\nMulti-Query. By employing prompt engineering to expand\nqueries via LLMs, these queries can then be executed in\nparallel. The expansion of queries is not random, but rather\nmeticulously designed.\nSub-Query. The process of sub-question planning represents\nthe generation of the necessary sub-questions to contextualize\nand fully answer the original question when combined. This\nprocess of adding relevant context is, in principle, similar\nto query expansion. Specifically, a complex question can be\ndecomposed into a series of simpler sub-questions using the\nleast-to-most prompting method [92].\nChain-of-Verification(CoVe). The expanded queries undergo\nvalidation by LLM to achieve the effect of reducing halluci-\nnations. Validated expanded queries typically exhibit higher\nreliability [93].", "mimetype": "text/plain", "start_char_idx": 3830, "end_char_idx": 6291, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd9b2d05-26a2-4463-999d-9740e106b536": {"__data__": {"id_": "fd9b2d05-26a2-4463-999d-9740e106b536", "embedding": null, "metadata": {"page_label": "9", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1bf610f3-69fc-4de9-ae7c-b738a8f645da", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "8c0582c67d0653bc0c0eab131f40fb14d0497d8f4044304298d9d6b58be5f6ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9df8ad8-ef33-47cb-ae97-8d1696759811", "node_type": "1", "metadata": {}, "hash": "bd87d430e327c33b3617f949bd4cf316ac35b35a3e7e888b63e284665a2d8fbe", "class_name": "RelatedNodeInfo"}}, "text": "9\n2) Query Transformation: The core concept is to retrieve\nchunks based on a transformed query instead of the user\u2019s\noriginal query.\nQuery Rewrite.The original queries are not always optimal\nfor LLM retrieval, especially in real-world scenarios. There-\nfore, we can prompt LLM to rewrite the queries. In addition to\nusing LLM for query rewriting, specialized smaller language\nmodels, such as RRR (Rewrite-retrieve-read) [7]. The imple-\nmentation of the query rewrite method in the Taobao, known\nas BEQUE [9] has notably enhanced recall effectiveness for\nlong-tail queries, resulting in a rise in GMV .\nAnother query transformation method is to use prompt\nengineering to let LLM generate a query based on the original\nquery for subsequent retrieval. HyDE [11] construct hypothet-\nical documents (assumed answers to the original query). It\nfocuses on embedding similarity from answer to answer rather\nthan seeking embedding similarity for the problem or query.\nUsing the Step-back Prompting method [10], the original\nquery is abstracted to generate a high-level concept question\n(step-back question). In the RAG system, both the step-back\nquestion and the original query are used for retrieval, and both\nthe results are utilized as the basis for language model answer\ngeneration.\n3) Query Routing: Based on varying queries, routing to\ndistinct RAG pipeline,which is suitable for a versatile RAG\nsystem designed to accommodate diverse scenarios.\nMetadata Router/ Filter . The first step involves extracting\nkeywords (entity) from the query, followed by filtering based\non the keywords and metadata within the chunks to narrow\ndown the search scope.\nSemantic Router is another method of routing involves\nleveraging the semantic information of the query. Specific\napprach see Semantic Router 6. Certainly, a hybrid routing\napproach can also be employed, combining both semantic and\nmetadata-based methods for enhanced query routing.\nD. Embedding\nIn RAG, retrieval is achieved by calculating the similarity\n(e.g. cosine similarity) between the embeddings of the ques-\ntion and document chunks, where the semantic representation\ncapability of embedding models plays a key role. This mainly\nincludes a sparse encoder (BM25) and a dense retriever (BERT\narchitecture Pre-training language models). Recent research\nhas introduced prominent embedding models such as AngIE,\nV oyage, BGE,etc [94]\u2013[96], which are benefit from multi-task\ninstruct tuning. Hugging Face\u2019s MTEB leaderboard 7 evaluates\nembedding models across 8 tasks, covering 58 datasests. Ad-\nditionally, C-MTEB focuses on Chinese capability, covering\n6 tasks and 35 datasets. There is no one-size-fits-all answer\nto \u201cwhich embedding model to use.\u201d However, some specific\nmodels are better suited for particular use cases.\n1) Mix/hybrid Retrieval : Sparse and dense embedding\napproaches capture different relevance features and can ben-\nefit from each other by leveraging complementary relevance\ninformation. For instance, sparse retrieval models can be used\n6https://github.com/aurelio-labs/semantic-router\n7https://huggingface.co/spaces/mteb/leaderboard\nto provide initial search results for training dense retrieval\nmodels. Additionally, pre-training language models (PLMs)\ncan be utilized to learn term weights to enhance sparse\nretrieval. Specifically, it also demonstrates that sparse retrieval\nmodels can enhance the zero-shot retrieval capability of dense\nretrieval models and assist dense retrievers in handling queries\ncontaining rare entities, thereby improving robustness.\n2) Fine-tuning Embedding Model: In instances where the\ncontext significantly deviates from pre-training corpus, partic-\nularly within highly specialized disciplines such as healthcare,\nlegal practice, and other sectors replete with proprietary jargon,\nfine-tuning the embedding model on your own domain dataset\nbecomes essential to mitigate such discrepancies.\nIn addition to supplementing domain knowledge, another\npurpose of fine-tuning is to align the retriever and generator,\nfor example, using the results of LLM as the supervision signal\nfor fine-tuning, known as LSR (LM-supervised Retriever).\nPROMPTAGATOR [21] utilizes the LLM as a few-shot query\ngenerator to create task-specific retrievers, addressing chal-\nlenges in supervised fine-tuning, particularly in data-scarce\ndomains. Another approach, LLM-Embedder [97], exploits\nLLMs to generate reward signals across multiple downstream\ntasks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9df8ad8-ef33-47cb-ae97-8d1696759811": {"__data__": {"id_": "d9df8ad8-ef33-47cb-ae97-8d1696759811", "embedding": null, "metadata": {"page_label": "9", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1bf610f3-69fc-4de9-ae7c-b738a8f645da", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "8c0582c67d0653bc0c0eab131f40fb14d0497d8f4044304298d9d6b58be5f6ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd9b2d05-26a2-4463-999d-9740e106b536", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "9a8d237078bf448230f591f6d8d402c2e2ecd03b9db0e8368de7fec6c21e368e", "class_name": "RelatedNodeInfo"}}, "text": "2) Fine-tuning Embedding Model: In instances where the\ncontext significantly deviates from pre-training corpus, partic-\nularly within highly specialized disciplines such as healthcare,\nlegal practice, and other sectors replete with proprietary jargon,\nfine-tuning the embedding model on your own domain dataset\nbecomes essential to mitigate such discrepancies.\nIn addition to supplementing domain knowledge, another\npurpose of fine-tuning is to align the retriever and generator,\nfor example, using the results of LLM as the supervision signal\nfor fine-tuning, known as LSR (LM-supervised Retriever).\nPROMPTAGATOR [21] utilizes the LLM as a few-shot query\ngenerator to create task-specific retrievers, addressing chal-\nlenges in supervised fine-tuning, particularly in data-scarce\ndomains. Another approach, LLM-Embedder [97], exploits\nLLMs to generate reward signals across multiple downstream\ntasks. The retriever is fine-tuned with two types of supervised\nsignals: hard labels for the dataset and soft rewards from\nthe LLMs. This dual-signal approach fosters a more effective\nfine-tuning process, tailoring the embedding model to diverse\ndownstream applications. REPLUG [72] utilizes a retriever\nand an LLM to calculate the probability distributions of the\nretrieved documents and then performs supervised training\nby computing the KL divergence. This straightforward and\neffective training method enhances the performance of the\nretrieval model by using an LM as the supervisory signal,\neliminating the need for specific cross-attention mechanisms.\nMoreover, inspired by RLHF (Reinforcement Learning from\nHuman Feedback), utilizing LM-based feedback to reinforce\nthe retriever through reinforcement learning.\nE. Adapter\nFine-tuning models may present challenges, such as in-\ntegrating functionality through an API or addressing con-\nstraints arising from limited local computational resources.\nConsequently, some approaches opt to incorporate an external\nadapter to aid in alignment.\nTo optimize the multi-task capabilities of LLM, UP-\nRISE [20] trained a lightweight prompt retriever that can\nautomatically retrieve prompts from a pre-built prompt pool\nthat are suitable for a given zero-shot task input. AAR\n(Augmentation-Adapted Retriver) [47] introduces a universal\nadapter designed to accommodate multiple downstream tasks.\nWhile PRCA [69] add a pluggable reward-driven contextual\nadapter to enhance performance on specific tasks. BGM [26]\nkeeps the retriever and LLM fixed,and trains a bridge Seq2Seq\nmodel in between. The bridge model aims to transform the\nretrieved information into a format that LLMs can work with\neffectively, allowing it to not only rerank but also dynami-\ncally select passages for each query, and potentially employ\nmore advanced strategies like repetition. Furthermore, PKG", "mimetype": "text/plain", "start_char_idx": 3536, "end_char_idx": 6344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "737463ba-c93f-4fdd-93fc-eeaadf7111d3": {"__data__": {"id_": "737463ba-c93f-4fdd-93fc-eeaadf7111d3", "embedding": null, "metadata": {"page_label": "10", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bde80ebd-a216-4211-afd9-b0a403478f83", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "4b8e36e243504d739a1a276ec4c86d7fe6dccc6f96c5401c8a1d3a8d28d4520c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e091581-56aa-449a-ab80-da628374840e", "node_type": "1", "metadata": {}, "hash": "bfb0d86fbe6e73e660f2a965868fc09ec51e4de014197d93e88bf6b6fefeb248", "class_name": "RelatedNodeInfo"}}, "text": "10\nintroduces an innovative method for integrating knowledge\ninto white-box models via directive fine-tuning [75]. In this\napproach, the retriever module is directly substituted to gen-\nerate relevant documents according to a query. This method\nassists in addressing the difficulties encountered during the\nfine-tuning process and enhances model performance.\nIV. G ENERATION\nAfter retrieval, it is not a good practice to directly input all\nthe retrieved information to the LLM for answering questions.\nFollowing will introduce adjustments from two perspectives:\nadjusting the retrieved content and adjusting the LLM.\nA. Context Curation\nRedundant information can interfere with the final gener-\nation of LLM, and overly long contexts can also lead LLM\nto the \u201cLost in the middle\u201d problem [98]. Like humans, LLM\ntends to only focus on the beginning and end of long texts,\nwhile forgetting the middle portion. Therefore, in the RAG\nsystem, we typically need to further process the retrieved\ncontent.\n1) Reranking: Reranking fundamentally reorders document\nchunks to highlight the most pertinent results first, effectively\nreducing the overall document pool, severing a dual purpose\nin information retrieval, acting as both an enhancer and a\nfilter, delivering refined inputs for more precise language\nmodel processing [70]. Reranking can be performed using\nrule-based methods that depend on predefined metrics like\nDiversity, Relevance, and MRR, or model-based approaches\nlike Encoder-Decoder models from the BERT series (e.g.,\nSpanBERT), specialized reranking models such as Cohere\nrerank or bge-raranker-large, and general large language mod-\nels like GPT [12], [99].\n2) Context Selection/Compression: A common misconcep-\ntion in the RAG process is the belief that retrieving as many\nrelevant documents as possible and concatenating them to form\na lengthy retrieval prompt is beneficial. However, excessive\ncontext can introduce more noise, diminishing the LLM\u2019s\nperception of key information .\n(Long) LLMLingua [100], [101] utilize small language\nmodels (SLMs) such as GPT-2 Small or LLaMA-7B, to\ndetect and remove unimportant tokens, transforming it into\na form that is challenging for humans to comprehend but\nwell understood by LLMs. This approach presents a direct\nand practical method for prompt compression, eliminating the\nneed for additional training of LLMs while balancing language\nintegrity and compression ratio. PRCA tackled this issue by\ntraining an information extractor [69]. Similarly, RECOMP\nadopts a comparable approach by training an information\ncondenser using contrastive learning [71]. Each training data\npoint consists of one positive sample and five negative sam-\nples, and the encoder undergoes training using contrastive loss\nthroughout this process [102] .\nIn addition to compressing the context, reducing the num-\nber of documents aslo helps improve the accuracy of the\nmodel\u2019s answers. Ma et al. [103] propose the \u201cFilter-Reranker\u201d\nparadigm, which combines the strengths of LLMs and SLMs.\nIn this paradigm, SLMs serve as filters, while LLMs function\nas reordering agents. The research shows that instructing\nLLMs to rearrange challenging samples identified by SLMs\nleads to significant improvements in various Information\nExtraction (IE) tasks. Another straightforward and effective\napproach involves having the LLM evaluate the retrieved\ncontent before generating the final answer. This allows the\nLLM to filter out documents with poor relevance through LLM\ncritique. For instance, in Chatlaw [104], the LLM is prompted\nto self-suggestion on the referenced legal provisions to assess\ntheir relevance.\nB. LLM Fine-tuning\nTargeted fine-tuning based on the scenario and data char-\nacteristics on LLMs can yield better results. This is also one\nof the greatest advantages of using on-premise LLMs. When\nLLMs lack data in a specific domain, additional knowledge can\nbe provided to the LLM through fine-tuning. Huggingface\u2019s\nfine-tuning data can also be used as an initial step.\nAnother benefit of fine-tuning is the ability to adjust the\nmodel\u2019s input and output. For example, it can enable LLM to\nadapt to specific data formats and generate responses in a par-\nticular style as instructed [37].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4222, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e091581-56aa-449a-ab80-da628374840e": {"__data__": {"id_": "1e091581-56aa-449a-ab80-da628374840e", "embedding": null, "metadata": {"page_label": "10", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bde80ebd-a216-4211-afd9-b0a403478f83", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "4b8e36e243504d739a1a276ec4c86d7fe6dccc6f96c5401c8a1d3a8d28d4520c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "737463ba-c93f-4fdd-93fc-eeaadf7111d3", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "28aa8ca5925b775d1e41d29dbda13f7bf18161236e7684c9ffb3b00e3e3befda", "class_name": "RelatedNodeInfo"}}, "text": "This allows the\nLLM to filter out documents with poor relevance through LLM\ncritique. For instance, in Chatlaw [104], the LLM is prompted\nto self-suggestion on the referenced legal provisions to assess\ntheir relevance.\nB. LLM Fine-tuning\nTargeted fine-tuning based on the scenario and data char-\nacteristics on LLMs can yield better results. This is also one\nof the greatest advantages of using on-premise LLMs. When\nLLMs lack data in a specific domain, additional knowledge can\nbe provided to the LLM through fine-tuning. Huggingface\u2019s\nfine-tuning data can also be used as an initial step.\nAnother benefit of fine-tuning is the ability to adjust the\nmodel\u2019s input and output. For example, it can enable LLM to\nadapt to specific data formats and generate responses in a par-\nticular style as instructed [37]. For retrieval tasks that engage\nwith structured data, the SANTA framework [76] implements\na tripartite training regimen to effectively encapsulate both\nstructural and semantic nuances. The initial phase focuses on\nthe retriever, where contrastive learning is harnessed to refine\nthe query and document embeddings.\nAligning LLM outputs with human or retriever preferences\nthrough reinforcement learning is a potential approach. For\ninstance, manually annotating the final generated answers\nand then providing feedback through reinforcement learning.\nIn addition to aligning with human preferences, it is also\npossible to align with the preferences of fine-tuned models\nand retrievers [79]. When circumstances prevent access to\npowerful proprietary models or larger parameter open-source\nmodels, a simple and effective method is to distill the more\npowerful models(e.g. GPT-4). Fine-tuning of LLM can also\nbe coordinated with fine-tuning of the retriever to align pref-\nerences. A typical approach, such as RA-DIT [27], aligns the\nscoring functions between Retriever and Generator using KL\ndivergence.\nV. A UGMENTATION PROCESS IN RAG\nIn the domain of RAG, the standard practice often involves\na singular (once) retrieval step followed by generation, which\ncan lead to inefficiencies and sometimes is typically insuffi-\ncient for complex problems demanding multi-step reasoning,\nas it provides a limited scope of information [105]. Many\nstudies have optimized the retrieval process in response to this\nissue, and we have summarised them in Figure 5.\nA. Iterative Retrieval\nIterative retrieval is a process where the knowledge base\nis repeatedly searched based on the initial query and the text\ngenerated so far, providing a more comprehensive knowledge", "mimetype": "text/plain", "start_char_idx": 3414, "end_char_idx": 5972, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c26a7c8-80cd-4a68-ba33-57259c3d5a38": {"__data__": {"id_": "8c26a7c8-80cd-4a68-ba33-57259c3d5a38", "embedding": null, "metadata": {"page_label": "11", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9bb0cde4-e229-43e8-8579-490e9d192022", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "747f540377dafeb78e136d6bfcd1a8fb71237c51236b106ddb9294d5ddd6c702", "class_name": "RelatedNodeInfo"}}, "text": "11\nFig. 5. In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves\nalternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval\ninvolves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval\nand generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary\nand when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.\nbase for LLMs. This approach has been shown to enhance\nthe robustness of subsequent answer generation by offering\nadditional contextual references through multiple retrieval\niterations. However, it may be affected by semantic discon-\ntinuity and the accumulation of irrelevant information. ITER-\nRETGEN [14] employs a synergistic approach that lever-\nages \u201cretrieval-enhanced generation\u201d alongside \u201cgeneration-\nenhanced retrieval\u201d for tasks that necessitate the reproduction\nof specific information. The model harnesses the content\nrequired to address the input task as a contextual basis for\nretrieving pertinent knowledge, which in turn facilitates the\ngeneration of improved responses in subsequent iterations.\nB. Recursive Retrieval\nRecursive retrieval is often used in information retrieval and\nNLP to improve the depth and relevance of search results.\nThe process involves iteratively refining search queries based\non the results obtained from previous searches. Recursive\nRetrieval aims to enhance the search experience by gradu-\nally converging on the most pertinent information through a\nfeedback loop. IRCoT [61] uses chain-of-thought to guide\nthe retrieval process and refines the CoT with the obtained\nretrieval results. ToC [57] creates a clarification tree that\nsystematically optimizes the ambiguous parts in the Query. It\ncan be particularly useful in complex search scenarios where\nthe user\u2019s needs are not entirely clear from the outset or where\nthe information sought is highly specialized or nuanced. The\nrecursive nature of the process allows for continuous learning\nand adaptation to the user\u2019s requirements, often resulting in\nimproved satisfaction with the search outcomes.\nTo address specific data scenarios, recursive retrieval and\nmulti-hop retrieval techniques are utilized together. Recursive\nretrieval involves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\nC. Adaptive Retrieval\nAdaptive retrieval methods, exemplified by Flare [24] and\nSelf-RAG [25], refine the RAG framework by enabling LLMs\nto actively determine the optimal moments and content for\nretrieval, thus enhancing the efficiency and relevance of the\ninformation sourced.\nThese methods are part of a broader trend wherein\nLLMs employ active judgment in their operations, as seen\nin model agents like AutoGPT, Toolformer, and Graph-\nToolformer [107]\u2013[109]. Graph-Toolformer, for instance, di-\nvides its retrieval process into distinct steps where LLMs\nproactively use retrievers, apply Self-Ask techniques, and em-\nploy few-shot prompts to initiate search queries. This proactive\nstance allows LLMs to decide when to search for necessary\ninformation, akin to how an agent utilizes tools.\nWebGPT [110] integrates a reinforcement learning frame-\nwork to train the GPT-3 model in autonomously using a\nsearch engine during text generation. It navigates this process\nusing special tokens that facilitate actions such as search\nengine queries, browsing results, and citing references, thereby\nexpanding GPT-3\u2019s capabilities through the use of external\nsearch engines. Flare automates timing retrieval by monitoring\nthe confidence of the generation process, as indicated by the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e33e0277-18f8-4e89-8410-012916491129": {"__data__": {"id_": "e33e0277-18f8-4e89-8410-012916491129", "embedding": null, "metadata": {"page_label": "12", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "409bb29e-17e8-420e-833e-9d8b18248de1", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "753458188b2a551025a5b669e65ebf32d379254b1c8600b79c02ff8af01f649d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c5934bf-0e79-4464-a754-fcbc3eae02aa", "node_type": "1", "metadata": {}, "hash": "d8beb2ba9d89ce5d05946ba351ddacf9c9a41529bd49b68c73eaa9dd440616fe", "class_name": "RelatedNodeInfo"}}, "text": "12\nprobability of generated terms [24]. When the probability falls\nbelow a certain threshold would activates the retrieval system\nto collect relevant information, thus optimizing the retrieval\ncycle. Self-RAG [25] introduces \u201creflection tokens\u201d that allow\nthe model to introspect its outputs. These tokens come in\ntwo varieties: \u201cretrieve\u201d and \u201ccritic\u201d. The model autonomously\ndecides when to activate retrieval, or alternatively, a predefined\nthreshold may trigger the process. During retrieval, the gen-\nerator conducts a fragment-level beam search across multiple\nparagraphs to derive the most coherent sequence. Critic scores\nare used to update the subdivision scores, with the flexibility\nto adjust these weights during inference, tailoring the model\u2019s\nbehavior. Self-RAG\u2019s design obviates the need for additional\nclassifiers or reliance on Natural Language Inference (NLI)\nmodels, thus streamlining the decision-making process for\nwhen to engage retrieval mechanisms and improving the\nmodel\u2019s autonomous judgment capabilities in generating ac-\ncurate responses.\nVI. T ASK AND EVALUATION\nThe rapid advancement and growing adoption of RAG\nin the field of NLP have propelled the evaluation of RAG\nmodels to the forefront of research in the LLMs community.\nThe primary objective of this evaluation is to comprehend\nand optimize the performance of RAG models across diverse\napplication scenarios.This chapter will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\nA. Downstream Task\nThe core task of RAG remains Question Answering (QA),\nincluding traditional single-hop/multi-hop QA, multiple-\nchoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Infor-\nmation Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of RAG and their corresponding\ndatasets are summarized in Table II.\nB. Evaluation Target\nHistorically, RAG models assessments have centered on\ntheir execution in specific downstream tasks. These evaluations\nemploy established metrics suitable to the tasks at hand. For\ninstance, question answering evaluations might rely on EM\nand F1 scores [7], [45], [59], [72], whereas fact-checking\ntasks often hinge on Accuracy as the primary metric [4],\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these task-\nspecific metrics [160]. Despite this, there is a notable paucity\nof research dedicated to evaluating the distinct characteristics\nof RAG models.The main evaluation objectives include:\nRetrieval Quality. Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\nof search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\nGeneration Quality . The assessment of generation quality\ncenters on the generator\u2019s capacity to synthesize coherent and\nrelevant answers from the retrieved context. This evaluation\ncan be categorized based on the content\u2019s objectives: unlabeled\nand labeled content. For unlabeled content, the evaluation\nencompasses the faithfulness, relevance, and non-harmfulness\nof the generated answers. In contrast, for labeled content,\nthe focus is on the accuracy of the information produced by\nthe model [161]. Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation methods [29], [161], [163].\nC. Evaluation Aspects\nContemporary evaluation practices of RAG models empha-\nsize three primary quality scores and four essential abilities,\nwhich collectively inform the evaluation of the two principal\ntargets of the RAG model: retrieval and generation.\n1) Quality Scores: Quality scores include context rele-\nvance, answer faithfulness, and answer relevance. These qual-\nity scores evaluate the efficiency of the RAG model from\ndifferent perspectives in the process of information retrieval\nand generation [164]\u2013[166].\nContext Relevance evaluates the precision and specificity\nof the retrieved context, ensuring relevance and minimizing\nprocessing costs associated with extraneous content.\nAnswer Faithfulness ensures that the generated answers\nremain true to the retrieved context, maintaining consistency\nand avoiding contradictions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c5934bf-0e79-4464-a754-fcbc3eae02aa": {"__data__": {"id_": "5c5934bf-0e79-4464-a754-fcbc3eae02aa", "embedding": null, "metadata": {"page_label": "12", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "409bb29e-17e8-420e-833e-9d8b18248de1", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "753458188b2a551025a5b669e65ebf32d379254b1c8600b79c02ff8af01f649d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e33e0277-18f8-4e89-8410-012916491129", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "f41a3bd1f658ac78c3d4b427ae7e08901df912e395d939d233e84fb7b922a130", "class_name": "RelatedNodeInfo"}}, "text": "Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation methods [29], [161], [163].\nC. Evaluation Aspects\nContemporary evaluation practices of RAG models empha-\nsize three primary quality scores and four essential abilities,\nwhich collectively inform the evaluation of the two principal\ntargets of the RAG model: retrieval and generation.\n1) Quality Scores: Quality scores include context rele-\nvance, answer faithfulness, and answer relevance. These qual-\nity scores evaluate the efficiency of the RAG model from\ndifferent perspectives in the process of information retrieval\nand generation [164]\u2013[166].\nContext Relevance evaluates the precision and specificity\nof the retrieved context, ensuring relevance and minimizing\nprocessing costs associated with extraneous content.\nAnswer Faithfulness ensures that the generated answers\nremain true to the retrieved context, maintaining consistency\nand avoiding contradictions.\nAnswer Relevance requires that the generated answers are\ndirectly pertinent to the posed questions, effectively addressing\nthe core inquiry.\n2) Required Abilities: RAG evaluation also encompasses\nfour abilities indicative of its adaptability and efficiency:\nnoise robustness, negative rejection, information integration,\nand counterfactual robustness [167], [168]. These abilities are\ncritical for the model\u2019s performance under various challenges\nand complex scenarios, impacting the quality scores.\nNoise Robustness appraises the model\u2019s capability to man-\nage noise documents that are question-related but lack sub-\nstantive information.\nNegative Rejection assesses the model\u2019s discernment in\nrefraining from responding when the retrieved documents do\nnot contain the necessary knowledge to answer a question.\nInformation Integration evaluates the model\u2019s proficiency in\nsynthesizing information from multiple documents to address\ncomplex questions.\nCounterfactual Robustness tests the model\u2019s ability to rec-\nognize and disregard known inaccuracies within documents,\neven when instructed about potential misinformation.\nContext relevance and noise robustness are important for\nevaluating the quality of retrieval, while answer faithfulness,\nanswer relevance, negative rejection, information integration,\nand counterfactual robustness are important for evaluating the\nquality of generation.", "mimetype": "text/plain", "start_char_idx": 3690, "end_char_idx": 6068, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4656f740-93db-4aa3-9dd4-2be1a42592f7": {"__data__": {"id_": "4656f740-93db-4aa3-9dd4-2be1a42592f7", "embedding": null, "metadata": {"page_label": "13", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a31a05fe-7eca-465b-85da-4c39b0936020", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "38d986503e7cceaabc8d64c55475727ad01d73dcb8079690a23d1a8ba4c3faef", "class_name": "RelatedNodeInfo"}}, "text": "13\nTABLE II\nDOWNSTREAM TASKS AND DATASETS OF RAG\nTask Sub Task Dataset Method\nQA Single-hop Natural Qustion(NQ) [111]\n[26], [30], [34], [42], [45], [50], [52], [59], [64], [82]\n[3], [4], [22], [27], [40], [43], [54], [62], [71], [112]\n[20], [44], [72]\nTriviaQA(TQA) [113]\n[13], [30], [34], [45], [50], [64]\n[4], [27], [59], [62], [112]\n[22], [25], [43], [44], [71], [72]\nSQuAD [114] [20], [23], [30], [32], [45], [69], [112]\nWeb Questions(WebQ) [115] [3], [4], [13], [30], [50], [68]\nPopQA [116] [7], [25], [67]\nMS MARCO [117] [4], [40], [52]\nMulti-hop HotpotQA [118] [23], [26], [31], [34], [47], [51], [61], [82]\n[7], [14], [22], [27], [59], [62], [69], [71], [91]\n2WikiMultiHopQA [119] [14], [24], [48], [59], [61], [91]\nMuSiQue [120] [14], [51], [61], [91]\nLong-form QA ELI5 [121] [27], [34], [43], [49], [51]\nNarrativeQA(NQA) [122] [45], [60], [63], [123]\nASQA [124] [24], [57]\nQMSum(QM) [125] [60], [123]\nDomain QA Qasper [126] [60], [63]\nCOVID-QA [127] [35], [46]\nCMB [128],MMCU Medical [129] [81]\nMulti-Choice QA QuALITY [130] [60], [63]\nARC [131] [25], [67]\nCommonsenseQA [132] [58], [66]\nGraph QA GraphQA [84] [84]\nDialog Dialog Generation Wizard of Wikipedia (WoW) [133] [13], [27], [34], [42]\nPersonal Dialog KBP [134] [74], [135]\nDuleMon [136] [74]\nTask-oriented Dialog CamRest [137] [78], [79]\nRecommendation Amazon(Toys,Sport,Beauty) [138] [39], [40]\nIE Event Argument Extraction WikiEvent [139] [13], [27], [37], [42]\nRAMS [140] [36], [37]\nRelation Extraction T-REx [141],ZsRE [142] [27], [51]\nReasoning Commonsense Reasoning HellaSwag [143] [20], [66]\nCoT Reasoning CoT Reasoning [144] [27]\nComplex Reasoning CSQA [145] [55]\nOthers Language Understanding MMLU [146] [7], [27], [28], [42], [43], [47], [72]\nLanguage Modeling WikiText-103 [147] [5], [29], [64], [71]\nStrategyQA [148] [14], [24], [48], [51], [55], [58]\nFact Checking/Verification FEVER [149] [4], [13], [27], [34], [42], [50]\nPubHealth [150] [25], [67]\nText Generation Biography [151] [67]\nText Summarization WikiASP [152] [24]\nXSum [153] [17]\nText Classification VioLens [154] [19]\nTREC [155] [33]\nSentiment SST-2 [156] [20], [33], [38]\nCode Search CodeSearchNet [157] [76]\nRobustness Evaluation NoMIRACL [56] [56]\nMath GSM8K [158] [73]\nMachine Translation JRC-Acquis [159] [17]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2260, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b18740e-2bfd-4708-9bd9-ae593d739ca0": {"__data__": {"id_": "9b18740e-2bfd-4708-9bd9-ae593d739ca0", "embedding": null, "metadata": {"page_label": "14", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "249f8562-a7a5-40a4-a029-700ee5f1463d", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "edd31adcc456970d9d43ed19036c3cf36a3a239377cfe6b2b029f8d17c3e9a94", "class_name": "RelatedNodeInfo"}}, "text": "14\nTABLE III\nSUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG\nContext\nRelevance Faithfulness Answer\nRelevance\nNoise\nRobustness\nNegative\nRejection\nInformation\nIntegration\nCounterfactual\nRobustness\nAccuracy \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nEM \u2713\nRecall \u2713\nPrecision \u2713 \u2713\nR-Rate \u2713\nCosine Similarity \u2713\nHit Rate \u2713\nMRR \u2713\nNDCG \u2713\nBLEU \u2713 \u2713 \u2713\nROUGE/ROUGE-L \u2713 \u2713 \u2713\nThe specific metrics for each evaluation aspect are sum-\nmarized in Table III. It is essential to recognize that these\nmetrics, derived from related work, are traditional measures\nand do not yet represent a mature or standardized approach for\nquantifying RAG evaluation aspects. Custom metrics tailored\nto the nuances of RAG models, though not included here, have\nalso been developed in some evaluation studies.\nD. Evaluation Benchmarks and Tools\nA series of benchmark tests and tools have been proposed\nto facilitate the evaluation of RAG.These instruments furnish\nquantitative metrics that not only gauge RAG model perfor-\nmance but also enhance comprehension of the model\u2019s capabil-\nities across various evaluation aspects. Prominent benchmarks\nsuch as RGB, RECALL and CRUD [167]\u2013[169] focus on\nappraising the essential abilities of RAG models. Concur-\nrently, state-of-the-art automated tools like RAGAS [164],\nARES [165], and TruLens 8 employ LLMs to adjudicate the\nquality scores. These tools and benchmarks collectively form\na robust framework for the systematic evaluation of RAG\nmodels, as summarized in Table IV.\nVII. D ISCUSSION AND FUTURE PROSPECTS\nDespite the considerable progress in RAG technology, sev-\neral challenges persist that warrant in-depth research.This\nchapter will mainly introduce the current challenges and future\nresearch directions faced by RAG.\nA. RAG vs Long Context\nWith the deepening of related research, the context of LLMs\nis continuously expanding [170]\u2013[172]. Presently, LLMs can\neffortlessly manage contexts exceeding 200,000 tokens 9. This\ncapability signifies that long-document question answering,\npreviously reliant on RAG, can now incorporate the entire\ndocument directly into the prompt. This has also sparked\ndiscussions on whether RAG is still necessary when LLMs\n8https://www.trulens.org/trulens eval/core concepts rag triad/\n9https://kimi.moonshot.cn\nare not constrained by context. In fact, RAG still plays an\nirreplaceable role. On one hand, providing LLMs with a\nlarge amount of context at once will significantly impact its\ninference speed, while chunked retrieval and on-demand input\ncan significantly improve operational efficiency. On the other\nhand, RAG-based generation can quickly locate the original\nreferences for LLMs to help users verify the generated an-\nswers. The entire retrieval and reasoning process is observable,\nwhile generation solely relying on long context remains a\nblack box. Conversely, the expansion of context provides new\nopportunities for the development of RAG, enabling it to\naddress more complex problems and integrative or summary\nquestions that require reading a large amount of material to\nanswer [49]. Developing new RAG methods in the context of\nsuper-long contexts is one of the future research trends.\nB. RAG Robustness\nThe presence of noise or contradictory information during\nretrieval can detrimentally affect RAG\u2019s output quality. This\nsituation is figuratively referred to as \u201cMisinformation can\nbe worse than no information at all\u201d. Improving RAG\u2019s\nresistance to such adversarial or counterfactual inputs is gain-\ning research momentum and has become a key performance\nmetric [48], [50], [82]. Cuconasu et al. [54] analyze which\ntype of documents should be retrieved, evaluate the relevance\nof the documents to the prompt, their position, and the\nnumber included in the context. The research findings reveal\nthat including irrelevant documents can unexpectedly increase\naccuracy by over 30%, contradicting the initial assumption\nof reduced quality. These results underscore the importance\nof developing specialized strategies to integrate retrieval with\nlanguage generation models, highlighting the need for further\nresearch and exploration into the robustness of RAG.\nC. Hybrid Approaches\nCombining RAG with fine-tuning is emerging as a leading\nstrategy. Determining the optimal integration of RAG and\nfine-tuning whether sequential, alternating, or through end-to-\nend joint training\u2014and how to harness both parameterized", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4364, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4471ac0f-b54c-44bb-b02d-2977f49a2741": {"__data__": {"id_": "4471ac0f-b54c-44bb-b02d-2977f49a2741", "embedding": null, "metadata": {"page_label": "15", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d55e4055-e446-4d88-b7cb-b108aab513ed", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "a6023d0c7b2824a61a88497dec9a049798f34cf1c2b6be8509e7a6f6ce1e2194", "class_name": "RelatedNodeInfo"}}, "text": "15\nTABLE IV\nSUMMARY OF EVALUATION FRAMEWORKS\nEvaluation Framework Evaluation Targets Evaluation Aspects Quantitative Metrics\nRGB\u2020 Retrieval Quality\nGeneration Quality\nNoise Robustness\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\nAccuracy\nEM\nAccuracy\nAccuracy\nRECALL\u2020 Generation Quality Counterfactual Robustness R-Rate (Reappearance Rate)\nRAGAS\u2021 Retrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\nCosine Similarity\nARES\u2021 Retrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\nAccuracy\nAccuracy\nAccuracy\nTruLens\u2021 Retrieval Quality\nGeneration Quality\nContext Relevance\nFaithfulness\nAnswer Relevance\n*\n*\n*\nCRUD\u2020 Retrieval Quality\nGeneration Quality\nCreative Generation\nKnowledge-intensive QA\nError Correction\nSummarization\nBLEU\nROUGE-L\nBertScore\nRAGQuestEval\n\u2020 represents a benchmark, and \u2021 represents a tool. * denotes customized quantitative metrics, which deviate from traditional\nmetrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these\nmetrics, as required.\nand non-parameterized advantages are areas ripe for explo-\nration [27]. Another trend is to introduce SLMs with specific\nfunctionalities into RAG and fine-tuned by the results of RAG\nsystem. For example, CRAG [67] trains a lightweight retrieval\nevaluator to assess the overall quality of the retrieved docu-\nments for a query and triggers different knowledge retrieval\nactions based on confidence levels.\nD. Scaling laws of RAG\nEnd-to-end RAG models and pre-trained models based\non RAG are still one of the focuses of current re-\nsearchers [173].The parameters of these models are one of\nthe key factors.While scaling laws [174] are established for\nLLMs, their applicability to RAG remains uncertain. Initial\nstudies like RETRO++ [44] have begun to address this, yet the\nparameter count in RAG models still lags behind that of LLMs.\nThe possibility of an Inverse Scaling Law 10, where smaller\nmodels outperform larger ones, is particularly intriguing and\nmerits further investigation.\nE. Production-Ready RAG\nRAG\u2019s practicality and alignment with engineering require-\nments have facilitated its adoption. However, enhancing re-\ntrieval efficiency, improving document recall in large knowl-\nedge bases, and ensuring data security\u2014such as preventing\n10https://github.com/inverse-scaling/prize\ninadvertent disclosure of document sources or metadata by\nLLMs\u2014are critical engineering challenges that remain to be\naddressed [175].\nThe development of the RAG ecosystem is greatly impacted\nby the progression of its technical stack. Key tools like\nLangChain and LLamaIndex have quickly gained popularity\nwith the emergence of ChatGPT, providing extensive RAG-\nrelated APIs and becoming essential in the realm of LLMs.The\nemerging technology stack, while not as rich in features as\nLangChain and LLamaIndex, stands out through its specialized\nproducts. For example, Flowise AI prioritizes a low-code\napproach, allowing users to deploy AI applications, including\nRAG, through a user-friendly drag-and-drop interface. Other\ntechnologies like HayStack, Meltano, and Cohere Coral are\nalso gaining attention for their unique contributions to the field.\nIn addition to AI-focused vendors, traditional software and\ncloud service providers are expanding their offerings to include\nRAG-centric services. Weaviate\u2019s Verba 11 is designed for\npersonal assistant applications, while Amazon\u2019s Kendra 12\noffers intelligent enterprise search services, enabling users to\nbrowse various content repositories using built-in connectors.\nIn the development of RAG technology, there is a clear\ntrend towards different specialization directions, such as: 1)\nCustomization - tailoring RAG to meet specific requirements.\n2) Simplification - making RAG easier to use to reduce the\n11https://github.com/weaviate/Verba\n12https://aws.amazon.com/cn/kendra/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3939, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "887e64dd-b920-4466-857d-d700093d6ec0": {"__data__": {"id_": "887e64dd-b920-4466-857d-d700093d6ec0", "embedding": null, "metadata": {"page_label": "16", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b61cf34-83e5-4ce7-9f95-da6fd850700d", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "935ce458e5aeb468b174825a6facb6d9c9928c359befb36177dcaf18f0674d43", "class_name": "RelatedNodeInfo"}}, "text": "16\nFig. 6. Summary of RAG ecosystem\ninitial learning curve. 3) Specialization - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\nof RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept is still in the future, requiring\nfurther innovation and development.\nF . Multi-modal RAG\nRAG has transcended its initial text-based question-\nanswering confines, embracing a diverse array of modal data.\nThis expansion has spawned innovative multimodal models\nthat integrate RAG concepts across various domains:\nImage. RA-CM3 [176] stands as a pioneering multimodal\nmodel of both retrieving and generating text and images.\nBLIP-2 [177] leverages frozen image encoders alongside\nLLMs for efficient visual language pre-training, enabling zero-\nshot image-to-text conversions. The \u201cVisualize Before You\nWrite\u201d method [178] employs image generation to steer the\nLM\u2019s text generation, showing promise in open-ended text\ngeneration tasks.\nAudio and Video . The GSS method retrieves and stitches\ntogether audio clips to convert machine-translated data into\nspeech-translated data [179]. UEOP marks a significant ad-\nvancement in end-to-end automatic speech recognition by\nincorporating external, offline strategies for voice-to-text con-\nversion [180]. Additionally, KNN-based attention fusion lever-\nages audio embeddings and semantically related text embed-\ndings to refine ASR, thereby accelerating domain adaptation.\nVid2Seq augments language models with specialized temporal\nmarkers, facilitating the prediction of event boundaries and\ntextual descriptions within a unified output sequence [181].\nCode. RBPS [182] excels in small-scale learning tasks by\nretrieving code examples that align with developers\u2019 objectives\nthrough encoding and frequency analysis. This approach has\ndemonstrated efficacy in tasks such as test assertion genera-\ntion and program repair. For structured knowledge, the CoK\nmethod [106] first extracts facts pertinent to the input query\nfrom a knowledge graph, then integrates these facts as hints\nwithin the input, enhancing performance in knowledge graph\nquestion-answering tasks.\nVIII. C ONCLUSION\nThe summary of this paper, as depicted in Figure 6, empha-\nsizes RAG\u2019s significant advancement in enhancing the capa-\nbilities of LLMs by integrating parameterized knowledge from\nlanguage models with extensive non-parameterized data from\nexternal knowledge bases. The survey showcases the evolution\nof RAG technologies and their application on many different\ntasks. The analysis outlines three developmental paradigms\nwithin the RAG framework: Naive, Advanced, and Modu-\nlar RAG, each representing a progressive enhancement over\nits predecessors. RAG\u2019s technical integration with other AI\nmethodologies, such as fine-tuning and reinforcement learning,\nhas further expanded its capabilities. Despite the progress in\nRAG technology, there are research opportunities to improve\nits robustness and its ability to handle extended contexts.\nRAG\u2019s application scope is expanding into multimodal do-\nmains, adapting its principles to interpret and process diverse\ndata forms like images, videos, and code. This expansion high-\nlights RAG\u2019s significant practical implications for AI deploy-\nment, attracting interest from academic and industrial sectors.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3644, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "009a1bab-df08-4165-8996-45bd55b69613": {"__data__": {"id_": "009a1bab-df08-4165-8996-45bd55b69613", "embedding": null, "metadata": {"page_label": "17", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "718e8467-1957-4cb9-b710-f7a18ce719ca", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "88091653e0371d698e78dfb63bae57c6510826ede6f22835cd78b971e391e80e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fde697f6-1d81-4bd9-8fe8-f767145faa6a", "node_type": "1", "metadata": {}, "hash": "f2c1e02fb5b2aa248c628120c9e6fef2a4049cc0c0ed86fddf75b50abdf65b5f", "class_name": "RelatedNodeInfo"}}, "text": "17\nThe growing ecosystem of RAG is evidenced by the rise in\nRAG-centric AI applications and the continuous development\nof supportive tools. As RAG\u2019s application landscape broadens,\nthere is a need to refine evaluation methodologies to keep\npace with its evolution. Ensuring accurate and representative\nperformance assessments is crucial for fully capturing RAG\u2019s\ncontributions to the AI research and development community.\nREFERENCES\n[1] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, \u201cLarge\nlanguage models struggle to learn long-tail knowledge,\u201d in Interna-\ntional Conference on Machine Learning . PMLR, 2023, pp. 15 696\u2013\n15 707.\n[2] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\nY . Zhang, Y . Chenet al., \u201cSiren\u2019s song in the ai ocean: A survey on hal-\nlucination in large language models,\u201d arXiv preprint arXiv:2309.01219,\n2023.\n[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\nA. Sharma, \u201cGar-meets-rag paradigm for zero-shot information re-\ntrieval,\u201d arXiv preprint arXiv:2310.20158 , 2023.\n[4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\nH. K \u00a8uttler, M. Lewis, W.-t. Yih, T. Rockt \u00a8aschel et al. , \u201cRetrieval-\naugmented generation for knowledge-intensive nlp tasks,\u201d Advances in\nNeural Information Processing Systems, vol. 33, pp. 9459\u20139474, 2020.\n[5] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clarket al.,\n\u201cImproving language models by retrieving from trillions of tokens,\u201d\nin International conference on machine learning . PMLR, 2022, pp.\n2206\u20132240.\n[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al. , \u201cTraining language\nmodels to follow instructions with human feedback,\u201d Advances in\nneural information processing systems , vol. 35, pp. 27 730\u201327 744,\n2022.\n[7] X. Ma, Y . Gong, P. He, H. Zhao, and N. Duan, \u201cQuery rewrit-\ning for retrieval-augmented large language models,\u201d arXiv preprint\narXiv:2305.14283, 2023.\n[8] I. ILIN, \u201cAdvanced rag techniques: an il-\nlustrated overview,\u201d https://pub.towardsai.net/\nadvanced-rag-techniques-an-illustrated-overview-04d193d8fec6,\n2023.\n[9] W. Peng, G. Li, Y . Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al. ,\n\u201cLarge language model based long-tail query rewriting in taobao\nsearch,\u201d arXiv preprint arXiv:2311.03758 , 2023.\n[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V . Le,\nand D. Zhou, \u201cTake a step back: Evoking reasoning via abstraction in\nlarge language models,\u201d arXiv preprint arXiv:2310.06117 , 2023.\n[11] L. Gao, X. Ma, J. Lin, and J. Callan, \u201cPrecise zero-shot dense retrieval\nwithout relevance labels,\u201d arXiv preprint arXiv:2212.10496 , 2022.\n[12] V .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fde697f6-1d81-4bd9-8fe8-f767145faa6a": {"__data__": {"id_": "fde697f6-1d81-4bd9-8fe8-f767145faa6a", "embedding": null, "metadata": {"page_label": "17", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "718e8467-1957-4cb9-b710-f7a18ce719ca", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "88091653e0371d698e78dfb63bae57c6510826ede6f22835cd78b971e391e80e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "009a1bab-df08-4165-8996-45bd55b69613", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "fc9dfd38f42ce5af0e63598945cbec782b35eae79285edabc8c0571d31478d98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0e80c68-1ca3-4022-9f8b-10fd06558589", "node_type": "1", "metadata": {}, "hash": "539d79809c6bfad984357251048058f8ce40fc64e92f851b25a140d0a88450cc", "class_name": "RelatedNodeInfo"}}, "text": "Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al. ,\n\u201cLarge language model based long-tail query rewriting in taobao\nsearch,\u201d arXiv preprint arXiv:2311.03758 , 2023.\n[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V . Le,\nand D. Zhou, \u201cTake a step back: Evoking reasoning via abstraction in\nlarge language models,\u201d arXiv preprint arXiv:2310.06117 , 2023.\n[11] L. Gao, X. Ma, J. Lin, and J. Callan, \u201cPrecise zero-shot dense retrieval\nwithout relevance labels,\u201d arXiv preprint arXiv:2212.10496 , 2022.\n[12] V . Blagojevi, \u201cEnhancing rag pipelines in haystack: Introducing diver-\nsityranker and lostinthemiddleranker,\u201d https://towardsdatascience.com/\nenhancing-rag-pipelines-in-haystack-45f14e2bc9f5, 2023.\n[13] W. Yu, D. Iter, S. Wang, Y . Xu, M. Ju, S. Sanyal, C. Zhu, M. Zeng,\nand M. Jiang, \u201cGenerate rather than retrieve: Large language models\nare strong context generators,\u201d arXiv preprint arXiv:2209.10063, 2022.\n[14] Z. Shao, Y . Gong, Y . Shen, M. Huang, N. Duan, and W. Chen,\n\u201cEnhancing retrieval-augmented large language models with iterative\nretrieval-generation synergy,\u201d arXiv preprint arXiv:2305.15294 , 2023.\n[15] X. Wang, Q. Yang, Y . Qiu, J. Liang, Q. He, Z. Gu, Y . Xiao,\nand W. Wang, \u201cKnowledgpt: Enhancing large language models with\nretrieval and storage access on knowledge bases,\u201d arXiv preprint\narXiv:2308.11761, 2023.\n[16] A. H. Raudaschl, \u201cForget rag, the future\nis rag-fusion,\u201d https://towardsdatascience.com/\nforget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.\n[17] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, \u201cLift\nyourself up: Retrieval-augmented text generation with self memory,\u201d\narXiv preprint arXiv:2305.02437 , 2023.\n[18] S. Wang, Y . Xu, Y . Fang, Y . Liu, S. Sun, R. Xu, C. Zhu, and\nM. Zeng, \u201cTraining data is more valuable than you think: A simple\nand effective method by retrieving from training data,\u201d arXiv preprint\narXiv:2203.08773, 2022.\n[19] X. Li, E. Nie, and S. Liang, \u201cFrom classification to generation:\nInsights into crosslingual retrieval augmented icl,\u201d arXiv preprint\narXiv:2311.06595, 2023.\n[20] D. Cheng, S. Huang, J. Bi, Y . Zhan, J. Liu, Y . Wang, H. Sun,\nF. Wei, D. Deng, and Q. Zhang, \u201cUprise: Universal prompt retrieval\nfor improving zero-shot evaluation,\u201d arXiv preprint arXiv:2303.08518,\n2023.\n[21] Z. Dai, V . Y . Zhao, J. Ma, Y . Luan, J. Ni, J. Lu, A. Bakalov, K. Guu,\nK. B. Hall, and M.-W. Chang, \u201cPromptagator: Few-shot dense retrieval\nfrom 8 examples,\u201d arXiv preprint arXiv:2209.11755 , 2022.\n[22] Z. Sun, X. Wang, Y . Tay, Y .", "mimetype": "text/plain", "start_char_idx": 2244, "end_char_idx": 4762, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0e80c68-1ca3-4022-9f8b-10fd06558589": {"__data__": {"id_": "a0e80c68-1ca3-4022-9f8b-10fd06558589", "embedding": null, "metadata": {"page_label": "17", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "718e8467-1957-4cb9-b710-f7a18ce719ca", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "88091653e0371d698e78dfb63bae57c6510826ede6f22835cd78b971e391e80e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fde697f6-1d81-4bd9-8fe8-f767145faa6a", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "2348c478f3dba292cbbceaa0dab3781b342121f12682ac9ec7a3ea8dbb00f14a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c660e4a-385f-4979-bcee-989ff420df3e", "node_type": "1", "metadata": {}, "hash": "9f6c0d68f23e2fdd50538e63f0021efbb74c25acb57be18a590408d4d1ee385e", "class_name": "RelatedNodeInfo"}}, "text": "[20] D. Cheng, S. Huang, J. Bi, Y . Zhan, J. Liu, Y . Wang, H. Sun,\nF. Wei, D. Deng, and Q. Zhang, \u201cUprise: Universal prompt retrieval\nfor improving zero-shot evaluation,\u201d arXiv preprint arXiv:2303.08518,\n2023.\n[21] Z. Dai, V . Y . Zhao, J. Ma, Y . Luan, J. Ni, J. Lu, A. Bakalov, K. Guu,\nK. B. Hall, and M.-W. Chang, \u201cPromptagator: Few-shot dense retrieval\nfrom 8 examples,\u201d arXiv preprint arXiv:2209.11755 , 2022.\n[22] Z. Sun, X. Wang, Y . Tay, Y . Yang, and D. Zhou, \u201cRecitation-augmented\nlanguage models,\u201d arXiv preprint arXiv:2210.01296 , 2022.\n[23] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts,\nand M. Zaharia, \u201cDemonstrate-search-predict: Composing retrieval\nand language models for knowledge-intensive nlp,\u201d arXiv preprint\narXiv:2212.14024, 2022.\n[24] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,\nJ. Callan, and G. Neubig, \u201cActive retrieval augmented generation,\u201d\narXiv preprint arXiv:2305.06983 , 2023.\n[25] A. Asai, Z. Wu, Y . Wang, A. Sil, and H. Hajishirzi, \u201cSelf-rag:\nLearning to retrieve, generate, and critique through self-reflection,\u201d\narXiv preprint arXiv:2310.11511 , 2023.\n[26] Z. Ke, W. Kong, C. Li, M. Zhang, Q. Mei, and M. Bendersky,\n\u201cBridging the preference gap between retrievers and llms,\u201d arXiv\npreprint arXiv:2401.06954, 2024.\n[27] X. V . Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Ro-\ndriguez, J. Kahn, G. Szilvasy, M. Lewis et al. , \u201cRa-dit: Retrieval-\naugmented dual instruction tuning,\u201d arXiv preprint arXiv:2310.01352 ,\n2023.\n[28] O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, \u201cFine-tuning or\nretrieval? comparing knowledge injection in llms,\u201d arXiv preprint\narXiv:2312.05934, 2023.\n[29] T. Lan, D. Cai, Y . Wang, H. Huang, and X.-L. Mao, \u201cCopy is all\nyou need,\u201d in The Eleventh International Conference on Learning\nRepresentations, 2022.\n[30] T. Chen, H. Wang, S. Chen, W. Yu, K. Ma, X. Zhao, D. Yu, and\nH. Zhang, \u201cDense x retrieval: What retrieval granularity should we\nuse?\u201d arXiv preprint arXiv:2312.06648 , 2023.\n[31] F. Luo and M. Surdeanu, \u201cDivide & conquer for entailment-aware\nmulti-hop evidence retrieval,\u201d arXiv preprint arXiv:2311.02616 , 2023.\n[32] Q. Gou, Z. Xia, B. Yu, H. Yu, F. Huang, Y . Li, and N. Cam-Tu,\n\u201cDiversify question generation with retrieval-augmented style transfer,\u201d\narXiv preprint arXiv:2310.14503 , 2023.\n[33] Z. Guo, S. Cheng, Y . Wang, P. Li, and Y . Liu, \u201cPrompt-guided re-\ntrieval augmentation for non-knowledge-intensive tasks,\u201darXiv preprint\narXiv:2305.17653, 2023.", "mimetype": "text/plain", "start_char_idx": 4312, "end_char_idx": 6806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c660e4a-385f-4979-bcee-989ff420df3e": {"__data__": {"id_": "6c660e4a-385f-4979-bcee-989ff420df3e", "embedding": null, "metadata": {"page_label": "17", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "718e8467-1957-4cb9-b710-f7a18ce719ca", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "88091653e0371d698e78dfb63bae57c6510826ede6f22835cd78b971e391e80e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0e80c68-1ca3-4022-9f8b-10fd06558589", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "17ce452a7d8e92468afdccb948a9a743f873d3a5958c6ce8eb04c59d18c9fd68", "class_name": "RelatedNodeInfo"}}, "text": "[31] F. Luo and M. Surdeanu, \u201cDivide & conquer for entailment-aware\nmulti-hop evidence retrieval,\u201d arXiv preprint arXiv:2311.02616 , 2023.\n[32] Q. Gou, Z. Xia, B. Yu, H. Yu, F. Huang, Y . Li, and N. Cam-Tu,\n\u201cDiversify question generation with retrieval-augmented style transfer,\u201d\narXiv preprint arXiv:2310.14503 , 2023.\n[33] Z. Guo, S. Cheng, Y . Wang, P. Li, and Y . Liu, \u201cPrompt-guided re-\ntrieval augmentation for non-knowledge-intensive tasks,\u201darXiv preprint\narXiv:2305.17653, 2023.\n[34] Z. Wang, J. Araki, Z. Jiang, M. R. Parvez, and G. Neubig, \u201cLearning\nto filter context for retrieval-augmented generation,\u201d arXiv preprint\narXiv:2311.08377, 2023.\n[35] M. Seo, J. Baek, J. Thorne, and S. J. Hwang, \u201cRetrieval-augmented\ndata augmentation for low-resource domain tasks,\u201d arXiv preprint\narXiv:2402.13482, 2024.\n[36] Y . Ma, Y . Cao, Y . Hong, and A. Sun, \u201cLarge language model is not\na good few-shot information extractor, but a good reranker for hard\nsamples!\u201d arXiv preprint arXiv:2303.08559 , 2023.\n[37] X. Du and H. Ji, \u201cRetrieval-augmented generative question answering\nfor event argument extraction,\u201d arXiv preprint arXiv:2211.07067, 2022.\n[38] L. Wang, N. Yang, and F. Wei, \u201cLearning to retrieve in-context\nexamples for large language models,\u201darXiv preprint arXiv:2307.07164,\n2023.\n[39] S. Rajput, N. Mehta, A. Singh, R. H. Keshavan, T. Vu, L. Heldt,\nL. Hong, Y . Tay, V . Q. Tran, J. Samostet al., \u201cRecommender systems\nwith generative retrieval,\u201d arXiv preprint arXiv:2305.05065 , 2023.\n[40] B. Jin, H. Zeng, G. Wang, X. Chen, T. Wei, R. Li, Z. Wang, Z. Li,\nY . Li, H. Lu et al. , \u201cLanguage models as semantic indexers,\u201d arXiv\npreprint arXiv:2310.07815, 2023.\n[41] R. Anantha, T. Bethi, D. V odianik, and S. Chappidi, \u201cContext tuning\nfor retrieval augmented generation,\u201d arXiv preprint arXiv:2312.05708 ,\n2023.\n[42] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, \u201cFew-shot\nlearning with retrieval augmented language models,\u201d arXiv preprint\narXiv:2208.03299, 2022.\n[43] J. Huang, W. Ping, P. Xu, M. Shoeybi, K. C.-C. Chang, and B. Catan-\nzaro, \u201cRaven: In-context learning with retrieval augmented encoder-\ndecoder language models,\u201d arXiv preprint arXiv:2308.07922 , 2023.", "mimetype": "text/plain", "start_char_idx": 6320, "end_char_idx": 8574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b46d7635-7b6b-4ca3-bbed-c1cecb14b876": {"__data__": {"id_": "b46d7635-7b6b-4ca3-bbed-c1cecb14b876", "embedding": null, "metadata": {"page_label": "18", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f84bfaf7-4c6b-48bc-a7f7-8251d9c317fe", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "a58c047177b1e93035d31f9c3ade8287a92f2bf12c5739f68b9f3fd4b82730cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0faf4e5-0f00-43e0-9757-652488968f51", "node_type": "1", "metadata": {}, "hash": "9c54249ae3dc4bb6a65680f97e269a07fb01da1bc2dfd72c5f24f59818ef3c63", "class_name": "RelatedNodeInfo"}}, "text": "18\n[44] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y . Dong,\nO. Kuchaiev, B. Li, C. Xiao et al. , \u201cShall we pretrain autoregressive\nlanguage models with retrieval? a comprehensive study,\u201darXiv preprint\narXiv:2304.06762, 2023.\n[45] B. Wang, W. Ping, L. McAfee, P. Xu, B. Li, M. Shoeybi, and B. Catan-\nzaro, \u201cInstructretro: Instruction tuning post retrieval-augmented pre-\ntraining,\u201d arXiv preprint arXiv:2310.07713 , 2023.\n[46] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana,\nand S. Nanayakkara, \u201cImproving the domain adaptation of retrieval\naugmented generation (rag) models for open domain question answer-\ning,\u201d Transactions of the Association for Computational Linguistics ,\nvol. 11, pp. 1\u201317, 2023.\n[47] Z. Yu, C. Xiong, S. Yu, and Z. Liu, \u201cAugmentation-adapted retriever\nimproves generalization of language models as generic plug-in,\u201d arXiv\npreprint arXiv:2305.17331, 2023.\n[48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, \u201cMaking retrieval-\naugmented language models robust to irrelevant context,\u201d arXiv\npreprint arXiv:2310.01558, 2023.\n[49] H.-T. Chen, F. Xu, S. A. Arora, and E. Choi, \u201cUnderstanding re-\ntrieval augmentation for long-form question answering,\u201d arXiv preprint\narXiv:2310.12150, 2023.\n[50] W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu, \u201cChain-of-note:\nEnhancing robustness in retrieval-augmented language models,\u201d arXiv\npreprint arXiv:2311.09210, 2023.\n[51] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, \u201cSearch-in-the-\nchain: Towards accurate, credible and traceable large language models\nfor knowledgeintensive tasks,\u201d CoRR, vol. abs/2304.14732 , 2023.\n[52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat,\n\u201cOptimizing retrieval-augmented reader models via token elimination,\u201d\narXiv preprint arXiv:2310.13682 , 2023.\n[53] J. L \u00b4ala, O. O\u2019Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques,\nand A. D. White, \u201cPaperqa: Retrieval-augmented generative agent for\nscientific research,\u201d arXiv preprint arXiv:2312.07559 , 2023.\n[54] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\nY . Maarek, N. Tonellotto, and F. Silvestri, \u201cThe power of noise:\nRedefining retrieval for rag systems,\u201d arXiv preprint arXiv:2401.14887,\n2024.\n[55] Z. Zhang, X. Zhang, Y . Ren, S. Shi, M. Han, Y . Wu, R. Lai, and\nZ. Cao, \u201cIag: Induction-augmented generation framework for answer-\ning reasoning questions,\u201d in Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing , 2023, pp. 1\u201314.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0faf4e5-0f00-43e0-9757-652488968f51": {"__data__": {"id_": "a0faf4e5-0f00-43e0-9757-652488968f51", "embedding": null, "metadata": {"page_label": "18", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f84bfaf7-4c6b-48bc-a7f7-8251d9c317fe", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "a58c047177b1e93035d31f9c3ade8287a92f2bf12c5739f68b9f3fd4b82730cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b46d7635-7b6b-4ca3-bbed-c1cecb14b876", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "92f853bacc2b23a7d0f6f4108771edf33b3b29a5c62f29ae79664cc3fb06ab99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "035b4efb-be0e-4e9d-89b2-fd2185c25560", "node_type": "1", "metadata": {}, "hash": "56804900c72da181dc052563646d7c28595882501d4a25c89be741a46add8fda", "class_name": "RelatedNodeInfo"}}, "text": "[54] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\nY . Maarek, N. Tonellotto, and F. Silvestri, \u201cThe power of noise:\nRedefining retrieval for rag systems,\u201d arXiv preprint arXiv:2401.14887,\n2024.\n[55] Z. Zhang, X. Zhang, Y . Ren, S. Shi, M. Han, Y . Wu, R. Lai, and\nZ. Cao, \u201cIag: Induction-augmented generation framework for answer-\ning reasoning questions,\u201d in Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing , 2023, pp. 1\u201314.\n[56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo,\nD. Alfonso-Hermelo, X. Li, Q. Liu, B. Chen, M. Rezagholizadeh et al.,\n\u201cNomiracl: Knowing when you don\u2019t know for robust multilingual\nretrieval-augmented generation,\u201d arXiv preprint arXiv:2312.11361 ,\n2023.\n[57] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, \u201cTree of clarifica-\ntions: Answering ambiguous questions with retrieval-augmented large\nlanguage models,\u201d arXiv preprint arXiv:2310.14696 , 2023.\n[58] Y . Wang, P. Li, M. Sun, and Y . Liu, \u201cSelf-knowledge guided\nretrieval augmentation for large language models,\u201d arXiv preprint\narXiv:2310.05002, 2023.\n[59] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, \u201cRetrieval-\ngeneration synergy augmented large language models,\u201d arXiv preprint\narXiv:2310.05149, 2023.\n[60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, \u201cRetrieval meets long\ncontext large language models,\u201d arXiv preprint arXiv:2310.03025 ,\n2023.\n[61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, \u201cInterleav-\ning retrieval with chain-of-thought reasoning for knowledge-intensive\nmulti-step questions,\u201d arXiv preprint arXiv:2212.10509 , 2022.\n[62] R. Ren, Y . Wang, Y . Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.-\nR. Wen, and H. Wang, \u201cInvestigating the factual knowledge boundary\nof large language models with retrieval augmentation,\u201d arXiv preprint\narXiv:2307.11019, 2023.\n[63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D.\nManning, \u201cRaptor: Recursive abstractive processing for tree-organized\nretrieval,\u201d arXiv preprint arXiv:2401.18059 , 2024.\n[64] O. Ram, Y . Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\nBrown, and Y . Shoham, \u201cIn-context retrieval-augmented language\nmodels,\u201d arXiv preprint arXiv:2302.00083 , 2023.\n[65] Y . Ren, Y . Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, \u201cRetrieve-and-\nsample: Document-level event argument extraction via hybrid retrieval\naugmentation,\u201d in Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\n2023, pp. 293\u2013306.", "mimetype": "text/plain", "start_char_idx": 2006, "end_char_idx": 4602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "035b4efb-be0e-4e9d-89b2-fd2185c25560": {"__data__": {"id_": "035b4efb-be0e-4e9d-89b2-fd2185c25560", "embedding": null, "metadata": {"page_label": "18", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f84bfaf7-4c6b-48bc-a7f7-8251d9c317fe", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "a58c047177b1e93035d31f9c3ade8287a92f2bf12c5739f68b9f3fd4b82730cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0faf4e5-0f00-43e0-9757-652488968f51", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "200ac516fb1ac5b80a07de90070a23f84060d4994135e6e57f2a70792409b1f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea429a41-eada-43d8-85de-173ac3aaa37e", "node_type": "1", "metadata": {}, "hash": "23339cec5a4f08847065162d390197aa4e370819c0bd940a573179663087b362", "class_name": "RelatedNodeInfo"}}, "text": "[64] O. Ram, Y . Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\nBrown, and Y . Shoham, \u201cIn-context retrieval-augmented language\nmodels,\u201d arXiv preprint arXiv:2302.00083 , 2023.\n[65] Y . Ren, Y . Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, \u201cRetrieve-and-\nsample: Document-level event argument extraction via hybrid retrieval\naugmentation,\u201d in Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\n2023, pp. 293\u2013306.\n[66] Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, \u201cZemi: Learning\nzero-shot semi-parametric language models from multiple tasks,\u201d arXiv\npreprint arXiv:2210.00185, 2022.\n[67] S.-Q. Yan, J.-C. Gu, Y . Zhu, and Z.-H. Ling, \u201cCorrective retrieval\naugmented generation,\u201d arXiv preprint arXiv:2401.15884 , 2024.\n[68] P. Jain, L. B. Soares, and T. Kwiatkowski, \u201c1-pager: One pass answer\ngeneration and evidence retrieval,\u201d arXiv preprint arXiv:2310.16568 ,\n2023.\n[69] H. Yang, Z. Li, Y . Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, \u201cPrca:\nFitting black-box large language models for retrieval question answer-\ning via pluggable reward-driven contextual adapter,\u201d arXiv preprint\narXiv:2310.18347, 2023.\n[70] S. Zhuang, B. Liu, B. Koopman, and G. Zuccon, \u201cOpen-source large\nlanguage models are strong zero-shot query likelihood models for\ndocument ranking,\u201d arXiv preprint arXiv:2310.13243 , 2023.\n[71] F. Xu, W. Shi, and E. Choi, \u201cRecomp: Improving retrieval-augmented\nlms with compression and selective augmentation,\u201d arXiv preprint\narXiv:2310.04408, 2023.\n[72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\nmoyer, and W.-t. Yih, \u201cReplug: Retrieval-augmented black-box lan-\nguage models,\u201d arXiv preprint arXiv:2301.12652 , 2023.\n[73] E. Melz, \u201cEnhancing llm intelligence with arm-rag: Auxiliary ra-\ntionale memory for retrieval augmented generation,\u201d arXiv preprint\narXiv:2311.04177, 2023.\n[74] H. Wang, W. Huang, Y . Deng, R. Wang, Z. Wang, Y . Wang, F. Mi,\nJ. Z. Pan, and K.-F. Wong, \u201cUnims-rag: A unified multi-source\nretrieval-augmented generation for personalized dialogue systems,\u201d\narXiv preprint arXiv:2401.13256 , 2024.\n[75] Z. Luo, C. Xu, P. Zhao, X. Geng, C. Tao, J. Ma, Q. Lin, and D. Jiang,\n\u201cAugmented large language models with parametric knowledge guid-\ning,\u201d arXiv preprint arXiv:2305.04757 , 2023.\n[76] X. Li, Z. Liu, C. Xiong, S. Yu, Y . Gu, Z. Liu, and G. Yu, \u201cStructure-\naware language model pretraining improves dense retrieval on struc-\ntured data,\u201d arXiv preprint arXiv:2305.19912 , 2023.\n[77] M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, \u201cKnowledge\ngraph-augmented language models for knowledge-grounded dialogue\ngeneration,\u201d arXiv preprint arXiv:2305.18846 , 2023.\n[78] W. Shen, Y .", "mimetype": "text/plain", "start_char_idx": 4120, "end_char_idx": 6833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea429a41-eada-43d8-85de-173ac3aaa37e": {"__data__": {"id_": "ea429a41-eada-43d8-85de-173ac3aaa37e", "embedding": null, "metadata": {"page_label": "18", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f84bfaf7-4c6b-48bc-a7f7-8251d9c317fe", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "a58c047177b1e93035d31f9c3ade8287a92f2bf12c5739f68b9f3fd4b82730cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "035b4efb-be0e-4e9d-89b2-fd2185c25560", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "8d7c55d7c1990671ed68dc82134c8fffd753c3b342572fad3f3d7e351f8b9e3e", "class_name": "RelatedNodeInfo"}}, "text": "[76] X. Li, Z. Liu, C. Xiong, S. Yu, Y . Gu, Z. Liu, and G. Yu, \u201cStructure-\naware language model pretraining improves dense retrieval on struc-\ntured data,\u201d arXiv preprint arXiv:2305.19912 , 2023.\n[77] M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, \u201cKnowledge\ngraph-augmented language models for knowledge-grounded dialogue\ngeneration,\u201d arXiv preprint arXiv:2305.18846 , 2023.\n[78] W. Shen, Y . Gao, C. Huang, F. Wan, X. Quan, and W. Bi, \u201cRetrieval-\ngeneration alignment for end-to-end task-oriented dialogue system,\u201d\narXiv preprint arXiv:2310.08877 , 2023.\n[79] T. Shi, L. Li, Z. Lin, T. Yang, X. Quan, and Q. Wang, \u201cDual-feedback\nknowledge retrieval for task-oriented dialogue systems,\u201d arXiv preprint\narXiv:2310.14528, 2023.\n[80] P. Ranade and A. Joshi, \u201cFabula: Intelligence report generation\nusing retrieval-augmented narrative construction,\u201d arXiv preprint\narXiv:2310.13848, 2023.\n[81] X. Jiang, R. Zhang, Y . Xu, R. Qiu, Y . Fang, Z. Wang, J. Tang,\nH. Ding, X. Chu, J. Zhao et al. , \u201cThink and retrieval: A hypothesis\nknowledge graph enhanced medical large language models,\u201d arXiv\npreprint arXiv:2312.15883, 2023.\n[82] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,\n\u201cKnowledge-augmented language model verification,\u201d arXiv preprint\narXiv:2310.12836, 2023.\n[83] L. Luo, Y .-F. Li, G. Haffari, and S. Pan, \u201cReasoning on graphs: Faithful\nand interpretable large language model reasoning,\u201d arXiv preprint\narXiv:2310.01061, 2023.\n[84] X. He, Y . Tian, Y . Sun, N. V . Chawla, T. Laurent, Y . LeCun,\nX. Bresson, and B. Hooi, \u201cG-retriever: Retrieval-augmented generation\nfor textual graph understanding and question answering,\u201darXiv preprint\narXiv:2402.07630, 2024.\n[85] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su,\nX. Li, A. Su et al., \u201cTablegpt: Towards unifying tables, nature language\nand commands into one gpt,\u201d arXiv preprint arXiv:2307.08674 , 2023.\n[86] M. Gaur, K. Gunaratna, V . Srinivasan, and H. Jin, \u201cIseeq: Information\nseeking question generation using dynamic meta-information retrieval\nand knowledge graphs,\u201d in Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 36, no. 10, 2022, pp. 10 672\u201310 680.\n[87] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Sch \u00a8arli,\nand D. Zhou, \u201cLarge language models can be easily distracted by\nirrelevant context,\u201d in International Conference on Machine Learning .\nPMLR, 2023, pp. 31 210\u201331 227.\n[88] R. Teja, \u201cEvaluating the ideal chunk size for a rag\nsystem using llamaindex,\u201d https://www.llamaindex.ai/blog/\nevaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\n2023.", "mimetype": "text/plain", "start_char_idx": 6439, "end_char_idx": 9049, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03da3e55-13b3-4f33-8a85-3988c6f51815": {"__data__": {"id_": "03da3e55-13b3-4f33-8a85-3988c6f51815", "embedding": null, "metadata": {"page_label": "19", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "920f1b13-bf7c-4e18-a5ce-3c0b6d700a7f", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "6c23fe5bd08d27ad8aee057e324c1c16686abcfda4dbc82fb064db24c331e598", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfc0400c-2177-4553-b7eb-8ca3caa69b43", "node_type": "1", "metadata": {}, "hash": "a9b86a850f86996739ceda2af71d817069ed0b56d01fdcac44d0ca0a5b760b94", "class_name": "RelatedNodeInfo"}}, "text": "19\n[89] Langchain, \u201cRecursively split by character,\u201d https://python.langchain.\ncom/docs/modules/data connection/document transformers/recursive\ntext splitter, 2023.\n[90] S. Yang, \u201cAdvanced rag 01: Small-to-\nbig retrieval,\u201d https://towardsdatascience.com/\nadvanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.\n[91] Y . Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\n\u201cKnowledge graph prompting for multi-document question answering,\u201d\narXiv preprint arXiv:2308.11730 , 2023.\n[92] D. Zhou, N. Sch \u00a8arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\nurmans, C. Cui, O. Bousquet, Q. Le et al., \u201cLeast-to-most prompting\nenables complex reasoning in large language models,\u201d arXiv preprint\narXiv:2205.10625, 2022.\n[93] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\nand J. Weston, \u201cChain-of-verification reduces hallucination in large\nlanguage models,\u201d arXiv preprint arXiv:2309.11495 , 2023.\n[94] X. Li and J. Li, \u201cAngle-optimized text embeddings,\u201d arXiv preprint\narXiv:2309.12871, 2023.\n[95] V oyageAI, \u201cV oyage\u2019s embedding models,\u201d https://docs.voyageai.com/\nembeddings/, 2023.\n[96] BAAI, \u201cFlagembedding,\u201d https://github.com/FlagOpen/\nFlagEmbedding, 2023.\n[97] P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y . Nie, \u201cRetrieve anything\nto augment large language models,\u201d arXiv preprint arXiv:2310.07554 ,\n2023.\n[98] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni,\nand P. Liang, \u201cLost in the middle: How language models use long\ncontexts,\u201d arXiv preprint arXiv:2307.03172 , 2023.\n[99] Y . Gao, T. Sheng, Y . Xiang, Y . Xiong, H. Wang, and J. Zhang, \u201cChat-\nrec: Towards interactive and explainable llms-augmented recommender\nsystem,\u201d arXiv preprint arXiv:2303.14524 , 2023.\n[100] N. Anderson, C. Wilson, and S. D. Richardson, \u201cLingua: Addressing\nscenarios for live interpretation and automatic dubbing,\u201d inProceedings\nof the 15th Biennial Conference of the Association for Machine\nTranslation in the Americas (Volume 2: Users and Providers Track\nand Government Track) , J. Campbell, S. Larocca, J. Marciano,\nK. Savenkov, and A. Yanishevsky, Eds. Orlando, USA: Association\nfor Machine Translation in the Americas, Sep. 2022, pp. 202\u2013209.\n[Online]. Available: https://aclanthology.org/2022.amta-upg.14\n[101] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y . Lin, Y . Yang, and L. Qiu,\n\u201cLongllmlingua: Accelerating and enhancing llms in long context\nscenarios via prompt compression,\u201d arXiv preprint arXiv:2310.06839 ,\n2023.\n[102] V . Karpukhin, B. O \u02d8guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen,\nand W.-t. Yih, \u201cDense passage retrieval for open-domain question\nanswering,\u201d arXiv preprint arXiv:2004.04906 , 2020.\n[103] Y . Ma, Y . Cao, Y .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2682, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfc0400c-2177-4553-b7eb-8ca3caa69b43": {"__data__": {"id_": "cfc0400c-2177-4553-b7eb-8ca3caa69b43", "embedding": null, "metadata": {"page_label": "19", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "920f1b13-bf7c-4e18-a5ce-3c0b6d700a7f", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "6c23fe5bd08d27ad8aee057e324c1c16686abcfda4dbc82fb064db24c331e598", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03da3e55-13b3-4f33-8a85-3988c6f51815", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "19587d8f5a8cff0f627eddf091d26509446622f0f08b5fdd1197efa4ee29f1d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e54686e1-12f7-444a-9857-9d9b3443b52a", "node_type": "1", "metadata": {}, "hash": "bf8160af68de03aebaa7993cfece481f7c6f6df4c1bebf3ad15ada89c8b11ca6", "class_name": "RelatedNodeInfo"}}, "text": "Available: https://aclanthology.org/2022.amta-upg.14\n[101] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y . Lin, Y . Yang, and L. Qiu,\n\u201cLongllmlingua: Accelerating and enhancing llms in long context\nscenarios via prompt compression,\u201d arXiv preprint arXiv:2310.06839 ,\n2023.\n[102] V . Karpukhin, B. O \u02d8guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen,\nand W.-t. Yih, \u201cDense passage retrieval for open-domain question\nanswering,\u201d arXiv preprint arXiv:2004.04906 , 2020.\n[103] Y . Ma, Y . Cao, Y . Hong, and A. Sun, \u201cLarge language model is\nnot a good few-shot information extractor, but a good reranker for\nhard samples!\u201d ArXiv, vol. abs/2303.08559, 2023. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:257532405\n[104] J. Cui, Z. Li, Y . Yan, B. Chen, and L. Yuan, \u201cChatlaw: Open-source\nlegal large language model with integrated external knowledge bases,\u201d\narXiv preprint arXiv:2306.16092 , 2023.\n[105] O. Yoran, T. Wolfson, O. Ram, and J. Berant, \u201cMaking retrieval-\naugmented language models robust to irrelevant context,\u201d arXiv\npreprint arXiv:2310.01558, 2023.\n[106] X. Li, R. Zhao, Y . K. Chia, B. Ding, L. Bing, S. Joty, and S. Poria,\n\u201cChain of knowledge: A framework for grounding large language mod-\nels with structured knowledge bases,\u201darXiv preprint arXiv:2305.13269,\n2023.\n[107] H. Yang, S. Yue, and Y . He, \u201cAuto-gpt for online decision\nmaking: Benchmarks and additional opinions,\u201d arXiv preprint\narXiv:2306.02224, 2023.\n[108] T. Schick, J. Dwivedi-Yu, R. Dess `\u0131, R. Raileanu, M. Lomeli, L. Zettle-\nmoyer, N. Cancedda, and T. Scialom, \u201cToolformer: Language models\ncan teach themselves to use tools,\u201d arXiv preprint arXiv:2302.04761 ,\n2023.\n[109] J. Zhang, \u201cGraph-toolformer: To empower llms with graph rea-\nsoning ability via prompt augmented by chatgpt,\u201d arXiv preprint\narXiv:2304.11116, 2023.\n[110] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\nC. Hesse, S. Jain, V . Kosaraju, W. Saunders et al., \u201cWebgpt: Browser-\nassisted question-answering with human feedback,\u201d arXiv preprint\narXiv:2112.09332, 2021.\n[111] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., \u201cNatural\nquestions: a benchmark for question answering research,\u201d Transactions\nof the Association for Computational Linguistics , vol. 7, pp. 453\u2013466,\n2019.\n[112] Y . Liu, S. Yavuz, R. Meng, M. Moorthy, S. Joty, C. Xiong, and Y . Zhou,\n\u201cExploring the integration strategies of retriever and large language\nmodels,\u201d arXiv preprint arXiv:2308.12574 , 2023.", "mimetype": "text/plain", "start_char_idx": 2198, "end_char_idx": 4722, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e54686e1-12f7-444a-9857-9d9b3443b52a": {"__data__": {"id_": "e54686e1-12f7-444a-9857-9d9b3443b52a", "embedding": null, "metadata": {"page_label": "19", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "920f1b13-bf7c-4e18-a5ce-3c0b6d700a7f", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "6c23fe5bd08d27ad8aee057e324c1c16686abcfda4dbc82fb064db24c331e598", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfc0400c-2177-4553-b7eb-8ca3caa69b43", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "f0a4ffbc753df4ec593ed95e060dc6c57b722817ec88b34c376ac4381e31d19b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a81adbc-12c6-4168-bb07-352586f3b64f", "node_type": "1", "metadata": {}, "hash": "21522f6089a5c73ad5e74ddacdf6a57a9ac4dca0c63c9397cd91bdaff1efea34", "class_name": "RelatedNodeInfo"}}, "text": "[111] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., \u201cNatural\nquestions: a benchmark for question answering research,\u201d Transactions\nof the Association for Computational Linguistics , vol. 7, pp. 453\u2013466,\n2019.\n[112] Y . Liu, S. Yavuz, R. Meng, M. Moorthy, S. Joty, C. Xiong, and Y . Zhou,\n\u201cExploring the integration strategies of retriever and large language\nmodels,\u201d arXiv preprint arXiv:2308.12574 , 2023.\n[113] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, \u201cTriviaqa: A large\nscale distantly supervised challenge dataset for reading comprehen-\nsion,\u201d arXiv preprint arXiv:1705.03551 , 2017.\n[114] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, \u201cSquad: 100,000+\nquestions for machine comprehension of text,\u201d arXiv preprint\narXiv:1606.05250, 2016.\n[115] J. Berant, A. Chou, R. Frostig, and P. Liang, \u201cSemantic parsing on\nfreebase from question-answer pairs,\u201d in Proceedings of the 2013\nconference on empirical methods in natural language processing, 2013,\npp. 1533\u20131544.\n[116] A. Mallen, A. Asai, V . Zhong, R. Das, H. Hajishirzi, and D. Khashabi,\n\u201cWhen not to trust language models: Investigating effectiveness and\nlimitations of parametric and non-parametric memories,\u201d arXiv preprint\narXiv:2212.10511, 2022.\n[117] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder,\nand L. Deng, \u201cMs marco: A human-generated machine reading com-\nprehension dataset,\u201d 2016.\n[118] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. W. Cohen, R. Salakhutdi-\nnov, and C. D. Manning, \u201cHotpotqa: A dataset for diverse, explain-\nable multi-hop question answering,\u201d arXiv preprint arXiv:1809.09600,\n2018.\n[119] X. Ho, A.-K. D. Nguyen, S. Sugawara, and A. Aizawa, \u201cConstructing a\nmulti-hop qa dataset for comprehensive evaluation of reasoning steps,\u201d\narXiv preprint arXiv:2011.01060 , 2020.\n[120] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, \u201cMusique:\nMultihop questions via single-hop question composition,\u201d Transactions\nof the Association for Computational Linguistics , vol. 10, pp. 539\u2013554,\n2022.\n[121] A. Fan, Y . Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli, \u201cEli5:\nLong form question answering,\u201d arXiv preprint arXiv:1907.09190 ,\n2019.\n[122] T. Ko \u02c7cisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis,\nand E. Grefenstette, \u201cThe narrativeqa reading comprehension chal-\nlenge,\u201d Transactions of the Association for Computational Linguistics ,\nvol. 6, pp. 317\u2013328, 2018.\n[123] K.-H. Lee, X. Chen, H. Furuta, J. Canny, and I. Fischer, \u201cA human-\ninspired reading agent with gist memory of very long contexts,\u201d arXiv\npreprint arXiv:2402.09727, 2024.\n[124] I. Stelmakh, Y . Luan, B. Dhingra, and M.-W.", "mimetype": "text/plain", "start_char_idx": 4232, "end_char_idx": 6942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a81adbc-12c6-4168-bb07-352586f3b64f": {"__data__": {"id_": "6a81adbc-12c6-4168-bb07-352586f3b64f", "embedding": null, "metadata": {"page_label": "19", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "920f1b13-bf7c-4e18-a5ce-3c0b6d700a7f", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "6c23fe5bd08d27ad8aee057e324c1c16686abcfda4dbc82fb064db24c331e598", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e54686e1-12f7-444a-9857-9d9b3443b52a", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "142b7a52d37e7b8edca2902d73124ded3c95bb9b926f7c5e96c762aeb6388d44", "class_name": "RelatedNodeInfo"}}, "text": "[122] T. Ko \u02c7cisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis,\nand E. Grefenstette, \u201cThe narrativeqa reading comprehension chal-\nlenge,\u201d Transactions of the Association for Computational Linguistics ,\nvol. 6, pp. 317\u2013328, 2018.\n[123] K.-H. Lee, X. Chen, H. Furuta, J. Canny, and I. Fischer, \u201cA human-\ninspired reading agent with gist memory of very long contexts,\u201d arXiv\npreprint arXiv:2402.09727, 2024.\n[124] I. Stelmakh, Y . Luan, B. Dhingra, and M.-W. Chang, \u201cAsqa: Factoid\nquestions meet long-form answers,\u201d arXiv preprint arXiv:2204.06092 ,\n2022.\n[125] M. Zhong, D. Yin, T. Yu, A. Zaidi, M. Mutuma, R. Jha, A. H.\nAwadallah, A. Celikyilmaz, Y . Liu, X. Qiu et al. , \u201cQmsum: A new\nbenchmark for query-based multi-domain meeting summarization,\u201d\narXiv preprint arXiv:2104.05938 , 2021.\n[126] P. Dasigi, K. Lo, I. Beltagy, A. Cohan, N. A. Smith, and M. Gardner,\n\u201cA dataset of information-seeking questions and answers anchored in\nresearch papers,\u201d arXiv preprint arXiv:2105.03011 , 2021.\n[127] T. M \u00a8oller, A. Reina, R. Jayakumar, and M. Pietsch, \u201cCovid-qa: A\nquestion answering dataset for covid-19,\u201d in ACL 2020 Workshop on\nNatural Language Processing for COVID-19 (NLP-COVID) , 2020.\n[128] X. Wang, G. H. Chen, D. Song, Z. Zhang, Z. Chen, Q. Xiao, F. Jiang,\nJ. Li, X. Wan, B. Wang et al. , \u201cCmb: A comprehensive medical\nbenchmark in chinese,\u201d arXiv preprint arXiv:2308.08833 , 2023.\n[129] H. Zeng, \u201cMeasuring massive multitask chinese understanding,\u201d arXiv\npreprint arXiv:2304.12986, 2023.\n[130] R. Y . Pang, A. Parrish, N. Joshi, N. Nangia, J. Phang, A. Chen, V . Pad-\nmakumar, J. Ma, J. Thompson, H. He et al. , \u201cQuality: Question an-\nswering with long input texts, yes!\u201d arXiv preprint arXiv:2112.08608 ,\n2021.\n[131] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\nand O. Tafjord, \u201cThink you have solved question answering? try arc,\nthe ai2 reasoning challenge,\u201d arXiv preprint arXiv:1803.05457 , 2018.\n[132] A. Talmor, J. Herzig, N. Lourie, and J. Berant, \u201cCommonsenseqa:\nA question answering challenge targeting commonsense knowledge,\u201d\narXiv preprint arXiv:1811.00937 , 2018.\n[133] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston,\n\u201cWizard of wikipedia: Knowledge-powered conversational agents,\u201d\narXiv preprint arXiv:1811.01241 , 2018.\n[134] H. Wang, M. Hu, Y . Deng, R. Wang, F. Mi, W. Wang, Y . Wang, W.-\nC. Kwan, I. King, and K.-F. Wong, \u201cLarge language models as source", "mimetype": "text/plain", "start_char_idx": 6472, "end_char_idx": 8906, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b539a732-b2fd-463a-b408-e1325681b672": {"__data__": {"id_": "b539a732-b2fd-463a-b408-e1325681b672", "embedding": null, "metadata": {"page_label": "20", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0a69297-355a-4dc3-bb23-e9e52f949efa", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "66f8957d67b5d604acc43be8fe6b7734b626af61c06fd7560fdac05fe032dde7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "845f08f6-836d-4cc9-b44b-3c25a42e54c8", "node_type": "1", "metadata": {}, "hash": "2e8417acc705fd88ecbf58c3beb9c400dc2d443d9aa9b8a7a1420ea9f93d5a0a", "class_name": "RelatedNodeInfo"}}, "text": "20\nplanner for personalized knowledge-grounded dialogue,\u201d arXiv preprint\narXiv:2310.08840, 2023.\n[135] \u2014\u2014, \u201cLarge language models as source planner for personal-\nized knowledge-grounded dialogue,\u201d arXiv preprint arXiv:2310.08840,\n2023.\n[136] X. Xu, Z. Gou, W. Wu, Z.-Y . Niu, H. Wu, H. Wang, and S. Wang,\n\u201cLong time no see! open-domain conversation with long-term persona\nmemory,\u201d arXiv preprint arXiv:2203.05797 , 2022.\n[137] T.-H. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona, P.-H.\nSu, S. Ultes, D. Vandyke, and S. Young, \u201cConditional generation\nand snapshot learning in neural dialogue systems,\u201d arXiv preprint\narXiv:1606.03352, 2016.\n[138] R. He and J. McAuley, \u201cUps and downs: Modeling the visual evolution\nof fashion trends with one-class collaborative filtering,\u201d in proceedings\nof the 25th international conference on world wide web , 2016, pp.\n507\u2013517.\n[139] S. Li, H. Ji, and J. Han, \u201cDocument-level event argument extraction\nby conditional generation,\u201d arXiv preprint arXiv:2104.05919 , 2021.\n[140] S. Ebner, P. Xia, R. Culkin, K. Rawlins, and B. Van Durme, \u201cMulti-\nsentence argument linking,\u201d arXiv preprint arXiv:1911.03766 , 2019.\n[141] H. Elsahar, P. V ougiouklis, A. Remaci, C. Gravier, J. Hare, F. Laforest,\nand E. Simperl, \u201cT-rex: A large scale alignment of natural language\nwith knowledge base triples,\u201d in Proceedings of the Eleventh Inter-\nnational Conference on Language Resources and Evaluation (LREC\n2018), 2018.\n[142] O. Levy, M. Seo, E. Choi, and L. Zettlemoyer, \u201cZero-shot relation ex-\ntraction via reading comprehension,\u201d arXiv preprint arXiv:1706.04115,\n2017.\n[143] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi, \u201cHel-\nlaswag: Can a machine really finish your sentence?\u201d arXiv preprint\narXiv:1905.07830, 2019.\n[144] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, and M. Seo,\n\u201cThe cot collection: Improving zero-shot and few-shot learning of\nlanguage models via chain-of-thought fine-tuning,\u201d arXiv preprint\narXiv:2305.14045, 2023.\n[145] A. Saha, V . Pahuja, M. Khapra, K. Sankaranarayanan, and S. Chandar,\n\u201cComplex sequential question answering: Towards learning to converse\nover linked question answer pairs with a knowledge graph,\u201d inProceed-\nings of the AAAI conference on artificial intelligence , vol. 32, no. 1,\n2018.\n[146] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\nJ. Steinhardt, \u201cMeasuring massive multitask language understanding,\u201d\narXiv preprint arXiv:2009.03300 , 2020.\n[147] S. Merity, C. Xiong, J. Bradbury, and R. Socher, \u201cPointer sentinel\nmixture models,\u201d arXiv preprint arXiv:1609.07843 , 2016.\n[148] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,\n\u201cDid aristotle use a laptop? a question answering benchmark with\nimplicit reasoning strategies,\u201d Transactions of the Association for\nComputational Linguistics, vol. 9, pp. 346\u2013361, 2021.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2845, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "845f08f6-836d-4cc9-b44b-3c25a42e54c8": {"__data__": {"id_": "845f08f6-836d-4cc9-b44b-3c25a42e54c8", "embedding": null, "metadata": {"page_label": "20", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0a69297-355a-4dc3-bb23-e9e52f949efa", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "66f8957d67b5d604acc43be8fe6b7734b626af61c06fd7560fdac05fe032dde7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b539a732-b2fd-463a-b408-e1325681b672", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "2ba1f9f2e061d872551ef85dd2d07fc6ab00d82d1031d03b0ae908ca37a3b063", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b55c0e3-dfd0-4c84-9aa7-5b124898848a", "node_type": "1", "metadata": {}, "hash": "deacf64ba875749962e4405d15d2e0dfb0a53e8ca3d1cc389296ba2528000e5f", "class_name": "RelatedNodeInfo"}}, "text": "[146] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\nJ. Steinhardt, \u201cMeasuring massive multitask language understanding,\u201d\narXiv preprint arXiv:2009.03300 , 2020.\n[147] S. Merity, C. Xiong, J. Bradbury, and R. Socher, \u201cPointer sentinel\nmixture models,\u201d arXiv preprint arXiv:1609.07843 , 2016.\n[148] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,\n\u201cDid aristotle use a laptop? a question answering benchmark with\nimplicit reasoning strategies,\u201d Transactions of the Association for\nComputational Linguistics, vol. 9, pp. 346\u2013361, 2021.\n[149] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, \u201cFever: a\nlarge-scale dataset for fact extraction and verification,\u201d arXiv preprint\narXiv:1803.05355, 2018.\n[150] N. Kotonya and F. Toni, \u201cExplainable automated fact-checking for\npublic health claims,\u201d arXiv preprint arXiv:2010.09926 , 2020.\n[151] R. Lebret, D. Grangier, and M. Auli, \u201cNeural text generation from\nstructured data with application to the biography domain,\u201d arXiv\npreprint arXiv:1603.07771, 2016.\n[152] H. Hayashi, P. Budania, P. Wang, C. Ackerson, R. Neervannan,\nand G. Neubig, \u201cWikiasp: A dataset for multi-domain aspect-based\nsummarization,\u201d Transactions of the Association for Computational\nLinguistics, vol. 9, pp. 211\u2013225, 2021.\n[153] S. Narayan, S. B. Cohen, and M. Lapata, \u201cDon\u2019t give me the details,\njust the summary! topic-aware convolutional neural networks for ex-\ntreme summarization,\u201d arXiv preprint arXiv:1808.08745 , 2018.\n[154] S. Saha, J. A. Junaed, M. Saleki, A. S. Sharma, M. R. Rifat, M. Rahouti,\nS. I. Ahmed, N. Mohammed, and M. R. Amin, \u201cVio-lens: A novel\ndataset of annotated social network posts leading to different forms\nof communal violence and its evaluation,\u201d in Proceedings of the First\nWorkshop on Bangla Language Processing (BLP-2023), 2023, pp. 72\u2013\n84.\n[155] X. Li and D. Roth, \u201cLearning question classifiers,\u201d in COLING 2002:\nThe 19th International Conference on Computational Linguistics, 2002.\n[156] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y . Ng,\nand C. Potts, \u201cRecursive deep models for semantic compositionality\nover a sentiment treebank,\u201d in Proceedings of the 2013 conference on\nempirical methods in natural language processing , 2013, pp. 1631\u2013\n1642.\n[157] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\n\u201cCodesearchnet challenge: Evaluating the state of semantic code\nsearch,\u201d arXiv preprint arXiv:1909.09436 , 2019.\n[158] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\nM. Plappert, J. Tworek, J. Hilton, R. Nakano et al., \u201cTraining verifiers\nto solve math word problems,\u201d arXiv preprint arXiv:2110.14168, 2021.", "mimetype": "text/plain", "start_char_idx": 2275, "end_char_idx": 4940, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b55c0e3-dfd0-4c84-9aa7-5b124898848a": {"__data__": {"id_": "2b55c0e3-dfd0-4c84-9aa7-5b124898848a", "embedding": null, "metadata": {"page_label": "20", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0a69297-355a-4dc3-bb23-e9e52f949efa", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "66f8957d67b5d604acc43be8fe6b7734b626af61c06fd7560fdac05fe032dde7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "845f08f6-836d-4cc9-b44b-3c25a42e54c8", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "fbc15afb32fa6966b51532dce3c142ed5176bf34376c2c331c06a76a387a21e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "450eb635-f69a-4acf-8167-9e9f9936ae29", "node_type": "1", "metadata": {}, "hash": "61452fa24676b1912ff77e8d96bd07009b58661551e100b699acc70b6a44a60a", "class_name": "RelatedNodeInfo"}}, "text": "1631\u2013\n1642.\n[157] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\n\u201cCodesearchnet challenge: Evaluating the state of semantic code\nsearch,\u201d arXiv preprint arXiv:1909.09436 , 2019.\n[158] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\nM. Plappert, J. Tworek, J. Hilton, R. Nakano et al., \u201cTraining verifiers\nto solve math word problems,\u201d arXiv preprint arXiv:2110.14168, 2021.\n[159] R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Erjavec, D. Tufis,\nand D. Varga, \u201cThe jrc-acquis: A multilingual aligned parallel corpus\nwith 20+ languages,\u201d arXiv preprint cs/0609058 , 2006.\n[160] Y . Hoshi, D. Miyashita, Y . Ng, K. Tatsuno, Y . Morioka, O. Torii,\nand J. Deguchi, \u201cRalle: A framework for developing and eval-\nuating retrieval-augmented large language models,\u201d arXiv preprint\narXiv:2308.10633, 2023.\n[161] J. Liu, \u201cBuilding production-ready rag applications,\u201d https://www.ai.\nengineer/summit/schedule/building-production-ready-rag-applications,\n2023.\n[162] I. Nguyen, \u201cEvaluating rag part i: How to evaluate document retrieval,\u201d\nhttps://www.deepset.ai/blog/rag-evaluation-retrieval, 2023.\n[163] Q. Leng, K. Uhlenhuth, and A. Polyzotis, \u201cBest practices for\nllm evaluation of rag applications,\u201d https://www.databricks.com/blog/\nLLM-auto-eval-best-practices-RAG, 2023.\n[164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, \u201cRagas: Au-\ntomated evaluation of retrieval augmented generation,\u201d arXiv preprint\narXiv:2309.15217, 2023.\n[165] J. Saad-Falcon, O. Khattab, C. Potts, and M. Zaharia, \u201cAres: An\nautomated evaluation framework for retrieval-augmented generation\nsystems,\u201d arXiv preprint arXiv:2311.09476 , 2023.\n[166] C. Jarvis and J. Allard, \u201cA survey of techniques for\nmaximizing llm performance,\u201d https://community.openai.\ncom/t/openai-dev-day-2023-breakout-sessions/505213#\na-survey-of-techniques-for-maximizing-llm-performance-2, 2023.\n[167] J. Chen, H. Lin, X. Han, and L. Sun, \u201cBenchmarking large lan-\nguage models in retrieval-augmented generation,\u201d arXiv preprint\narXiv:2309.01431, 2023.\n[168] Y . Liu, L. Huang, S. Li, S. Chen, H. Zhou, F. Meng, J. Zhou, and\nX. Sun, \u201cRecall: A benchmark for llms robustness against external\ncounterfactual knowledge,\u201d arXiv preprint arXiv:2311.08147 , 2023.\n[169] Y . Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\nT. Xu, and E. Chen, \u201cCrud-rag: A comprehensive chinese benchmark\nfor retrieval-augmented generation of large language models,\u201d arXiv\npreprint arXiv:2401.17043, 2024.\n[170] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, \u201cRetrieval meets long\ncontext large language models,\u201d arXiv preprint arXiv:2310.03025 ,\n2023.\n[171] C. Packer, V .", "mimetype": "text/plain", "start_char_idx": 4531, "end_char_idx": 7249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "450eb635-f69a-4acf-8167-9e9f9936ae29": {"__data__": {"id_": "450eb635-f69a-4acf-8167-9e9f9936ae29", "embedding": null, "metadata": {"page_label": "20", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0a69297-355a-4dc3-bb23-e9e52f949efa", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "66f8957d67b5d604acc43be8fe6b7734b626af61c06fd7560fdac05fe032dde7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b55c0e3-dfd0-4c84-9aa7-5b124898848a", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "6c50d3462026567a5cb31c78fc70c7f6e0409f170791fc7e6153ecf64c9cec8b", "class_name": "RelatedNodeInfo"}}, "text": "[169] Y . Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\nT. Xu, and E. Chen, \u201cCrud-rag: A comprehensive chinese benchmark\nfor retrieval-augmented generation of large language models,\u201d arXiv\npreprint arXiv:2401.17043, 2024.\n[170] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, \u201cRetrieval meets long\ncontext large language models,\u201d arXiv preprint arXiv:2310.03025 ,\n2023.\n[171] C. Packer, V . Fang, S. G. Patil, K. Lin, S. Wooders, and J. E. Gon-\nzalez, \u201cMemgpt: Towards llms as operating systems,\u201d arXiv preprint\narXiv:2310.08560, 2023.\n[172] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis, \u201cEfficient\nstreaming language models with attention sinks,\u201d arXiv preprint\narXiv:2309.17453, 2023.\n[173] T. Zhang, S. G. Patil, N. Jain, S. Shen, M. Zaharia, I. Stoica, and J. E.\nGonzalez, \u201cRaft: Adapting language model to domain specific rag,\u201d\narXiv preprint arXiv:2403.10131 , 2024.\n[174] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, \u201cScaling laws\nfor neural language models,\u201d arXiv preprint arXiv:2001.08361 , 2020.\n[175] U. Alon, F. Xu, J. He, S. Sengupta, D. Roth, and G. Neubig, \u201cNeuro-\nsymbolic language modeling with automaton-augmented retrieval,\u201d in\nInternational Conference on Machine Learning . PMLR, 2022, pp.\n468\u2013485.\n[176] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang,\nM. Lewis, L. Zettlemoyer, and W.-t. Yih, \u201cRetrieval-augmented multi-\nmodal language modeling,\u201d arXiv preprint arXiv:2211.12561 , 2022.\n[177] J. Li, D. Li, S. Savarese, and S. Hoi, \u201cBlip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language\nmodels,\u201d arXiv preprint arXiv:2301.12597 , 2023.\n[178] W. Zhu, A. Yan, Y . Lu, W. Xu, X. E. Wang, M. Eckstein, and W. Y .\nWang, \u201cVisualize before you write: Imagination-guided open-ended\ntext generation,\u201d arXiv preprint arXiv:2210.03765 , 2022.\n[179] J. Zhao, G. Haffar, and E. Shareghi, \u201cGenerating synthetic speech from\nspokenvocab for speech translation,\u201d arXiv preprint arXiv:2210.08174,\n2022.\n[180] D. M. Chan, S. Ghosh, A. Rastrow, and B. Hoffmeister, \u201cUsing external\noff-policy speech-to-text mappings in contextual end-to-end automated\nspeech recognition,\u201d arXiv preprint arXiv:2301.02736 , 2023.", "mimetype": "text/plain", "start_char_idx": 6779, "end_char_idx": 9110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbe23339-0b40-4b8f-8e44-88c2eae299fb": {"__data__": {"id_": "bbe23339-0b40-4b8f-8e44-88c2eae299fb", "embedding": null, "metadata": {"page_label": "21", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "31cc409f-9256-431c-8121-9c5756707107", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}, "hash": "2c2846f973c4ee76224388f1e0d2f648e9dc68aefba15597ba5645678e9bcacc", "class_name": "RelatedNodeInfo"}}, "text": "21\n[181] A. Yang, A. Nagrani, P. H. Seo, A. Miech, J. Pont-Tuset, I. Laptev,\nJ. Sivic, and C. Schmid, \u201cVid2seq: Large-scale pretraining of a visual\nlanguage model for dense video captioning,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2023, pp. 10 714\u201310 726.\n[182] N. Nashid, M. Sintaha, and A. Mesbah, \u201cRetrieval-based prompt\nselection for code-related few-shot learning,\u201d in 2023 IEEE/ACM 45th\nInternational Conference on Software Engineering (ICSE) , 2023, pp.\n2450\u20132462.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"3300603e-6396-4503-ace0-497dcd8029cf": {"node_ids": ["43cbcae8-6cc7-4c56-b367-e7d073c73e2e"], "metadata": {"page_label": "1", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "0cd449d0-3faf-43e2-8997-77a60dd8bc6e": {"node_ids": ["3b2d001f-00fc-4be0-914f-6d363e110b28", "852669ea-9eaa-4666-959f-feeeef0043d4"], "metadata": {"page_label": "2", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "36163862-d97e-44e2-9afa-b6588cc4e49b": {"node_ids": ["dd63402e-3d67-4c55-a6ca-534e31dceb92"], "metadata": {"page_label": "3", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "1f45d831-0530-4252-970d-3f3cf0636715": {"node_ids": ["2d3ef193-bd71-4d92-a043-52f2543179e2", "1d037085-21aa-40c4-9af7-09e68658c56c"], "metadata": {"page_label": "4", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "d3c86b19-ae2a-4cbf-b2dc-cbc196285c11": {"node_ids": ["a730939c-ce27-4c35-9d7c-eb55921c6671", "09763838-24c2-4eb1-9e1b-4eac3aff565e"], "metadata": {"page_label": "5", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "abd720f7-e7c7-4f0c-90b0-55c5c600a64a": {"node_ids": ["b5b522c3-0b03-41d8-9c58-af812f9d2cc9", "d378120f-4074-4020-8adf-dd9cb539b3a0"], "metadata": {"page_label": "6", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "c1aaab2c-7d44-497e-8cc6-6ec01aaf2b67": {"node_ids": ["02110f77-ff94-4534-b3dd-7c4c7c0976c2", "361afec3-2d22-46ef-8e45-b692f342f90e"], "metadata": {"page_label": "7", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "46b2ca3a-e9c9-4443-9a1e-bdb5ebf5cfa9": {"node_ids": ["72064272-5b7a-4547-b7da-9b1b02c4f638", "14a36df0-a23a-4bff-8d9a-bec8352fd24c"], "metadata": {"page_label": "8", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "3fe1691c-6cb0-40c8-961d-e25dfc04a1fe": {"node_ids": ["32e367c7-d4d9-42cf-af1e-fcc2ef7a4916"], "metadata": {"page_label": "9", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "6b01cfc6-6781-42a8-98b0-3bc0ae981d95": {"node_ids": ["213385ad-7f81-43e4-b831-83b0a542143b", "11d20e68-07f8-49ae-b5ae-4afa2c177498"], "metadata": {"page_label": "10", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "702dbea1-3fd7-4e63-98f3-514d0596c2bb": {"node_ids": ["c57e6931-2394-43c7-859b-47395aada179", "c9630248-3990-4524-94d8-92e12325c13e"], "metadata": {"page_label": "11", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "12ac5c52-43db-47fd-8c86-b6acc1d7b19b": {"node_ids": ["2b53c0a7-6c0f-48e9-be31-f72eb7bac96c", "562dd12c-08d7-42d4-8352-04ce8ff6bc35"], "metadata": {"page_label": "12", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "15cb1b71-56a6-4191-94ca-b2417fc78085": {"node_ids": ["696bf2ab-3eb5-4680-8d43-d16c9c40bf49", "47a6a4a1-753d-4588-bbde-b69906a25140"], "metadata": {"page_label": "13", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "f7bba5bf-f672-4398-8849-16ba4dd084fc": {"node_ids": ["9209a8d9-7f9c-442b-965a-3938a956c95c", "9c8061f2-9b95-48b4-a614-19d60f880b64"], "metadata": {"page_label": "14", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "04947ad0-6e74-4888-ad68-561644f6ce91": {"node_ids": ["2231e565-3d42-44d2-a8ba-8449cf61ba30", "8d251e7a-b243-4039-94b0-d52417ddba74"], "metadata": {"page_label": "15", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "199c7629-974f-40ae-94a7-b516538f02cd": {"node_ids": ["cc4926e3-c0f9-4ca9-8b15-a3bbef9e36d9"], "metadata": {"page_label": "16", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "48769f2d-0bc5-4887-9019-0f89da8f3a15": {"node_ids": ["73e97549-8258-4c51-91ee-03c795f38ad7"], "metadata": {"page_label": "17", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "e5c821c4-f03f-40b6-8875-c89eeac79c24": {"node_ids": ["c5ae9077-55ab-40aa-a6c1-70da3f751a61"], "metadata": {"page_label": "18", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "072d472c-0b9e-4c76-9399-7b1237cdf258": {"node_ids": ["f2f4b5bb-0be8-4794-b3f9-8054454b1858"], "metadata": {"page_label": "19", "file_name": "2005.11401v4.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2005.11401v4.pdf", "file_type": "application/pdf", "file_size": 885323, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "50278bb3-b938-4698-a028-5b396c83bcdb": {"node_ids": ["6c75945b-edeb-4716-8c12-3b75e62b5cc0"], "metadata": {"page_label": "1", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "2f6b3f08-cf99-44da-9369-338a6365b30b": {"node_ids": ["65977908-800a-4f24-9295-9fb50c47da63", "cb6ac9e0-df2d-43d3-8098-51bccd70affc"], "metadata": {"page_label": "2", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "31f060e9-ff3a-48b7-83c9-724b39d49613": {"node_ids": ["c303ac3a-0f9e-47fc-9ee3-c304a1211b67", "5e77d204-2d3e-469f-a933-4798c6863343"], "metadata": {"page_label": "3", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "de25f2b7-b7a3-4cf1-80ba-053408f47823": {"node_ids": ["2b732bcb-5b93-4522-9b26-7501c5fe7f31"], "metadata": {"page_label": "4", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "66f4c94e-ebda-460b-b057-e2956e0cc581": {"node_ids": ["38c7dee0-b302-4558-b0a3-458f6dddfdcb", "4b3dcc69-3fcf-433b-b378-86cb258bb995"], "metadata": {"page_label": "5", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "1ed27771-4fba-4efb-ba51-eaf25a346d2f": {"node_ids": ["e7843b5a-427f-4d2d-9a26-804a34e7d2d6", "e833b503-49b1-4528-8f30-bc99b6eaf50e"], "metadata": {"page_label": "6", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "211288c5-2a13-4149-b0db-ddb5f946ff17": {"node_ids": ["a8c5a05e-dfd3-404f-80fe-f1ad7052bf23", "8a430c60-6f3d-4fdc-b214-c8575cbc2516"], "metadata": {"page_label": "7", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "8d726df1-939d-456c-99cc-0ff9bd6fa13f": {"node_ids": ["c66aa112-8b1b-44d8-a2df-9b48ee326fa8", "b8f2cd42-0ed7-4539-af00-e815bf902a5a"], "metadata": {"page_label": "8", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "e30dacd8-a1b3-4a64-bdf3-07ac52a35a25": {"node_ids": ["e780335a-b086-448b-bfd6-c1234b599d90", "74bf5371-f584-4934-a790-ea07bcfd790e"], "metadata": {"page_label": "9", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "7aab6e5a-8007-4760-aa2d-c59bdb1d7cdf": {"node_ids": ["685d07e4-b022-4a17-a856-7717d4daf536", "1d3cecd9-7eb6-4659-854a-a5024d6fc392"], "metadata": {"page_label": "10", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "eb6d4eb8-f779-4455-ab83-71f04ad97cc4": {"node_ids": ["cc72ac2f-18f0-4268-8e58-dcb42dc5214e", "7db51537-9b7c-4734-a6a1-f79558b98f44"], "metadata": {"page_label": "11", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "2ff2ec0f-515d-4cf3-a152-beb3dc3f5428": {"node_ids": ["c605b43d-3828-46d1-82be-b1228d343cd3", "8c2e0021-e472-4709-8c41-fde361665cc2"], "metadata": {"page_label": "12", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "fd7cb11f-69ff-4a19-bb3d-6eb2a9d7d6ca": {"node_ids": ["219c7d5c-8799-4a31-bf1b-4b92afd5b816", "49c016ca-6098-4fb8-a072-8ab16dd6c5de"], "metadata": {"page_label": "13", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "8c735fee-441e-4c0c-b648-3f8927f54a84": {"node_ids": ["8f83b4b3-47f1-4757-8858-f93f28b838ea", "de8ccd1b-e2d3-490a-9d8a-db24a3092b7d"], "metadata": {"page_label": "14", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "45814271-f6ec-4320-b1e3-0ab05b675675": {"node_ids": ["d3f8c1af-7e9a-4b67-9c7b-9246b2116036"], "metadata": {"page_label": "15", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "c3fc50ad-4f2d-4c95-ae46-5044eb9ce075": {"node_ids": ["2dc58b05-c1c5-4e89-91c1-e72f86e3aa85"], "metadata": {"page_label": "16", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "6c02fa2c-5fa8-4429-b452-20051d98420a": {"node_ids": ["4655e0f3-39c6-4dd4-96d8-353eebc675fa"], "metadata": {"page_label": "17", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "fa132b5f-9ded-444a-b92a-c1fd59ea8883": {"node_ids": ["57523290-bf7c-4d9d-a849-c81ff114c2e3"], "metadata": {"page_label": "18", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "92cbcd51-6bba-470b-9f79-b1f11f89181d": {"node_ids": ["984215be-3ca3-4b90-8c25-b9e4c5b5c2a5"], "metadata": {"page_label": "19", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "4fa2b42e-4e5a-4f5a-8421-38540b1c6c1b": {"node_ids": ["077ecf39-393c-46f8-9ac6-384df21fdf5f", "a33f50d1-5428-413a-8894-75726f2ab16f"], "metadata": {"page_label": "20", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "95d855cc-70e1-4f45-a1ca-bf68dc39041b": {"node_ids": ["7ee9492a-7a04-41fb-bafa-b6d612cc8650"], "metadata": {"page_label": "21", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "379ee6e7-e1e5-4cc1-b19d-b6ba9ee3474d": {"node_ids": ["599bcf1e-c86f-4247-8db2-2d4fa38ab8fc", "1998f6c9-cfec-43dc-b82a-7ba1e5207fae"], "metadata": {"page_label": "22", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "0e469dfe-ab9a-44e9-8705-5c55f9815440": {"node_ids": ["e0617267-0559-4be0-b685-01ede02a1288"], "metadata": {"page_label": "23", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "0531013c-7af7-43f3-80c9-108a054e3641": {"node_ids": ["f291389e-20c8-4879-a191-16a10346f8e1", "e97960c3-17be-4375-a3c9-51895b8a47aa"], "metadata": {"page_label": "24", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "f39d25a6-d2f8-41da-a57e-5675bf769a17": {"node_ids": ["27c68578-8d05-4f09-9091-d4dd1d868f56"], "metadata": {"page_label": "25", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "35f12626-eb91-4636-9d1e-b11150a477a1": {"node_ids": ["90a63ced-3824-4f8b-900c-5962cb1b1d6e"], "metadata": {"page_label": "26", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "fb9635b4-1982-4ae3-b466-0d05e732bb67": {"node_ids": ["f26be59c-50a6-4d1b-a6e9-b4615d093fc3"], "metadata": {"page_label": "27", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "5b7b49de-1f13-4737-a47c-32ac293626f9": {"node_ids": ["65c368e9-5405-4029-b69e-aca2923db5d2"], "metadata": {"page_label": "28", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "cd42df60-847b-4bde-b4cc-5306dfde5f1b": {"node_ids": ["d8c75822-2004-4b11-9fc2-58fad2da8cce"], "metadata": {"page_label": "29", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "725f8c93-52b6-48e3-86a2-8c30d1c419b0": {"node_ids": ["594568b9-6160-41fe-8d4b-1c02fb040bd9"], "metadata": {"page_label": "30", "file_name": "2310.11511v1.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2310.11511v1.pdf", "file_type": "application/pdf", "file_size": 1405127, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "c7c28efb-12e7-47e1-a7fe-7b2fab2f2107": {"node_ids": ["25990a46-6ebb-4a08-831d-62367de64ccd", "ff2396d2-24ab-4457-b9cd-99b63f9aa420"], "metadata": {"page_label": "1", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "6d9917be-8db3-4511-b957-1bcc2b4be680": {"node_ids": ["3e982401-b33f-4181-90b7-10b349e28e39"], "metadata": {"page_label": "2", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "c844d012-1072-4175-91e7-79f4de4f077a": {"node_ids": ["1a2c8590-90ff-4ca1-915a-45629f0eeaf2"], "metadata": {"page_label": "3", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "b851bf06-f4c9-4140-94b6-dd2b06d017f7": {"node_ids": ["6e22e3fe-324e-4026-aead-1dee8f8aab59"], "metadata": {"page_label": "4", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "53102d6c-a640-4c4f-baad-a18dbcf4efc7": {"node_ids": ["284e0294-fcd7-492b-9a69-a2136ca1ed9e", "8964255e-b611-434d-95bb-441c4aca752e"], "metadata": {"page_label": "5", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "3d542561-e275-4984-9121-7a6ad0e6bf52": {"node_ids": ["d62e0cf2-f0eb-4f5b-ac56-124b06b0a874", "b212b645-338d-4d26-a9e6-3df4f47a738b"], "metadata": {"page_label": "6", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "b54e3c4b-2163-4710-8425-e7ea55298923": {"node_ids": ["33307481-bd83-4f83-899b-72fefe367f1d"], "metadata": {"page_label": "7", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "e5dd65d2-5eda-4373-a9f5-258e655492bf": {"node_ids": ["54257664-c5a5-42d6-86d0-67b5aeef3c71", "03dcabff-c43a-4000-ae85-f2e5bed6d276"], "metadata": {"page_label": "8", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "1bf610f3-69fc-4de9-ae7c-b738a8f645da": {"node_ids": ["fd9b2d05-26a2-4463-999d-9740e106b536", "d9df8ad8-ef33-47cb-ae97-8d1696759811"], "metadata": {"page_label": "9", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "bde80ebd-a216-4211-afd9-b0a403478f83": {"node_ids": ["737463ba-c93f-4fdd-93fc-eeaadf7111d3", "1e091581-56aa-449a-ab80-da628374840e"], "metadata": {"page_label": "10", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "9bb0cde4-e229-43e8-8579-490e9d192022": {"node_ids": ["8c26a7c8-80cd-4a68-ba33-57259c3d5a38"], "metadata": {"page_label": "11", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "409bb29e-17e8-420e-833e-9d8b18248de1": {"node_ids": ["e33e0277-18f8-4e89-8410-012916491129", "5c5934bf-0e79-4464-a754-fcbc3eae02aa"], "metadata": {"page_label": "12", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "a31a05fe-7eca-465b-85da-4c39b0936020": {"node_ids": ["4656f740-93db-4aa3-9dd4-2be1a42592f7"], "metadata": {"page_label": "13", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "249f8562-a7a5-40a4-a029-700ee5f1463d": {"node_ids": ["9b18740e-2bfd-4708-9bd9-ae593d739ca0"], "metadata": {"page_label": "14", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "d55e4055-e446-4d88-b7cb-b108aab513ed": {"node_ids": ["4471ac0f-b54c-44bb-b02d-2977f49a2741"], "metadata": {"page_label": "15", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "0b61cf34-83e5-4ce7-9f95-da6fd850700d": {"node_ids": ["887e64dd-b920-4466-857d-d700093d6ec0"], "metadata": {"page_label": "16", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "718e8467-1957-4cb9-b710-f7a18ce719ca": {"node_ids": ["009a1bab-df08-4165-8996-45bd55b69613", "fde697f6-1d81-4bd9-8fe8-f767145faa6a", "a0e80c68-1ca3-4022-9f8b-10fd06558589", "6c660e4a-385f-4979-bcee-989ff420df3e"], "metadata": {"page_label": "17", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "f84bfaf7-4c6b-48bc-a7f7-8251d9c317fe": {"node_ids": ["b46d7635-7b6b-4ca3-bbed-c1cecb14b876", "a0faf4e5-0f00-43e0-9757-652488968f51", "035b4efb-be0e-4e9d-89b2-fd2185c25560", "ea429a41-eada-43d8-85de-173ac3aaa37e"], "metadata": {"page_label": "18", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "920f1b13-bf7c-4e18-a5ce-3c0b6d700a7f": {"node_ids": ["03da3e55-13b3-4f33-8a85-3988c6f51815", "cfc0400c-2177-4553-b7eb-8ca3caa69b43", "e54686e1-12f7-444a-9857-9d9b3443b52a", "6a81adbc-12c6-4168-bb07-352586f3b64f"], "metadata": {"page_label": "19", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "a0a69297-355a-4dc3-bb23-e9e52f949efa": {"node_ids": ["b539a732-b2fd-463a-b408-e1325681b672", "845f08f6-836d-4cc9-b44b-3c25a42e54c8", "2b55c0e3-dfd0-4c84-9aa7-5b124898848a", "450eb635-f69a-4acf-8167-9e9f9936ae29"], "metadata": {"page_label": "20", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}, "31cc409f-9256-431c-8121-9c5756707107": {"node_ids": ["bbe23339-0b40-4b8f-8e44-88c2eae299fb"], "metadata": {"page_label": "21", "file_name": "2312.10997v5.pdf", "file_path": "/home/olawale/Desktop/PROJECTS/llms/beyond-abstracts/data/upload/2312.10997v5.pdf", "file_type": "application/pdf", "file_size": 1662567, "creation_date": "2024-12-19", "last_modified_date": "2024-12-19"}}}}