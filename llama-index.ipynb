{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "from llama_index.program.openai import OpenAIPydanticProgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Song(BaseModel):\n",
    "    title: str\n",
    "    length_seconds: int\n",
    "\n",
    "\n",
    "class Album(BaseModel):\n",
    "    name: str\n",
    "    artist: str\n",
    "    songs: List[Song]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_str = \"\"\"\\\n",
    "Generate an example album, with an artist and a list of 10 songs. \\\n",
    "Using the movie {movie_name} as inspiration.\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "program = OpenAIPydanticProgram.from_defaults(output_cls=Album, \n",
    "                                              prompt_template_str=prompt_template_str, \n",
    "                                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = program(\n",
    "    movie_name=\"The Unveiling\", description=\"Data model for an album.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.program.evaporate.df import DFFullProgram, DataFrame, DataFrameRowsOnly\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "program = OpenAIPydanticProgram.from_defaults(\n",
    "    output_cls=DataFrame,\n",
    "    llm=OpenAI(temperature=0, model=\"gpt-4-0613\"),\n",
    "    prompt_template_str=(\n",
    "        \"Please extract the following query into a structured data according\"\n",
    "        \" to: {input_str}.Please extract both the set of column names and a\"\n",
    "        \" set of rows.\"\n",
    "    ),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_obj = program(\n",
    "    input_str=\"\"\"My name is John and I am 25 years old. I live in \n",
    "        New York and I like to play basketball. His name is \n",
    "        Mike and he is 30 years old. He lives in San Francisco \n",
    "        and he likes to play baseball. Sarah is 20 years old \n",
    "        and she lives in Los Angeles. She likes to play tennis.\n",
    "        Her name is Mary and she is 35 years old. \n",
    "        She lives in Chicago.\"\"\"\n",
    ")\n",
    "response_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Patch\n",
    "import io\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import AutoModelForObjectDetection\n",
    "import torch\n",
    "import openai\n",
    "import os\n",
    "import fitz\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"llama2.pdf\"\n",
    "\n",
    "# Split the base name and extension\n",
    "output_directory_path, _ = os.path.splitext(pdf_file)\n",
    "\n",
    "if not os.path.exists(output_directory_path):\n",
    "    os.makedirs(output_directory_path)\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_document = fitz.open(pdf_file)\n",
    "\n",
    "# Iterate through each page and convert to an image\n",
    "for page_number in range(pdf_document.page_count):\n",
    "    # Get the page\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Convert the page to an image\n",
    "    pix = page.get_pixmap()\n",
    "\n",
    "    # Create a Pillow Image object from the pixmap\n",
    "    image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "    # Save the image\n",
    "    image.save(f\"./{output_directory_path}/page_{page_number + 1}.png\")\n",
    "\n",
    "# Close the PDF file\n",
    "pdf_document.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "for img_path in os.listdir(\"./llama2\"):\n",
    "    image_paths.append(str(os.path.join(\"./llama2\", img_path)))\n",
    "\n",
    "\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(3, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 9:\n",
    "                break\n",
    "\n",
    "\n",
    "plot_images(image_paths[9:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-qdrant llama-index-readers-file llama-index-embeddings-fastembed llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-embeddings-clip\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import qdrant_client\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "from llama_index.core.schema import ImageDocument\n",
    "\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.schema import ImageNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "\n",
    "openai_mm_llm = OpenAIMultiModal(\n",
    "    model=\"gpt-4o\", api_key=OPENAI_API_KEY, max_new_tokens=1500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the images\n",
    "documents_images = SimpleDirectoryReader(\"./llama2/\").load_data()\n",
    "\n",
    "# Create a local Qdrant vector store\n",
    "# client = qdrant_client.QdrantClient(path=\"qdrant_index\")\n",
    "\n",
    "text_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"text_collection\"\n",
    ")\n",
    "image_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"image_collection\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store, image_store=image_store\n",
    ")\n",
    "\n",
    "# Create the MultiModal index\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    documents_images,\n",
    "    storage_context=storage_context,\n",
    ")\n",
    "\n",
    "retriever_engine = index.as_retriever(image_similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.multi_modal.retriever import (\n",
    "    MultiModalVectorIndexRetriever,\n",
    ")\n",
    "\n",
    "query = \"Compare llama2 with llama1?\"\n",
    "assert isinstance(retriever_engine, MultiModalVectorIndexRetriever)\n",
    "# retrieve for the query using text to image retrieval\n",
    "retrieval_results = retriever_engine.text_to_image_retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_images = []\n",
    "for res_node in retrieval_results:\n",
    "    if isinstance(res_node.node, ImageNode):\n",
    "        retrieved_images.append(res_node.node.metadata[\"file_path\"])\n",
    "    else:\n",
    "        display_source_node(res_node, source_length=200)\n",
    "\n",
    "\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(3, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 9:\n",
    "                break\n",
    "            \n",
    "plot_images(retrieved_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_documents = [\n",
    "    ImageDocument(image_path=image_path) for image_path in retrieved_images\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai_mm_llm.complete(\n",
    "    prompt=\"Compare llama2 with llama1?\",\n",
    "    image_documents=image_documents,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
